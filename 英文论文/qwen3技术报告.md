# Qwen3 Technical Report

Qwen Team

https://huggingface.co/Qwenhttps://modelscope.cn/organization/qwenhttps://github.com/QwenLM/Qwen3

# Abstract

In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3comprises a series of large language models (LLMs) designed to advance performance,efficiency, and multilingual capabilities. The Qwen3 series includes models of both denseand Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex,multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into aunified framework. This eliminates the need to switch between different models—–suchas chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)—–and enables dynamic mode switching based on user queries or chat templates.Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocatecomputational resources adaptively during inference, thereby balancing latency andperformance based on task complexity. Moreover, by leveraging the knowledge from theflagship models, we significantly reduce the computational resources required to buildsmaller-scale models, while ensuring their highly competitive performance. Empiricalevaluations demonstrate that Qwen3 achieves state-of-the-art results across diversebenchmarks, including tasks in code generation, mathematical reasoning, agent tasks,etc., competitive against larger MoE models and proprietary models. Compared to itspredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languagesand dialects, enhancing global accessibility through improved cross-lingual understand-ing and generation capabilities. To facilitate reproducibility and community-drivenresearch and development, all Qwen3 models are publicly accessible under Apache 2.0.

# 1 Introduction

The pursuit of artificial general intelligence (AGI) or artificial super intelligence (ASI) has long been a goalfor humanity. Recent advancements in large foundation models, e.g., GPT-4o (OpenAI, 2024), Claude3.7 (Anthropic, 2025), Gemini 2.5 (DeepMind, 2025), DeepSeek-V3 (Liu et al., 2024a), Llama-4 (Meta-AI,2025), and Qwen2.5 (Yang et al., 2024b), have demonstrated significant progress toward this objective.These models are trained on vast datasets spanning trillions of tokens across diverse domains and tasks,effectively distilling human knowledge and capabilities into their parameters. Furthermore, recentdevelopments in reasoning models, optimized through reinforcement learning, highlight the potentialfor foundation models to enhance inference-time scaling and achieve higher levels of intelligence, e.g.,o3 (OpenAI, 2025), DeepSeek-R1 (Guo et al., 2025). While most state-of-the-art models remain proprietary,the rapid growth of open-source communities has substantially reduced the performance gap betweenopen-weight and closed-source models. Notably, an increasing number of top-tier models (Meta-AI, 2025;Liu et al., $2 0 2 4 \mathsf { a }$ ; Guo et al., 2025; Yang et al., 2024b) are now being released as open-source, fosteringbroader research and innovation in artificial intelligence.

In this work, we introduce Qwen3, the latest series in our foundation model family, Qwen. Qwen3 isa collection of open-weight large language models (LLMs) that achieve state-of-the-art performanceacross a wide variety of tasks and domains. We release both dense and Mixture-of-Experts (MoE) models,with the number of parameters ranging from 0.6 billion to 235 billion, to meet the needs of differentdownstream applications. Notably, the flagship model, Qwen3-235B-A22B, is an MoE model with atotal of 235 billion parameters and 22 billion activated ones per token. This design ensures both highperformance and efficient inference.

Qwen3 introduces several key advancements to enhance its functionality and usability. First, it integratestwo distinct operating modes, thinking mode and non-thinking mode, into a single model. This allowsusers to switch between these modes without alternating between different models, e.g., switching fromQwen2.5 to QwQ (Qwen Team, 2024). This flexibility ensures that developers and users can adapt themodel’s behavior to suit specific tasks efficiently. Additionally, Qwen3 incorporates thinking budgets, pro-viding users with fine-grained control over the level of reasoning effort applied by the model during taskexecution. This capability is crucial to the optimization of computational resources and performance, tai-loring the model’s thinking behavior to meet varying complexity in real-world applications. Furthermore,Qwen3 has been pre-trained on 36 trillion tokens covering up to 119 languages and dialects, effectivelyenhancing its multilingual capabilities. This broadened language support amplifies its potential fordeployment in global use cases and international applications. These advancements together establishQwen3 as a cutting-edge open-source large language model family, capable of effectively addressingcomplex tasks across various domains and languages.

The pre-training process for Qwen3 utilizes a large-scale dataset consisting of approximately 36 trilliontokens, curated to ensure linguistic and domain diversity. To efficiently expand the training data, weemploy a multi-modal approach: Qwen2.5-VL (Bai et al., 2025) is finetuned to extract text from extensivePDF documents. We also generate synthetic data using domain-specific models: Qwen2.5-Math (Yanget al., 2024c) for mathematical content and Qwen2.5-Coder (Hui et al., 2024) for code-related data. Thepre-training process follows a three-stage strategy. In the first stage, the model is trained on about 30trillion tokens to build a strong foundation of general knowledge. In the second stage, it is further trainedon knowledge-intensive data to enhance reasoning abilities in areas like science, technology, engineering,and mathematics (STEM) and coding. Finally, in the third stage, the model is trained on long-contextdata to increase its maximum context length from 4,096 to 32,768 tokens.

To better align foundation models with human preferences and downstream applications, we employ amulti-stage post-training approach that empowers both thinking (reasoning) and non-thinking modes. Inthe first two stages, we focus on developing strong reasoning abilities through long chain-of-thought(CoT) cold-start finetuning and reinforcement learning focusing on mathematics and coding tasks. In thefinal two stages, we combine data with and without reasoning paths into a unified dataset for furtherfine-tuning, enabling the model to handle both types of input effectively, and we then apply general-domain reinforcement learning to improve performance across a wide range of downstream tasks. Forsmaller models, we use strong-to-weak distillation, leveraging both off-policy and on-policy knowledgetransfer from larger models to enhance their capabilities. Distillation from advanced teacher modelssignificantly outperforms reinforcement learning in performance and training efficiency.

We evaluate both pre-trained and post-trained versions of our models across a comprehensive set ofbenchmarks spanning multiple tasks and domains. Experimental results show that our base pre-trainedmodels achieve state-of-the-art performance. The post-trained models, whether in thinking or non-thinking mode, perform competitively against leading proprietary models and large mixture-of-experts(MoE) models such as o1, o3-mini, and DeepSeek-V3. Notably, our models excel in coding, mathematics,and agent-related tasks. For example, the flagship model Qwen3-235B-A22B achieves 85.7 on AIME’24

and 81.5 on AIME’25 (AIME, 2025), 70.7 on LiveCodeBench v5 (Jain et al., 2024), 2,056 on CodeForces,and 70.8 on BFCL v3 (Yan et al., 2024). In addition, other models in the Qwen3 series also show strongperformance relative to their size. Furthermore, we observe that increasing the thinking budget forthinking tokens leads to a consistent improvement in the model’s performance across various tasks.

In the following sections, we describe the design of the model architecture, provide details on its trainingprocedures, present the experimental results of pre-trained and post-trained models, and finally, concludethis technical report by summarizing the key findings and outlining potential directions for futureresearch.

# 2 Architecture

The Qwen3 series includes 6 dense models, namely Qwen3-0.6B, Qwen3-1.7B, Qwen3-4B, Qwen3-8B,Qwen3-14B, and Qwen3-32B, and 2 MoE models, Qwen3-30B-A3B and Qwen3-235B-A22B. The flagshipmodel, Qwen3-235B-A22B, has a total of 235B parameters with 22B activated ones. Below, we elaborateon the architecture of the Qwen3 models.

The architecture of the Qwen3 dense models is similar to Qwen2.5 (Yang et al., 2024b), including usingGrouped Query Attention (GQA, Ainslie et al., 2023), SwiGLU (Dauphin et al., 2017), Rotary PositionalEmbeddings (RoPE, Su et al., 2024), and RMSNorm (Jiang et al., 2023) with pre-normalization. Besides,we remove QKV-bias used in Qwen2 (Yang et al., 2024a) and introduce QK-Norm (Dehghani et al., 2023)to the attention mechanism to ensure stable training for Qwen3. Key information on model architectureis provided in Table 1.

The Qwen3 MoE models share the same fundamental architecture as the Qwen3 dense models. Keyinformation on model architecture is provided in Table 2. We follow Qwen2.5-MoE (Yang et al., 2024b)and implement fine-grained expert segmentation (Dai et al., 2024). The Qwen3 MoE models have 128 totalexperts with 8 activated experts per token. Unlike Qwen2.5-MoE, the Qwen3-MoE design excludes sharedexperts. Furthermore, we adopt the global-batch load balancing loss (Qiu et al., 2025) to encourage expertspecialization. These architectural and training innovations have yielded substantial improvements inmodel performance across downstream tasks.

Qwen3 models utilize Qwen’s tokenizer (Bai et al., 2023), which implements byte-level byte-pair encoding(BBPE, Brown et al., 2020; Wang et al., 2020; Sennrich et al., 2016) with a vocabulary size of 151,669.


Table 1: Model architecture of Qwen3 dense models.


<table><tr><td>Models</td><td>Layers</td><td>Heads (Q / KV)</td><td>Tie Embedding</td><td>Context Length</td></tr><tr><td>Qwen3-0.6B</td><td>28</td><td>16 / 8</td><td>Yes</td><td>32K</td></tr><tr><td>Qwen3-1.7B</td><td>28</td><td>16 / 8</td><td>Yes</td><td>32K</td></tr><tr><td>Qwen3-4B</td><td>36</td><td>32 / 8</td><td>Yes</td><td>128K</td></tr><tr><td>Qwen3-8B</td><td>36</td><td>32 / 8</td><td>No</td><td>128K</td></tr><tr><td>Qwen3-14B</td><td>40</td><td>40 / 8</td><td>No</td><td>128K</td></tr><tr><td>Qwen3-32B</td><td>64</td><td>64 / 8</td><td>No</td><td>128K</td></tr></table>


Table 2: Model architecture of Qwen3 MoE models.


<table><tr><td>Models</td><td>Layers</td><td>Heads (Q / KV)</td><td># Experts (Total / Activated)</td><td>Context Length</td></tr><tr><td>Qwen3-30B-A3B</td><td>48</td><td>32 / 4</td><td>128 / 8</td><td>128K</td></tr><tr><td>Qwen3-235B-A22B</td><td>94</td><td>64 / 4</td><td>128 / 8</td><td>128K</td></tr></table>

# 3 Pre-training

In this section, we describe the construction of our pretraining data, the details of our pretrainingapproach, and present experimental results from evaluating the base models on standard benchmarks.

# 3.1 Pre-training Data

Compared with Qwen2.5 (Yang et al., 2024b), we have significantly expanded the scale and diversity ofour training data. Specifically, we collected twice as many pre-training tokens—covering three timesmore languages. All Qwen3 models are trained on a large and diverse dataset consisting of 119 languagesand dialects, with a total of 36 trillion tokens. This dataset includes high-quality content in various

domains such as coding, STEM (Science, Technology, Engineering, and Mathematics), reasoning tasks,books, multilingual texts, and synthetic data.

To further expand the pre-training data corpus, we first employ the Qwen2.5-VL model (Bai et al., 2025)to perform text recognition on a large volume of PDF-like documents. The recognized text is then refinedusing the Qwen2.5 model (Yang et al., 2024b), which helps improve its quality. Through this two-stepprocess, we are able to obtain an additional set of high-quality text tokens, amounting to trillions in total.Besides, we employ Qwen2.5 (Yang et al., 2024b), Qwen2.5-Math (Yang et al., 2024c), and Qwen2.5-Coder(Hui et al., 2024) models to synthesize trillions of text tokens in different formats, including textbooks,question-answering, instructions, and code snippets, covering dozens of domains. Finally, we furtherexpand the pre-training corpus by incorporating additional multilingual data and introducing morelanguages. Compared to the pre-training data used in Qwen2.5, the number of supported languages hasbeen significantly increased from 29 to 119, enhancing the model’s linguistic coverage and cross-lingualcapabilities.

We have developed a multilingual data annotation system designed to enhance both the quality anddiversity of training data. This system has been applied to our large-scale pre-training datasets, annotatingover 30 trillion tokens across multiple dimensions such as educational value, fields, domains, and safety.These detailed annotations support more effective data filtering and combination. Unlike previousstudies (Xie et al., 2023; Fan et al., 2023; Liu et al., 2024b) that optimize the data mixture at the data sourceor domain level, our method optimizes the data mixture at the instance-level through extensive ablationexperiments on small proxy models with the fine-grained data labels.

# 3.2 Pre-training Stage

The Qwen3 models are pre-trained through a three-stage process:

(1) General Stage (S1): At the first pre-training stage, all Qwen3 models are trained on over 30trillion tokens using a sequence length of 4,096 tokens. At this stage, the models have been fullypre-trained on language proficiency and general world knowledge, with training data covering119 languages and dialects.

(2) Reasoning Stage (S2): To further improve the reasoning ability, we optimize the pre-trainingcorpus of this stage by increasing the proportion of STEM, coding, reasoning, and synthetic data.The models are further pre-trained with about 5T higher-quality tokens at a sequence length of4,096 tokens. We also accelerate the learning rate decay during this stage.

(3) Long Context Stage: In the final pre-training stage, we collect high-quality long context corporato extend the context length of Qwen3 models. All models are pre-trained on hundreds of billionsof tokens with a sequence length of 32,768 tokens. The long context corpus includes $7 5 \%$ of textbetween 16,384 to 32,768 tokens in length, and $2 5 \%$ of text between 4,096 to 16,384 in length.Following Qwen2.5 (Yang et al., 2024b), we increase the base frequency of RoPE from 10,000 to1,000,000 using the ABF technique (Xiong et al., 2023). Meanwhile, we introduce YARN (Penget al., 2023) and Dual Chunk Attention (DCA, An et al., 2024) to achieve a four-fold increase insequence length capacity during inference.

Similar to Qwen2.5 (Yang et al., 2024b), we develop scaling laws for optimal hyper-parameters (e.g.,learning rate scheduler, and batch size) predictions based on three pre-training stages mentioned above.Through extensive experiments, we systematically study the relationship between model architecture,training data, training stage, and optimal training hyper-parameters. Finally, we set the predicted optimallearning rate and batch size strategy for each dense or MoE model.

# 3.3 Pre-training Evaluation

We conduct comprehensive evaluations of the base language models of the Qwen3 series. The evaluationof base models mainly focuses on their performance in general knowledge, reasoning, mathematics,scientific knowledge, coding, and multilingual capabilities. The evaluation datasets for pre-trained basemodels include 15 benchmarks:

• General Tasks: MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang et al., 2024) (5-shot, CoT), MMLU-redux (Gema et al., 2024) (5-shot), BBH (Suzgun et al., 2023) (3-shot, CoT),SuperGPQA (Du et al., 2025)(5-shot, CoT).

• Math & STEM Tasks: GPQA (Rein et al., 2023) (5-shot, CoT), GSM8K (Cobbe et al., 2021) (4-shot,CoT), MATH (Hendrycks et al., 2021b) (4-shot, CoT).

• Coding Tasks: EvalPlus (Liu et al., 2023a) (0-shot) (Average of HumanEval (Chen et al., 2021)MBPP (Austin et al., 2021), Humaneval+, ${ \mathrm { M B P P + } }$ ) (Liu et al., 2023a), MultiPL-E (Cassano et al.,2023) (0-shot) (Python, $\mathrm { C } { + } { + } ,$ , JAVA, PHP, TypeScript, C#, Bash, JavaScript), MBPP-3shot (Austinet al., 2021), CRUX-O of CRUXEval (1-shot) (Gu et al., 2024).

• Multilingual Tasks: MGSM (Shi et al., 2023) (8-shot, CoT), MMMLU (OpenAI, 2024) (5-shot),INCLUDE (Romanou et al., 2024) (5-shot).

For the base model baselines, we compare the Qwen3 series base models with the Qwen2.5 base models(Yang et al., 2024b) and other leading open-source base models, including DeepSeek-V3 Base (Liu et al.,2024a), Gemma-3 (Team et al., 2025), Llama-3 (Dubey et al., 2024), and Llama-4 (Meta-AI, 2025) seriesbase models, in terms of scale of parameters. All models are evaluated using the same evaluation pipelineand the widely-used evaluation settings to ensure fair comparison.

Summary of Evaluation Results Based on the overall evaluation results, we highlight some keyconclusions of Qwen3 base models.

(1) Compared with the previously open-source SOTA dense and MoE base models (such as DeepSeek-V3 Base, Llama-4-Maverick Base, and Qwen2.5-72B-Base), Qwen3-235B-A22B-Base outperformsthese models in most tasks with significantly fewer total parameters or activated parameters.

(2) For the Qwen3 MoE base models, our experimental results indicate that: (a) Using the samepre-training data, Qwen3 MoE base models can achieve similar performance to Qwen3 densebase models with only 1/5 activated parameters. (b) Due to the improvements of the Qwen3MoE architecture, the scale-up of the training tokens, and more advanced training strategies,the Qwen3 MoE base models can outperform the Qwen2.5 MoE base models with less than 1/2activated parameters and fewer total parameters. (c) Even with 1/10 of the activated parameters ofthe Qwen2.5 dense base model, the Qwen3 MoE base model can achieve comparable performance,which brings us significant advantages in inference and training costs.

(3) The overall performance of the Qwen3 dense base models is comparable to the Qwen2.5 basemodels at higher parameter scales. For example, Qwen3-1.7B/4B/8B/14B/32B-Base achievecomparable performance to Qwen2.5-3B/7B/14B/32B/72B-Base, respectively. Especially inSTEM, coding, and reasoning benchmarks, the performance of Qwen3 dense base models evensurpasses Qwen2.5 base models at higher parameter scales.

The detailed results are as follows.

Qwen3-235B-A22B-Base We compare Qwen3-235B-A22B-Base to our previous similar-sized MoEQwen2.5-Plus-Base (Yang et al., 2024b) and other leading open-source base models: Llama-4-Maverick(Meta-AI, 2025), Qwen2.5-72B-Base (Yang et al., 2024b), DeepSeek-V3 Base (Liu et al., 2024a). Fromthe results in Table 3, the Qwen3-235B-A22B-Base model attains the highest performance scores acrossmost of the evaluated benchmarks. We further compare Qwen3-235B-A22B-Base with other baselinesseparately for the detailed analysis.

(1) Compared with the recently open-source model Llama-4-Maverick-Base, which has about twicethe number of parameters, Qwen3-235B-A22B-Base still performs better on most benchmarks.

(2) Compared with the previously state-of-the-art open-source model DeepSeek-V3-Base, Qwen3-235B-A22B-Base outperforms DeepSeek-V3-Base on 14 out of 15 evaluation benchmarks withonly about 1/3 the total number of parameters and 2/3 activated parameters, demonstrating thepowerful and cost-effectiveness of our models.

(3) Compared with our previous MoE Qwen2.5-Plus of similar size, Qwen3-235B-A22B-Base sig-nificantly outperforms it with fewer parameters and activated parameters, which shows theremarkable advantages of Qwen3 in pre-training data, training strategy, and model architecture.

(4) Compared with our previous flagship open-source dense model Qwen2.5-72B-Base, Qwen3-235B-A22B-Base surpasses the latter in all benchmarks and uses fewer than 1/3 of the activatedparameters. Meanwhile, due to the advantage of the model architecture, the inference costs andtraining costs on each trillion tokens of Qwen3-235B-A22B-Base are much cheaper than those ofQwen2.5-72B-Base.

Qwen3-32B-Base Qwen3-32B-Base is our largest dense model among the Qwen3 series. We compareit to the baselines of similar sizes, including Gemma-3-27B (Team et al., 2025) and Qwen2.5-32B (Yanget al., 2024b). In addition, we introduce two strong baselines: the recently open-source MoE model Llama-4-Scout, which has three times the parameters of Qwen3-32B-Base but half the activated parameters;


Table 3: Comparison among Qwen3-235B-A22B-Base and other representative strong open-sourcebaselines. The highest, the second-best scores are shown in bold and underlined, respectively.


<table><tr><td></td><td>Qwen2.5-72B</td><td>Qwen2.5-Plus</td><td>Llama-4-Maverick</td><td>DeepSeek-V3</td><td>Qwen3-235B-A22B</td></tr><tr><td></td><td>Base</td><td>Base</td><td>Base</td><td>Base</td><td>Base</td></tr><tr><td>Architecture</td><td>Dense</td><td>MoE</td><td>MoE</td><td>MoE</td><td>MoE</td></tr><tr><td># Total Params</td><td>72B</td><td>271B</td><td>402B</td><td>671B</td><td>235B</td></tr><tr><td># Activated Params</td><td>72B</td><td>37B</td><td>17B</td><td>37B</td><td>22B</td></tr><tr><td colspan="6">General Tasks</td></tr><tr><td>MMLU</td><td>86.06</td><td>85.02</td><td>85.16</td><td>87.19</td><td>87.81</td></tr><tr><td>MMLU-Redux</td><td>83.91</td><td>82.69</td><td>84.05</td><td>86.14</td><td>87.40</td></tr><tr><td>MMLU-Pro</td><td>58.07</td><td>63.52</td><td>63.91</td><td>59.84</td><td>68.18</td></tr><tr><td>SuperGPQA</td><td>36.20</td><td>37.18</td><td>40.85</td><td>41.53</td><td>44.06</td></tr><tr><td>BBH</td><td>86.30</td><td>85.60</td><td>83.62</td><td>86.22</td><td>88.87</td></tr><tr><td colspan="6">Math &amp; STEM Tasks</td></tr><tr><td>GPQA</td><td>45.88</td><td>41.92</td><td>43.94</td><td>41.92</td><td>47.47</td></tr><tr><td>GSM8K</td><td>91.50</td><td>91.89</td><td>87.72</td><td>87.57</td><td>94.39</td></tr><tr><td>MATH</td><td>62.12</td><td>62.78</td><td>63.32</td><td>62.62</td><td>71.84</td></tr><tr><td colspan="6">Coding Tasks</td></tr><tr><td>EvalPlus</td><td>65.93</td><td>61.43</td><td>68.38</td><td>63.75</td><td>77.60</td></tr><tr><td>MultiPL-E</td><td>58.70</td><td>62.16</td><td>57.28</td><td>62.26</td><td>65.94</td></tr><tr><td>MBPP</td><td>76.00</td><td>74.60</td><td>75.40</td><td>74.20</td><td>81.40</td></tr><tr><td>CRUX-O</td><td>66.20</td><td>68.50</td><td>77.00</td><td>76.60</td><td>79.00</td></tr><tr><td colspan="6">Multilingual Tasks</td></tr><tr><td>MGSM</td><td>82.40</td><td>82.21</td><td>79.69</td><td>82.68</td><td>83.53</td></tr><tr><td>MMMLU</td><td>84.40</td><td>83.49</td><td>83.09</td><td>85.88</td><td>86.70</td></tr><tr><td>INCLUDE</td><td>69.05</td><td>66.97</td><td>73.47</td><td>75.17</td><td>73.46</td></tr></table>


Table 4: Comparison among Qwen3-32B-Base and other strong open-source baselines. The highestand second-best scores are shown in bold and underlined, respectively.


<table><tr><td></td><td>Qwen2.5-32B Base</td><td>Qwen2.5-72B Base</td><td>Gemma-3-27B Base</td><td>Llama-4-Scout Base</td><td>Qwen3-32B Base</td></tr><tr><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>MoE</td><td>Dense</td></tr><tr><td># Total Params</td><td>32B</td><td>72B</td><td>27B</td><td>109B</td><td>32B</td></tr><tr><td># Activated Params</td><td>32B</td><td>72B</td><td>27B</td><td>17B</td><td>32B</td></tr><tr><td colspan="6">General Tasks</td></tr><tr><td>MMLU</td><td>83.32</td><td>86.06</td><td>78.69</td><td>78.27</td><td>83.61</td></tr><tr><td>MMLU-Redux</td><td>81.97</td><td>83.91</td><td>76.53</td><td>71.09</td><td>83.41</td></tr><tr><td>MMLU-Pro</td><td>55.10</td><td>58.07</td><td>52.88</td><td>56.13</td><td>65.54</td></tr><tr><td>SuperGPQA</td><td>33.55</td><td>36.20</td><td>29.87</td><td>26.51</td><td>39.78</td></tr><tr><td>BBH</td><td>84.48</td><td>86.30</td><td>79.95</td><td>82.40</td><td>87.38</td></tr><tr><td colspan="6">Math &amp; STEM Tasks</td></tr><tr><td>GPQA</td><td>47.97</td><td>45.88</td><td>26.26</td><td>40.40</td><td>49.49</td></tr><tr><td>GSM8K</td><td>92.87</td><td>91.50</td><td>81.20</td><td>85.37</td><td>93.40</td></tr><tr><td>MATH</td><td>57.70</td><td>62.12</td><td>51.78</td><td>51.66</td><td>61.62</td></tr><tr><td colspan="6">Coding Tasks</td></tr><tr><td>EvalPlus</td><td>66.25</td><td>65.93</td><td>55.78</td><td>59.90</td><td>72.05</td></tr><tr><td>MultiPL-E</td><td>58.30</td><td>58.70</td><td>45.03</td><td>47.38</td><td>67.06</td></tr><tr><td>MBPP</td><td>73.60</td><td>76.00</td><td>68.40</td><td>68.60</td><td>78.20</td></tr><tr><td>CRUX-O</td><td>67.80</td><td>66.20</td><td>60.00</td><td>61.90</td><td>72.50</td></tr><tr><td colspan="6">Multilingual Tasks</td></tr><tr><td>MGSM</td><td>78.12</td><td>82.40</td><td>73.74</td><td>79.93</td><td>83.06</td></tr><tr><td>MMMLU</td><td>82.40</td><td>84.40</td><td>77.62</td><td>74.83</td><td>83.83</td></tr><tr><td>INCLUDE</td><td>64.35</td><td>69.05</td><td>68.94</td><td>68.09</td><td>67.87</td></tr></table>


Table 5: Comparison among Qwen3-14B-Base, Qwen3-30B-A3B-Base, and other strong open-sourcebaselines. The highest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td></td><td>Gemma-3-12B</td><td>Qwen2.5-14B</td><td>Qwen2.5-32B</td><td>Qwen2.5-Turbo</td><td>Qwen3-14B</td><td>Qwen3-30B-A3B</td></tr><tr><td></td><td>Base</td><td>Base</td><td>Base</td><td>Base</td><td>Base</td><td>Base</td></tr><tr><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>MoE</td><td>Dense</td><td>MoE</td></tr><tr><td># Total Params</td><td>12B</td><td>14B</td><td>32B</td><td>42B</td><td>14B</td><td>30B</td></tr><tr><td># Activated Params</td><td>12B</td><td>14B</td><td>32B</td><td>6B</td><td>14B</td><td>3B</td></tr><tr><td colspan="7">General Tasks</td></tr><tr><td>MMLU</td><td>73.87</td><td>79.66</td><td>83.32</td><td>79.50</td><td>81.05</td><td>81.38</td></tr><tr><td>MMLU-Redux</td><td>70.70</td><td>76.64</td><td>81.97</td><td>77.11</td><td>79.88</td><td>81.17</td></tr><tr><td>MMLU-Pro</td><td>44.91</td><td>51.16</td><td>55.10</td><td>55.60</td><td>61.03</td><td>61.49</td></tr><tr><td>SuperGPQA</td><td>24.61</td><td>30.68</td><td>33.55</td><td>31.19</td><td>34.27</td><td>35.72</td></tr><tr><td>BBH</td><td>74.28</td><td>78.18</td><td>84.48</td><td>76.10</td><td>81.07</td><td>81.54</td></tr><tr><td colspan="7">Math &amp; STEM Tasks</td></tr><tr><td>GPQA</td><td>31.31</td><td>32.83</td><td>47.97</td><td>41.41</td><td>39.90</td><td>43.94</td></tr><tr><td>GSM8K</td><td>78.01</td><td>90.22</td><td>92.87</td><td>88.32</td><td>92.49</td><td>91.81</td></tr><tr><td>MATH</td><td>44.43</td><td>55.64</td><td>57.70</td><td>55.60</td><td>62.02</td><td>59.04</td></tr><tr><td colspan="7">Coding Tasks</td></tr><tr><td>EvalPlus</td><td>52.65</td><td>60.70</td><td>66.25</td><td>61.23</td><td>72.23</td><td>71.45</td></tr><tr><td>MultiPL-E</td><td>43.03</td><td>54.79</td><td>58.30</td><td>53.24</td><td>61.69</td><td>66.53</td></tr><tr><td>MBPP</td><td>60.60</td><td>69.00</td><td>73.60</td><td>67.60</td><td>73.40</td><td>74.40</td></tr><tr><td>CRUX-O</td><td>52.00</td><td>61.10</td><td>67.80</td><td>60.20</td><td>68.60</td><td>67.20</td></tr><tr><td colspan="7">Multilingual Tasks</td></tr><tr><td>MGSM</td><td>64.35</td><td>74.68</td><td>78.12</td><td>70.45</td><td>79.20</td><td>79.11</td></tr><tr><td>MMMLU</td><td>72.50</td><td>78.34</td><td>82.40</td><td>79.76</td><td>79.69</td><td>81.46</td></tr><tr><td>INCLUDE</td><td>63.34</td><td>60.26</td><td>64.35</td><td>59.25</td><td>64.55</td><td>67.00</td></tr></table>


Table 6: Comparison among Qwen8B-Base and other strong open-source baselines. The highest andsecond-best scores are shown in bold and underlined, respectively.


<table><tr><td></td><td>Llama-3-8B Base</td><td>Qwen2.5-7B Base</td><td>Qwen2.5-14B Base</td><td>Qwen3-8B Base</td></tr><tr><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td></tr><tr><td># Total Params</td><td>8B</td><td>7B</td><td>14B</td><td>8B</td></tr><tr><td># Activated Params</td><td>8B</td><td>7B</td><td>14B</td><td>8B</td></tr><tr><td colspan="5">General Tasks</td></tr><tr><td>MMLU</td><td>66.60</td><td>74.16</td><td>79.66</td><td>76.89</td></tr><tr><td>MMLU-Redux</td><td>61.59</td><td>71.06</td><td>76.64</td><td>76.17</td></tr><tr><td>MMLU-Pro</td><td>35.36</td><td>45.00</td><td>51.16</td><td>56.73</td></tr><tr><td>SuperGPQA</td><td>20.54</td><td>26.34</td><td>30.68</td><td>31.64</td></tr><tr><td>BBH</td><td>57.70</td><td>70.40</td><td>78.18</td><td>78.40</td></tr><tr><td colspan="5">Math &amp; STEM Tasks</td></tr><tr><td>GPQA</td><td>25.80</td><td>36.36</td><td>32.83</td><td>44.44</td></tr><tr><td>GSM8K</td><td>55.30</td><td>85.36</td><td>90.22</td><td>89.84</td></tr><tr><td>MATH</td><td>20.50</td><td>49.80</td><td>55.64</td><td>60.80</td></tr><tr><td colspan="5">Coding Tasks</td></tr><tr><td>EvalPlus</td><td>44.13</td><td>62.18</td><td>60.70</td><td>67.65</td></tr><tr><td>MultiPL-E</td><td>31.45</td><td>50.73</td><td>54.79</td><td>58.75</td></tr><tr><td>MBPP</td><td>48.40</td><td>63.40</td><td>69.00</td><td>69.80</td></tr><tr><td>CRUX-O</td><td>36.80</td><td>48.50</td><td>61.10</td><td>62.00</td></tr><tr><td colspan="5">Multilingual Tasks</td></tr><tr><td>MGSM</td><td>38.92</td><td>63.60</td><td>74.68</td><td>76.02</td></tr><tr><td>MMMLU</td><td>59.65</td><td>71.34</td><td>78.34</td><td>75.72</td></tr><tr><td>IINCLUDE</td><td>44.94</td><td>53.98</td><td>60.26</td><td>59.40</td></tr></table>


Table 7: Comparison among Qwen3-4B-Base and other strong open-source baselines. The highest andsecond-best scores are shown in bold and underlined, respectively.


<table><tr><td></td><td>Gemma-3-4B
Base</td><td>Qwen2.5-3B
Base</td><td>Qwen2.5-7B
Base</td><td>Qwen3-4B
Base</td></tr><tr><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td></tr><tr><td># Total Params</td><td>4B</td><td>3B</td><td>7B</td><td>4B</td></tr><tr><td># Activated Params</td><td>4B</td><td>3B</td><td>7B</td><td>4B</td></tr><tr><td colspan="5">General Tasks</td></tr><tr><td>MMLU</td><td>59.51</td><td>65.62</td><td>74.16</td><td>72.99</td></tr><tr><td>MMLU-Redux</td><td>56.91</td><td>63.68</td><td>71.06</td><td>72.79</td></tr><tr><td>MMLU-Pro</td><td>29.23</td><td>34.61</td><td>45.00</td><td>50.58</td></tr><tr><td>SuperGPQA</td><td>17.68</td><td>20.31</td><td>26.34</td><td>28.43</td></tr><tr><td>BBH</td><td>51.70</td><td>56.30</td><td>70.40</td><td>72.59</td></tr><tr><td colspan="5">Math &amp; STEM Tasks</td></tr><tr><td>GPQA</td><td>24.24</td><td>26.26</td><td>36.36</td><td>36.87</td></tr><tr><td>GSM8K</td><td>43.97</td><td>79.08</td><td>85.36</td><td>87.79</td></tr><tr><td>MATH</td><td>26.10</td><td>42.64</td><td>49.80</td><td>54.10</td></tr><tr><td colspan="5">Coding Tasks</td></tr><tr><td>EvalPlus</td><td>43.23</td><td>46.28</td><td>62.18</td><td>63.53</td></tr><tr><td>MultiPL-E</td><td>28.06</td><td>39.65</td><td>50.73</td><td>53.13</td></tr><tr><td>MBPP</td><td>46.40</td><td>54.60</td><td>63.40</td><td>67.00</td></tr><tr><td>CRUX-O</td><td>34.00</td><td>36.50</td><td>48.50</td><td>55.00</td></tr><tr><td colspan="5">Multilingual Tasks</td></tr><tr><td>MGSM</td><td>33.11</td><td>47.53</td><td>63.60</td><td>67.74</td></tr><tr><td>MMMLU</td><td>59.62</td><td>65.55</td><td>71.34</td><td>71.42</td></tr><tr><td>INCLUDE</td><td>49.06</td><td>45.90</td><td>53.98</td><td>56.29</td></tr></table>


Table 8: Comparison among Qwen3-1.7B-Base, Qwen3-0.6B-Base, and other strong open-source base-lines. The highest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td></td><td>Qwen2.5-0.5B Base</td><td>Qwen3-0.6B Base</td><td>Gemma-3-1B Base</td><td>Qwen2.5-1.5B Base</td><td>Qwen3-1.7B Base</td></tr><tr><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td></tr><tr><td># Total Params</td><td>0.5B</td><td>0.6B</td><td>1B</td><td>1.5B</td><td>1.7B</td></tr><tr><td># Activated Params</td><td>0.5B</td><td>0.6B</td><td>1B</td><td>1.5B</td><td>1.7B</td></tr><tr><td colspan="6">General Tasks</td></tr><tr><td>MMLU</td><td>47.50</td><td>52.81</td><td>26.26</td><td>60.90</td><td>62.63</td></tr><tr><td>MMLU-Redux</td><td>45.10</td><td>51.26</td><td>25.99</td><td>58.46</td><td>61.66</td></tr><tr><td>MMLU-Pro</td><td>15.69</td><td>24.74</td><td>9.72</td><td>28.53</td><td>36.76</td></tr><tr><td>SuperGPQA</td><td>11.30</td><td>15.03</td><td>7.19</td><td>17.64</td><td>20.92</td></tr><tr><td>BBH</td><td>20.30</td><td>41.47</td><td>28.13</td><td>45.10</td><td>54.47</td></tr><tr><td colspan="6">Math &amp; STEM Tasks</td></tr><tr><td>GPQA</td><td>24.75</td><td>26.77</td><td>24.75</td><td>24.24</td><td>28.28</td></tr><tr><td>GSM8K</td><td>41.62</td><td>59.59</td><td>2.20</td><td>68.54</td><td>75.44</td></tr><tr><td>MATH</td><td>19.48</td><td>32.44</td><td>3.66</td><td>35.00</td><td>43.50</td></tr><tr><td colspan="6">Coding Tasks</td></tr><tr><td>EvalPlus</td><td>31.85</td><td>36.23</td><td>8.98</td><td>44.80</td><td>52.70</td></tr><tr><td>MultiPL-E</td><td>18.70</td><td>24.58</td><td>5.15</td><td>33.10</td><td>42.71</td></tr><tr><td>MBPP</td><td>29.80</td><td>36.60</td><td>9.20</td><td>43.60</td><td>55.40</td></tr><tr><td>CRUX-O</td><td>12.10</td><td>27.00</td><td>3.80</td><td>29.60</td><td>36.40</td></tr><tr><td colspan="6">Multilingual Tasks</td></tr><tr><td>MGSM</td><td>12.07</td><td>30.99</td><td>1.74</td><td>32.82</td><td>50.71</td></tr><tr><td>MMMLU</td><td>31.53</td><td>50.16</td><td>26.57</td><td>60.27</td><td>63.27</td></tr><tr><td>INCLUDE</td><td>24.74</td><td>34.26</td><td>25.62</td><td>39.55</td><td>45.57</td></tr></table>

and our previous flagship open-source dense model Qwen2.5-72B-Base, which has more than twice thenumber of parameters compared to Qwen3-32B-Base. The results are shown in Table 4, which supportthree key conclusions:

(1) Compared with the similar-sized models, Qwen3-32B-Base outperforms Qwen2.5-32B-Base andGemma-3-27B Base on most benchmarks. Notably, Qwen3-32B-Base achieves 65.54 on MMLU-Pro and 39.78 on SuperGPQA, significantly outperforming its predecessor Qwen2.5-32B-Base.In addition, Qwen3-32B-Base achieves significantly higher encoding benchmark scores than allbaseline models.

(2) Surprisingly, we find that Qwen3-32B-Base achieves competitive results compared to Qwen2.5-72B-Base. Although Qwen3-32B-Base has less than half the number of parameters of Qwen2.5-72B-Base, it outperforms Qwen2.5-72B-Base in 10 of the 15 evaluation benchmarks. On coding,mathematics, and reasoning benchmarks, Qwen3-32B-Base has remarkable advantages.

(3) Compared to Llama-4-Scout-Base, Qwen3-32B-Base significantly outperforms it on all 15 bench-marks, with only one-third of the number of parameters of Llama-4-Scout-Base, but twice thenumber of activated parameters.

Qwen3-14B-Base & Qwen3-30B-A3B-Base The evaluation of the Qwen3-14B-Base and Qwen3-30B-A3B-Base is compared against baselines of similar sizes, including Gemma-3-12B Base, Qwen2.5-14BBase. Similarly, we also introduce two strong baselines: (1) Qwen2.5-Turbo (Yang et al., 2024b), whichhas 42B parameters and 6B activated parameters. Note that its activated parameters are twice those ofQwen3-30B-A3B-Base. (2) Qwen2.5-32B-Base, which has 11 times the activated parameters of Qwen3-30B-A3B and more than twice that of Qwen3-14B. The results are shown in Table 5, where we can drawthe following conclusions.

(1) Compared with the similar-sized models, Qwen3-14B-Base significantly performs better thanQwen2.5-14B-Base and Gemma-3-12B-Base on all 15 benchmarks.

(2) Similarly, Qwen3-14B-Base also achieves very competitive results compared to Qwen2.5-32B-Basewith less than half of the parameters.

(3) With only 1/5 activated non-embedding parameters, Qwen3-30B-A3B significantly outperformsQwen2.5-14B-Base on all tasks, and achieves comparable performance to Qwen3-14B-Base andQwen2.5-32B-Base, which brings us significant advantages in inference and training costs.

Qwen3-8B / 4B / 1.7B / 0.6B-Base For edge-side models, we take similar-sized Qwen2.5, Llama-3, andGemma-3 base models as the baselines. The results can be seen in Table 6, Table 7, and Table 8. All Qwen38B / 4B / 1.7B / 0.6B-Base models continue to maintain strong performance across nearly all benchmarks.Notably, Qwen3-8B / 4B / 1.7B-Base models even outperform larger size Qwen2.5-14B / 7B / 3B Basemodels on over half of the benchmarks, especially on STEM-related and coding benchmarks, reflectingthe significant improvement of the Qwen3 models.

# 4 Post-training

![](images/4c16a939c7633629b6b957d630cc5973a7c4b5dd95f0eaccceead8c9c4d03bb3.jpg)



Figure 1: Post-training pipeline of the Qwen3 series models.


The post-training pipeline of Qwen3 is strategically designed with two core objectives:

(1) Thinking Control: This involves the integration of two distinct modes, namely the “non-thinking”and “thinking” modes, providing users with the flexibility to choose whether the model shouldengage in reasoning or not, and to control the depth of thinking by specifying a token budget forthe thinking process.

(2) Strong-to-Weak Distillation: This aims to streamline and optimize the post-training processfor lightweight models. By leveraging the knowledge from large-scale models, we substantiallyreduce both the computational costs and the development efforts required for building smaller-scale models.

As illustrated in Figure 1, the flagship models in the Qwen3 series follow a sophisticated four-stagetraining process. The first two stages focus on developing the models’ “thinking” abilities. The next twostages aim to integrate strong “non-thinking” functionalities into the models.

Preliminary experiments suggest that directly distilling the output logits from teacher models intolightweight student models can effectively enhance their performance while maintaining fine-grainedcontrol over their reasoning processes. This approach eliminates the necessity of performing an exhaustivefour-stage training process individually for every small-scale model. It leads to better immediateperformance, as indicated by higher Pass@1 scores, and also improves the model’s ability of exploration,as reflected in improved Pass@64 results. In addition, it achieves these gains with much greater trainingefficiency, requiring only 1/10 of the GPU hours compared to the four-stage training method.

In the following sections, we present the four-stage training process and provide a detailed explanationof the Strong-to-Weak Distillation approach.

# 4.1 Long-CoT Cold Start

We begin by curating a comprehensive dataset that spans a wide range of categories, including math,code, logical reasoning, and general STEM problems. Each problem in the dataset is paired with verifiedreference answers or code-based test cases. This dataset serves as the foundation for the “cold start”phase of long Chain-of-Thought (long-CoT) training.

The dataset construction involves a rigorous two-phase filtering process: query filtering and responsefiltering. In the query filtering phase, we use Qwen2.5-72B-Instruct to identify and remove queries thatare not easily verifiable. This includes queries containing multiple sub-questions or those asking forgeneral text generation. Furthermore, we exclude queries that Qwen2.5-72B-Instruct can answer correctlywithout using CoT reasoning. This helps prevent the model from relying on superficial guessing andensures that only complex problems requiring deeper reasoning are included. Additionally, we annotateeach query’s domain using Qwen2.5-72B-Instruct to maintain balanced domain representation across thedataset.

After reserving a validation query set, we generate $N$ candidate responses for each remaining queryusing QwQ-32B (Qwen Team, 2025). When QwQ-32B consistently fails to generate correct solutions,human annotators manually assess the accuracy of the responses. For queries with positive Pass@N,further stringent filtering criteria are applied to remove responses that (1) yield incorrect final answers,(2) contain substantial repetition, (3) clearly indicate guesswork without adequate reasoning, (4) exhibitinconsistencies between the thinking and summary contents, (5) involve inappropriate language mixing orstylistic shifts, or (6) are suspected of being overly similar to potential validation set items. Subsequently,a carefully selected subset of the refined dataset is used for the initial cold-start training of the reasoningpatterns. The objective at this stage is to instill foundational reasoning patterns in the model withoutoverly emphasizing immediate reasoning performance. This approach ensures that the model’s potentialis not limited, allowing for greater flexibility and improvement during the subsequent reinforcementlearning (RL) phase. To achieve this objective effectively, it is preferable to minimize both the number oftraining samples and the training steps during this preparatory phase.

# 4.2 Reasoning RL

The query-verifier pairs used in the Reasoning RL stage must satisfy the following four criteria: (1) Theywere not used during the cold-start phase. (2) They are learnable for the cold-start model. (3) They areas challenging as possible. (4) They cover a broad range of sub-domains. We ultimately collect a totalof 3,995 query-verifier pairs, and employed GRPO (Shao et al., 2024) to update the model parameters.We observe that using a large batch size and a high number of rollouts per query, along with off-policytraining to improve sample efficiency, is beneficial to the training process. We have also addressed howto balance exploration and exploitation by controlling the model’s entropy to increase steadily or remain


Table 9: Examples of SFT data for thinking and non-thinking modes during the thinking mode fusionstage. For the thinking mode, the /think flag can be omitted since it represents the default behavior. Thisfeature has been implemented in the chat template1 supported by the Hugging Face’s tokenizer, wherethe thinking mode can be disabled using an additional parameter enable thinking=False.


<table><tr><td>Thinking Mode</td><td>Non-Thinking Mode</td></tr><tr><td>&lt;|im_start|&gt;user
{query} /think&lt;|im_end|&gt;</td><td>&lt;|im_start|&gt;user
{query} /no Think&lt;|im_end|&gt;</td></tr><tr><td>&lt;|im_start|&gt;assistant
&lt;think&gt;</td><td>&lt;|im start|&gt;assistant
&lt;think&gt;</td></tr><tr><td>{thinking_content}</td><td>&lt;/think&gt;</td></tr><tr><td>{response}&lt;|im_end|&gt;</td><td>{response}&lt;|im_end|&gt;</td></tr></table>

stable, which is crucial for maintaining stable training. As a result, we achieve consistent improvementsin both training reward and validation performance over the course of a single RL run, without anymanual intervention on hyperparameters. For instance, the AIME’24 score of the Qwen3-235B-A22Bmodel increases from 70.1 to 85.1 over a total of 170 RL training steps.

# 4.3 Thinking Mode Fusion

The goal of the Thinking Mode Fusion stage is to integrate the “non-thinking” capabilities into thepreviously developed “thinking” model. This approach allows developers to manage and controlreasoning behaviors, while also reducing the cost and complexity of deploying separate models forthinking and non-thinking tasks. To achieve this, we conduct continual supervised fine-tuning (SFT)on the Reasoning RL model and design a chat template to fuse the two modes. Moreover, we find thatmodels capable of handling both modes proficiently perform consistently well under different thinkingbudgets.

Construction of SFT data. The SFT dataset combines both the “thinking” and “non-thinking” data.To ensure that the performance of the Stage 2 model is not compromised by the additional SFT, the“thinking” data is generated via rejection sampling on Stage 1 queries using the Stage 2 model itself. The“non-thinking” data, on the other hand, is carefully curated to cover a diverse range of tasks, includingcoding, mathematics, instruction-following, multilingual tasks, creative writing, question answering,and role-playing. Additionally, we employ automatically generated checklists for assessing the responsequality of “non-thinking” data. To enhance the performance on tasks with low-resource languages, weparticularly increase the proportion of translation tasks.

Chat Template Design. To better integrate the two modes and enable users to dynamically switch themodel’s thinking process, we design chat templates for Qwen3, as shown in Table 9. Specifically, forsamples in thinking mode and non-thinking mode, we introduce /think and /no think flags in the userquery or system message, respectively. This allows the model to follow the user’s input and select theappropriate thinking mode accordingly. For non-thinking mode samples, we retain an empty thinkingblock in the assistant’s response. This design ensures internal format consistency within the model andallows developers to prevent the model from engaging in thinking behavior by concatenating an emptythink block in the chat template. By default, the model operates in thinking mode; therefore, we addsome thinking mode training samples where the user queries do not include /think flags. For morecomplex multi-turn dialogs, we randomly insert multiple /think and /no think flags into users’ queries,with the model response adhering to the last flag encountered.

Thinking Budget. An additional advantage of Thinking Mode Fusion is that, once the model learns torespond in both non-thinking and thinking modes, it naturally develops the ability to handle intermediatecases—generating responses based on incomplete thinking. This capability lays the foundation forimplementing budget control over the model’s thinking process. Specifically, when the length of themodel’s thinking reaches a user-defined threshold, we manually halt the thinking process and insertthe stop-thinking instruction: “Considering the limited time by the user, I have to give thesolution based on the thinking directly now.\n</think>. $\backslash \mathtt { n } \backslash \bar { \mathtt { n } } ^ { \prime \prime }$ . After this instruction is inserted,the model proceeds to generate a final response based on its accumulated reasoning up to that point. Itis worth noting that this ability is not explicitly trained but emerges naturally as a result of applyingThinking Mode Fusion.

# 4.4 General RL

The General RL stage aims to broadly enhance the models’ capabilities and stability across diversescenarios. To facilitate this, we have established a sophisticated reward system covering over 20 distincttasks, each with customized scoring criteria. These tasks specifically target enhancements in the followingcore capabilities:

• Instruction Following: This capability ensures that models accurately interpret and follow userinstructions, including requirements related to content, format, length, and the use of structuredoutput, delivering responses that align with user expectations.

• Format Following: In addition to explicit instructions, we expect the model to adhere to specificformatting conventions. For instance, it should respond appropriately to the /think and /no think flags by switching between thinking and non-thinking modes, and consistently usedesignated tokens (e.g., <think> and </think>) to separate the thinking and response parts inthe final output.

• Preference Alignment: For open-ended queries, preference alignment focuses on improving themodel’s helpfulness, engagement, and style, ultimately delivering a more natural and satisfyinguser experience.

• Agent Ability: This involves training the model to correctly invoke tools via designated interfaces.During the RL rollout, the model is allowed to perform complete multi-turn interaction cycleswith real environment execution feedback, thereby improving its performance and stability inlong-horizon decision-making tasks.

• Abilities for Specialized Scenarios: In more specialized scenarios, we design tasks tailored to thespecific context. For example, in Retrieval-Augmented Generation (RAG) tasks, we incorporatereward signals to guide the model toward generating accurate and contextually appropriateresponses, thereby minimizing the risk of hallucination.

To provide feedback for the aforementioned tasks, we utilized three distinct types of rewards:

(1) Rule-based Reward: The rule-based reward has been widely used in the reasoning RL stage,and is also useful for general tasks such as instruction following (Lambert et al., 2024) and formatadherence. Well-designed rule-based rewards can assess the correctness of model outputs withhigh precision, preventing issues like reward hacking.

(2) Model-based Reward with Reference Answer: In this approach, we provide a reference answerfor each query and prompt Qwen2.5-72B-Instruct to score the model’s response based on thisreference. This method allows for more flexible handling of diverse tasks without requiring strictformatting, avoiding false negatives that can occur with purely rule-based rewards.

(3) Model-based Reward without Reference Answer: Leveraging human preference data, we traina reward model to assign scalar scores to model responses. This approach, which does notdepend on a reference answer, can handle a broader range of queries while effectively enhancingthe model’s engagement and helpfulness.

# 4.5 Strong-to-Weak Distillation

The Strong-to-Weak Distillation pipeline is specifically designed to optimize lightweight models, encom-passing 5 dense models (Qwen3-0.6B, 1.7B, 4B, 8B, and 14B) and one MoE model (Qwen3-30B-A3B). Thisapproach enhances model performance while effectively imparting robust mode-switching capabilities.The distillation process is divided into two primary phases:

(1) Off-policy Distillation: At this initial phase, we combine the outputs of teacher models generatedwith both /think and /no think modes for response distillation. This helps lightweight studentmodels develop basic reasoning skills and the ability to switch between different modes ofthinking, laying a solid foundation for the next on-policy training phase.

(2) On-policy Distillation: In this phase, the student model generates on-policy sequences forfine-tuning. Specifically, prompts are sampled, and the student model produces responses ineither /think or /no think mode. The student model is then fine-tuned by aligning its logitswith those of a teacher model (Qwen3-32B or Qwen3-235B-A22B) to minimize the KL divergence.

# 4.6 Post-training Evaluation

To comprehensively evaluate the quality of instruction-tuned models, we adopted automatic benchmarksto assess model performance under both thinking and non-thinking modes. These benchmarks are


Table 10: Multilingual benchmarks and the included languages. The languages are identified in IETFlanguage tags.


<table><tr><td>Benchmark</td><td># Langs</td><td>Languages</td></tr><tr><td>Multi-IF</td><td>8</td><td>en, es, fr, hi, it, pt, ru, zh</td></tr><tr><td>INCLUDE</td><td>44</td><td>ar, az, be, bg, bn, de, el, es, et, eu, fa, fi, fr, he, hi, hr, hu, hy, id, it, ja, ka, kk, ko, lt, mk, ml, ms, ne, nl, pl, pt, ru, sq, sr, ta, te, tl, tr, uk, ur, uz, vi, zh</td></tr><tr><td>MMMLU</td><td>14</td><td>ar, bn, de, en, es, fr, hi, id, it, ja, ko, pt, sw, zh</td></tr><tr><td>MT-AIME2024</td><td>55</td><td>af, ar, bg, bn, ca, cs, cy, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, kn, ko, lt, lv, mk, ml, mr, ne, nl, no, pa, pl, pt, ro, ru, sk, sl, so, sq, sv, sw, ta, te, th, tl, tr, uk, ur, vi, zh-Hans, zh-Hant</td></tr><tr><td>PolyMath</td><td>18</td><td>ar, bn, de, en, es, fr, id, it, ja, ko, ms, pt, ru, sw, te, th, vi, zh</td></tr><tr><td>MLogiQA</td><td>10</td><td>ar, en, es, fr, ja, ko, pt, th, vi, zh</td></tr></table>

categorized into several dimensions:

• General Tasks: We utilize benchmarks including MMLU-Redux (Gema et al., 2024), GPQA-Diamond (Rein et al., 2023), C-Eval (Huang et al., 2023), and LiveBench (2024-11-25) (White et al.,2024). For GPQA-Diamond, we sample 10 times for each query and report the averaged accuracy.

• Alignment Tasks: To evaluate how well the model aligns with human preferences, we employa suite of specialized benchmarks. For instruction-following performance, we report the strict-prompt accuracy of IFEval (Zhou et al., 2023). To assess alignment with human preferences ongeneral topics, we utilize Arena-Hard (Li et al., 2024) and AlignBench v1.1 (Liu et al., 2023b). Forwriting tasks, we rely on Creative Writing V3 (Paech, 2024) and WritingBench (Wu et al., 2025) toevaluate the model’s proficiency and creativity.

• Math & Text Reasoning: For evaluating mathematical and logical reasoning skills, we employhigh-level math benchmarks including MATH-500 (Lightman et al., 2023), AIME’24 and AIME’25(AIME, 2025), and text reasoning tasks including ZebraLogic (Lin et al., 2025) and AutoLogi(Zhu et al., 2025). For AIME problems, each year’s questions include Part I and Part II, totaling30 questions. For each question, we sample 64 times and take the average accuracy as the finalscore.

• Agent & Coding: To test the model’s proficiency in coding and agent-based tasks, we use BFCLv3 (Yan et al., 2024), LiveCodeBench (v5, 2024.10-2025.02) (Jain et al., 2024), and CodeforcesRatings from CodeElo (Quan et al., 2025). For BFCL, all Qwen3 models are evaluated using theFC format, and yarn was used to deploy the models to a context length of 64k for Multi-Turnevaluation. Some baselines are derived from the BFCL leaderboard, taking the higher scoresbetween FC and Prompt formats. For models not reported on the leaderboard, the Promptformats are evaluated. For LiveCodeBench, for the non-thinking mode, we use the officiallyrecommended prompt, while for the thinking mode, we adjust the prompt template to allowthe model to think more freely, by removing the restriction You will not return anythingexcept for the program. To evaluate the performance gap between models and competitiveprogramming experts, we use CodeForces to calculate Elo ratings. In our benchmark, eachproblem is solved by generating up to eight independent reasoning attempts.

• Multilingual Tasks: For multilingual capabilities, we evaluate four kinds of tasks: instructionfollowing, knowledge, mathematics, and logical reasoning. Instruction following is assessedusing Multi-IF (He et al., 2024), which focuses on 8 key languages. Knowledge assessmentconsisted of two types: regional knowledge evaluated through INCLUDE (Romanou et al.,2024), covering 44 languages, and general knowledge assessed with MMMLU (OpenAI, 2024)across 14 languages, excluding the unoptimized Yoruba language; for these two benchmarks,we sample only $1 \mathrm { \bar { 0 } \% }$ of the original data to improve evaluation efficiency. The mathematics taskemploy MT-AIME2024 (Son et al., 2025), encompassing 55 languages, and PolyMath (Wang et al.,2025), which includes 18 languages. Logical reasoning is evaluated using MlogiQA, covering 10languages, sourced from Zhang et al. (2024).

For all Qwen3 models in the thinking mode, we utilize a sampling temperature of 0.6, a top-p valueof 0.95, and a top-k value of 20. Additionally, for Creative Writing v3 and WritingBench, we apply apresence penalty of 1.5 to encourage the generation of more diverse content. For Qwen3 models in thenon-thinking mode, we configure the sampling hyperparameters with temperature $= 0 . 7$ , top- $\cdot \mathrm { p } = 0 . 8 ,$top- $\mathbf { \nabla } \cdot \mathbf { k } = 2 0$ , and presence penalty $= 1 . 5$ . For both the thinking and non-thinking modes, we set the maxoutput length to 32,768 tokens, except AIME’24 and AIME’25 where we extend this length to 38,912tokens to provide sufficient thinking space.


Table 11: Comparison among Qwen3-235B-A22B (Thinking) and other reasoning baselines. Thehighest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td colspan="2"></td><td>OpenAI-o1</td><td>DeepSeek-R1</td><td>Grok-3-Beta (Think)</td><td>Gemini2.5-Pro</td><td>Qwen3-235B-A22B</td></tr><tr><td rowspan="7">General Tasks</td><td>Architecture</td><td>-</td><td>MoE</td><td>-</td><td>-</td><td>MoE</td></tr><tr><td># Activated Params</td><td>-</td><td>37B</td><td>-</td><td>-</td><td>22B</td></tr><tr><td># Total Params</td><td>-</td><td>671B</td><td>-</td><td>-</td><td>235B</td></tr><tr><td>MMLU-Redux</td><td>92.8</td><td>92.9</td><td>-</td><td>93.7</td><td>92.7</td></tr><tr><td>GPQA-Diamond</td><td>78.0</td><td>71.5</td><td>80.2</td><td>84.0</td><td>71.1</td></tr><tr><td>C-Eval</td><td>85.5</td><td>91.8</td><td>-</td><td>82.9</td><td>89.6</td></tr><tr><td>LiveBench 2024-11-25</td><td>75.7</td><td>71.6</td><td>-</td><td>82.4</td><td>77.1</td></tr><tr><td rowspan="5">Alignment Tasks</td><td>IFEval strict prompt</td><td>92.6</td><td>83.3</td><td>-</td><td>89.5</td><td>83.4</td></tr><tr><td>Arena-Hard</td><td>92.1</td><td>92.3</td><td>-</td><td>96.4</td><td>95.6</td></tr><tr><td>AlignBench v1.1</td><td>8.86</td><td>8.76</td><td>-</td><td>9.03</td><td>8.94</td></tr><tr><td>Creative Writing v3</td><td>81.7</td><td>85.5</td><td>-</td><td>86.0</td><td>84.6</td></tr><tr><td>WritingBench</td><td>7.69</td><td>7.71</td><td>-</td><td>8.09</td><td>8.03</td></tr><tr><td rowspan="5">Math &amp; Text Reasoning</td><td>MATH-500</td><td>96.4</td><td>97.3</td><td></td><td>98.8</td><td>98.0</td></tr><tr><td>AIME&#x27;24</td><td>74.3</td><td>79.8</td><td>83.9</td><td>92.0</td><td>85.7</td></tr><tr><td>AIME&#x27;25</td><td>79.2</td><td>70.0</td><td>77.3</td><td>86.7</td><td>81.5</td></tr><tr><td>ZebraLogic</td><td>81.0</td><td>78.7</td><td>-</td><td>87.4</td><td>80.3</td></tr><tr><td>AutoLogi</td><td>79.8</td><td>86.1</td><td>-</td><td>85.4</td><td>89.0</td></tr><tr><td rowspan="3">Agent &amp; Coding</td><td>BFCL v3</td><td>67.8</td><td>56.9</td><td>-</td><td>62.9</td><td>70.8</td></tr><tr><td>LiveCodeBench v5</td><td>63.9</td><td>64.3</td><td>70.6</td><td>70.4</td><td>70.7</td></tr><tr><td>CodeForces (Rating / Percentile)</td><td>1891 / 96.7%</td><td>2029 / 98.1%</td><td>-</td><td>2001 / 97.9%</td><td>2056 / 98.2%</td></tr><tr><td rowspan="6">Multilingual Tasks</td><td>Multi-IF</td><td>48.8</td><td>67.7</td><td>-</td><td>77.8</td><td>71.9</td></tr><tr><td>INCLUDE</td><td>84.6</td><td>82.7</td><td>-</td><td>85.1</td><td>78.7</td></tr><tr><td>MMMLU 14 languages</td><td>88.4</td><td>86.4</td><td>-</td><td>86.9</td><td>84.3</td></tr><tr><td>MT-AIME2024</td><td>67.4</td><td>73.5</td><td>-</td><td>76.9</td><td>80.8</td></tr><tr><td>PolyMath</td><td>38.9</td><td>47.1</td><td>-</td><td>52.2</td><td>54.7</td></tr><tr><td>MLogiQA</td><td>75.5</td><td>73.8</td><td>-</td><td>75.6</td><td>77.1</td></tr></table>


Table 12: Comparison among Qwen3-235B-A22B (Non-thinking) and other non-reasoning baselines.The highest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td colspan="2"></td><td>GPT-4o
-2024-11-20</td><td>DeepSeek-V3</td><td>Qwen2.5-72B
-Instruct</td><td>LLaMA-4
-Maverick</td><td>Qwen3-235B-A22B</td></tr><tr><td></td><td>Architecture</td><td>-</td><td>MoE</td><td>Dense</td><td>MoE</td><td>MoE</td></tr><tr><td></td><td># Activated Params</td><td>-</td><td>37B</td><td>72B</td><td>17B</td><td>22B</td></tr><tr><td></td><td># Total Params</td><td>-</td><td>671B</td><td>72B</td><td>402B</td><td>235B</td></tr><tr><td rowspan="4">General Tasks</td><td>MMLU-Redux</td><td>87.0</td><td>89.1</td><td>86.8</td><td>91.8</td><td>89.2</td></tr><tr><td>GPQA-Diamond</td><td>46.0</td><td>59.1</td><td>49.0</td><td>69.8</td><td>62.9</td></tr><tr><td>C-Eval</td><td>75.5</td><td>86.5</td><td>84.7</td><td>83.5</td><td>86.1</td></tr><tr><td>LiveBench 2024-11-25</td><td>52.2</td><td>60.5</td><td>51.4</td><td>59.5</td><td>62.5</td></tr><tr><td rowspan="5">Alignment Tasks</td><td>IFEval strict prompt</td><td>86.5</td><td>86.1</td><td>84.1</td><td>86.7</td><td>83.2</td></tr><tr><td>Arena-Hard</td><td>85.3</td><td>85.5</td><td>81.2</td><td>82.7</td><td>96.1</td></tr><tr><td>AlignBench v1.1</td><td>8.42</td><td>8.64</td><td>7.89</td><td>7.97</td><td>8.91</td></tr><tr><td>Creative Writing v3</td><td>81.1</td><td>74.0</td><td>61.8</td><td>61.3</td><td>80.4</td></tr><tr><td>WritingBench</td><td>7.11</td><td>6.49</td><td>7.06</td><td>5.46</td><td>7.70</td></tr><tr><td rowspan="5">Math &amp; Text Reasoning</td><td>MATH-500</td><td>77.2</td><td>90.2</td><td>83.6</td><td>90.6</td><td>91.2</td></tr><tr><td>AIME&#x27;24</td><td>11.1</td><td>39.2</td><td>18.9</td><td>38.5</td><td>40.1</td></tr><tr><td>AIME&#x27;25</td><td>7.6</td><td>28.8</td><td>15.0</td><td>15.9</td><td>24.7</td></tr><tr><td>ZebraLogic</td><td>27.4</td><td>42.1</td><td>26.6</td><td>40.0</td><td>37.7</td></tr><tr><td>AutoLogi</td><td>65.9</td><td>76.1</td><td>66.1</td><td>75.2</td><td>83.3</td></tr><tr><td rowspan="3">Agent &amp; Coding</td><td>BFCL v3</td><td>72.5</td><td>57.6</td><td>63.4</td><td>52.9</td><td>68.0</td></tr><tr><td>LiveCodeBench v5</td><td>32.7</td><td>33.1</td><td>30.7</td><td>37.2</td><td>35.3</td></tr><tr><td>CodeForces (Rating / Percentile)</td><td>864 / 35.4%</td><td>1134 / 54.1%</td><td>859 / 35.0%</td><td>712 / 24.3%</td><td>1387 / 75.7%</td></tr><tr><td rowspan="6">Multilingual Tasks</td><td>Multi-IF</td><td>65.6</td><td>55.6</td><td>65.3</td><td>75.5</td><td>70.2</td></tr><tr><td>INCLUDE</td><td>78.8</td><td>76.7</td><td>69.6</td><td>80.9</td><td>75.6</td></tr><tr><td>MMMLU 14 languages</td><td>80.3</td><td>81.1</td><td>76.9</td><td>82.5</td><td>79.8</td></tr><tr><td>MT-AIME2024</td><td>9.2</td><td>20.9</td><td>12.7</td><td>27.0</td><td>32.4</td></tr><tr><td>PolyMath</td><td>13.7</td><td>20.4</td><td>16.9</td><td>26.1</td><td>27.0</td></tr><tr><td>MLogiQA</td><td>57.4</td><td>58.9</td><td>59.3</td><td>59.9</td><td>67.6</td></tr></table>

Summary of Evaluation Results From the evaluation results, we summarize several key conclusions ofthe finalized Qwen3 models as follows:

(1) Our flagship model, Qwen3-235B-A22B, demonstrates the state-of-the-art overall performanceamong open-source models in both the thinking and non-thinking modes, surpassing strongbaselines such as DeepSeek-R1 and DeepSeek-V3. Qwen3-235B-A22B is also highly competitiveto closed-source leading models, such as OpenAI-o1, Gemini2.5-Pro, and GPT-4o, showcasing itsprofound reasoning capabilities and comprehensive general abilities.

(2) Our flagship dense model, Qwen3-32B, outperforms our previous strongest reasoning model,QwQ-32B, in most of the benchmarks, and performs comparably to the closed-source OpenAI-o3-mini, indicating its compelling reasoning capabilities. Qwen3-32B is also remarkably performantin the non-thinking mode and surpasses our previous flagship non-reasoning dense model,Qwen2.5-72B-Instruct.

(3) Our lightweight models, including Qwen3-30B-A3B, Qwen3-14B, and other smaller dense ones,possess consistently superior performance to the open-source models with a close or largeramount of parameters, proving the success of our Strong-to-Weak Distillation approach.

The detailed results are as follows.

Qwen3-235B-A22B For our flagship model Qwen3-235B-A22B, we compare it with the leading reason-ing and non-reasoning models. For the thinking mode, we take OpenAI-o1 (OpenAI, 2024), DeepSeek-R1(Guo et al., 2025), Grok-3-Beta (Think) (xAI, 2025), and Gemini2.5-Pro (DeepMind, 2025) as the reasoningbaselines. For the non-thinking mode, we take GPT-4o-2024-11-20 (OpenAI, 2024), DeepSeek-V3 (Liuet al., 2024a), Qwen2.5-72B-Instruct (Yang et al., 2024b), and LLaMA-4-Maverick (Meta-AI, 2025) as thenon-reasoning baselines. We present the evaluation results in Table 11 and 12.

(1) From Table 11, with only $6 0 \%$ activated and $3 5 \%$ total parameters, Qwen3-235B-A22B (Thinking)outperforms DeepSeek-R1 on 17/23 the benchmarks, particularly on the reasoning-demandedtasks (e.g., mathematics, agent, and coding), demonstrating the state-of-the-art reasoning capabil-ities of Qwen3-235B-A22B among open-source models. Moreover, Qwen3-235B-A22B (Thinking)is also highly competitive to the closed-source OpenAI-o1, Grok-3-Beta (Think), and Gemini2.5-Pro, substantially narrowing the gap in the reasoning capabilities between open-source andclose-source models.

(2) From Table 12, Qwen3-235B-A22B (Non-thinking) exceeds the other leading open-source models,including DeepSeek-V3, LLaMA-4-Maverick, and our previous flagship model Qwen2.5-72B-Instruct, and also surpasses the closed-source GPT-4o-2024-11-20 in 18/23 the benchmarks,indicating its inherent strong capabilities even when not enhanced with the deliberate thinkingprocess.

Qwen3-32B For our flagship dense model, Qwen3-32B, we take DeepSeek-R1-Distill-Llama-70B, OpenAI-o3-mini (medium), and our previous strongest reasoning model, QwQ-32B (Qwen Team, 2025), as thebaselines in the thinking mode. We also take GPT-4o-mini-2024-07-18, LLaMA-4-Scout, and our previ-ous flagship model, Qwen2.5-72B-Instruct, as the baselines in the non-thinking mode. We present theevaluation results in Table 13 and 14.

(1) From Table 13, Qwen3-32B (Thinking) outperforms QwQ-32B on 17/23 the benchmarks, makingit the new state-of-the-art reasoning model at the sweet size of 32B. Moreover, Qwen3-32B (Think-ing) also competes with the closed-source OpenAI-o3-mini (medium) with better alignment andmultilingual performance.

(2) From Table 14, Qwen3-32B (Non-thinking) exhibits superior performance to all the baselineson almost all the benchmarks. Particularly, Qwen3-32B (Non-thinking) performs on par withQwen2.5-72B-Instruct on the general tasks with significant advantages on the alignment, multi-lingual, and reasoning-related tasks, again proving the fundamental improvements of Qwen3over our previous Qwen2.5 series models.

Qwen3-30B-A3B & Qwen3-14B For Qwen3-30B-A3B and Qwen3-14B, we compare them with DeepSeek-R1-Distill-Qwen-32B and QwQ-32B in the thinking mode, and Phi-4 (Abdin et al., 2024), Gemma-3-27B-IT(Team et al., 2025), and Qwen2.5-32B-Instruct in the non-thinking mode, respectively. We present theevaluation results in Table 15 and 16.

(1) From Table 15, Qwen3-30B-A3B and Qwen3-14B (Thinking) are both highly competitive toQwQ-32B, especially on the reasoning-related benchmarks. It is noteworthy that Qwen3-30B-A3B achieves comparable performance to QwQ-32B with a smaller model size and less than


Table 13: Comparison among Qwen3-32B (Thinking) and other reasoning baselines. The highest andsecond-best scores are shown in bold and underlined, respectively.


<table><tr><td colspan="2"></td><td>DeepSeek-R1-Distill-Llama-70B</td><td>QwQ-32B</td><td>OpenAI-o3-mini (medium)</td><td>Qwen3-32B</td></tr><tr><td rowspan="3"></td><td>Architecture</td><td>Dense</td><td>Dense</td><td>-</td><td>Dense</td></tr><tr><td># Activated Params</td><td>70B</td><td>32B</td><td>-</td><td>32B</td></tr><tr><td># Total Params</td><td>70B</td><td>32B</td><td>-</td><td>32B</td></tr><tr><td rowspan="4">General Tasks</td><td>MMLU-Redux</td><td>89.3</td><td>90.0</td><td>90.0</td><td>90.9</td></tr><tr><td>GPQA-Diamond</td><td>65.2</td><td>65.6</td><td>76.8</td><td>68.4</td></tr><tr><td>C-Eval</td><td>71.8</td><td>88.4</td><td>75.1</td><td>87.3</td></tr><tr><td>LiveBench 2024-11-25</td><td>54.5</td><td>72.0</td><td>70.0</td><td>74.9</td></tr><tr><td rowspan="5">Alignment Tasks</td><td>IFEval strict prompt</td><td>79.3</td><td>83.9</td><td>91.5</td><td>85.0</td></tr><tr><td>Arena-Hard</td><td>60.6</td><td>89.5</td><td>89.0</td><td>93.8</td></tr><tr><td>AlignBench v1.1</td><td>6.74</td><td>8.70</td><td>8.38</td><td>8.72</td></tr><tr><td>Creative Writing v3</td><td>62.1</td><td>82.4</td><td>74.8</td><td>81.0</td></tr><tr><td>WritingBench</td><td>6.08</td><td>7.86</td><td>7.52</td><td>7.90</td></tr><tr><td rowspan="5">Math &amp; Text Reasoning</td><td>MATH-500</td><td>94.5</td><td>98.0</td><td>98.0</td><td>97.2</td></tr><tr><td>AIME&#x27;24</td><td>70.0</td><td>79.5</td><td>79.6</td><td>81.4</td></tr><tr><td>AIME&#x27;25</td><td>56.3</td><td>69.5</td><td>74.8</td><td>72.9</td></tr><tr><td>ZebraLogic</td><td>71.3</td><td>76.8</td><td>88.9</td><td>88.8</td></tr><tr><td>AutoLogi</td><td>83.5</td><td>88.1</td><td>86.3</td><td>87.3</td></tr><tr><td rowspan="3">Agent &amp; Coding</td><td>BFCL v3</td><td>49.3</td><td>66.4</td><td>64.6</td><td>70.3</td></tr><tr><td>LiveCodeBench v5</td><td>54.5</td><td>62.7</td><td>66.3</td><td>65.7</td></tr><tr><td>CodeForces (Rating / Percentile)</td><td>1633 / 91.4%</td><td>1982 / 97.7%</td><td>2036 / 98.1%</td><td>1977 / 97.7%</td></tr><tr><td rowspan="6">Multilingual Tasks</td><td>Multi-IF</td><td>57.6</td><td>68.3</td><td>48.4</td><td>73.0</td></tr><tr><td>INCLUDE</td><td>62.1</td><td>69.7</td><td>73.1</td><td>73.7</td></tr><tr><td>MMMLU 14 languages</td><td>69.6</td><td>80.9</td><td>79.3</td><td>80.6</td></tr><tr><td>MT-AIME2024</td><td>29.3</td><td>68.0</td><td>73.9</td><td>75.0</td></tr><tr><td>PolyMath</td><td>29.4</td><td>45.9</td><td>38.6</td><td>47.4</td></tr><tr><td>MLogiQA</td><td>60.3</td><td>75.5</td><td>71.1</td><td>76.3</td></tr></table>


Table 14: Comparison among Qwen3-32B (Non-thinking) and other non-reasoning baselines. Thehighest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td></td><td></td><td>GPT-4o-mini
-2024-07-18</td><td>LLaMA-4
-Scout</td><td>Qwen2.5-72B
-Instruct</td><td>Qwen3-32B</td></tr><tr><td></td><td>Architecture</td><td>-</td><td>MoE</td><td>Dense</td><td>Dense</td></tr><tr><td></td><td># Activated Params</td><td>-</td><td>17B</td><td>72B</td><td>32B</td></tr><tr><td></td><td># Total Params</td><td>-</td><td>109B</td><td>72B</td><td>32B</td></tr><tr><td rowspan="4">General Tasks</td><td>MMLU-Redux</td><td>81.5</td><td>86.3</td><td>86.8</td><td>85.7</td></tr><tr><td>GPQA-Diamond</td><td>40.2</td><td>57.2</td><td>49.0</td><td>54.6</td></tr><tr><td>C-Eval</td><td>66.3</td><td>78.2</td><td>84.7</td><td>83.3</td></tr><tr><td>LiveBench 2024-11-25</td><td>41.3</td><td>47.6</td><td>51.4</td><td>59.8</td></tr><tr><td rowspan="5">Alignment Tasks</td><td>IFEval strict prompt</td><td>80.4</td><td>84.7</td><td>84.1</td><td>83.2</td></tr><tr><td>Arena-Hard</td><td>74.9</td><td>70.5</td><td>81.2</td><td>92.8</td></tr><tr><td>AlignBench v1.1</td><td>7.81</td><td>7.49</td><td>7.89</td><td>8.58</td></tr><tr><td>Creative Writing v3</td><td>70.3</td><td>55.0</td><td>61.8</td><td>78.3</td></tr><tr><td>WritingBench</td><td>5.98</td><td>5.49</td><td>7.06</td><td>7.54</td></tr><tr><td rowspan="5">Math &amp; Text Reasoning</td><td>MATH-500</td><td>78.2</td><td>82.6</td><td>83.6</td><td>88.6</td></tr><tr><td>AIME&#x27;24</td><td>8.1</td><td>28.6</td><td>18.9</td><td>31.0</td></tr><tr><td>AIME&#x27;25</td><td>8.8</td><td>10.0</td><td>15.0</td><td>20.2</td></tr><tr><td>ZebraLogic</td><td>20.1</td><td>24.2</td><td>26.6</td><td>29.2</td></tr><tr><td>AutoLogi</td><td>52.6</td><td>56.8</td><td>66.1</td><td>78.5</td></tr><tr><td rowspan="3">Agent &amp; Coding</td><td>BFCL v3</td><td>64.0</td><td>45.4</td><td>63.4</td><td>63.0</td></tr><tr><td>LiveCodeBench v5</td><td>27.9</td><td>29.8</td><td>30.7</td><td>31.3</td></tr><tr><td>CodeForces (Rating / Percentile)</td><td>1113 / 52.6%</td><td>981 / 43.7%</td><td>859 / 35.0%</td><td>1353 / 71.0%</td></tr><tr><td rowspan="6">Multilingual Tasks</td><td>Multi-IF</td><td>62.4</td><td>64.2</td><td>65.3</td><td>70.7</td></tr><tr><td>INCLUDE</td><td>66.0</td><td>74.1</td><td>69.6</td><td>70.9</td></tr><tr><td>MMMLU 14 languages</td><td>72.1</td><td>77.5</td><td>76.9</td><td>76.5</td></tr><tr><td>MT-AIME2024</td><td>6.0</td><td>19.1</td><td>12.7</td><td>24.1</td></tr><tr><td>PolyMath</td><td>12.0</td><td>20.9</td><td>16.9</td><td>22.5</td></tr><tr><td>MLogiQA</td><td>42.6</td><td>53.9</td><td>59.3</td><td>62.9</td></tr></table>


Table 15: Comparison among Qwen3-30B-A3B / Qwen3-14B (Thinking) and other reasoning baselines.The highest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td colspan="2"></td><td>DeepSeek-R1-Distill-Qwen-32B</td><td>QwQ-32B</td><td>Qwen3-14B</td><td>Qwen3-30B-A3B</td></tr><tr><td rowspan="3"></td><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td># Activated Params</td><td>32B</td><td>32B</td><td>14B</td><td>3B</td></tr><tr><td># Total Params</td><td>32B</td><td>32B</td><td>14B</td><td>30B</td></tr><tr><td rowspan="4">General Tasks</td><td>MMLU-Redux</td><td>88.2</td><td>90.0</td><td>88.6</td><td>89.5</td></tr><tr><td>GPQA-Diamond</td><td>62.1</td><td>65.6</td><td>64.0</td><td>65.8</td></tr><tr><td>C-Eval</td><td>82.2</td><td>88.4</td><td>86.2</td><td>86.6</td></tr><tr><td>LiveBench 2024-11-25</td><td>45.6</td><td>72.0</td><td>71.3</td><td>74.3</td></tr><tr><td rowspan="5">Alignment Tasks</td><td>IFEval strict prompt</td><td>72.5</td><td>83.9</td><td>85.4</td><td>86.5</td></tr><tr><td>Arena-Hard</td><td>60.8</td><td>89.5</td><td>91.7</td><td>91.0</td></tr><tr><td>AlignBench v1.1</td><td>7.25</td><td>8.70</td><td>8.56</td><td>8.70</td></tr><tr><td>Creative Writing v3</td><td>55.0</td><td>82.4</td><td>80.3</td><td>79.1</td></tr><tr><td>WritingBench</td><td>6.13</td><td>7.86</td><td>7.80</td><td>7.70</td></tr><tr><td rowspan="5">Math &amp; Text Reasoning</td><td>MATH-500</td><td>94.3</td><td>98.0</td><td>96.8</td><td>98.0</td></tr><tr><td>AIME&#x27;24</td><td>72.6</td><td>79.5</td><td>79.3</td><td>80.4</td></tr><tr><td>AIME&#x27;25</td><td>49.6</td><td>69.5</td><td>70.4</td><td>70.9</td></tr><tr><td>ZebraLogic</td><td>69.6</td><td>76.8</td><td>88.5</td><td>89.5</td></tr><tr><td>AutoLogi</td><td>74.6</td><td>88.1</td><td>89.2</td><td>88.7</td></tr><tr><td rowspan="3">Agent &amp; Coding</td><td>BFCL v3</td><td>53.5</td><td>66.4</td><td>70.4</td><td>69.1</td></tr><tr><td>LiveCodeBench v5</td><td>54.5</td><td>62.7</td><td>63.5</td><td>62.6</td></tr><tr><td>CodeForces (Rating / Percentile)</td><td>1691 / 93.4%</td><td>1982 / 97.7%</td><td>1766 / 95.3%</td><td>1974 / 97.7%</td></tr><tr><td rowspan="6">Multilingual Tasks</td><td>Multi-IF</td><td>31.3</td><td>68.3</td><td>74.8</td><td>72.2</td></tr><tr><td>INCLUDE</td><td>68.0</td><td>69.7</td><td>71.7</td><td>71.9</td></tr><tr><td>MMMLU 14 languages</td><td>78.6</td><td>80.9</td><td>77.9</td><td>78.4</td></tr><tr><td>MT-AIME2024</td><td>44.6</td><td>68.0</td><td>73.3</td><td>73.9</td></tr><tr><td>PolyMath</td><td>35.1</td><td>45.9</td><td>45.8</td><td>46.1</td></tr><tr><td>MLogiQA</td><td>63.3</td><td>75.5</td><td>71.1</td><td>70.1</td></tr></table>


Table 16: Comparison among Qwen3-30B-A3B / Qwen3-14B (Non-thinking) and other non-reasoningbaselines. The highest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td colspan="2"></td><td>Phi-4</td><td>Gemma-3 -27B-IT</td><td>Qwen2.5-32B -Instruct</td><td>Qwen3-14B</td><td>Qwen3-30B-A3B</td></tr><tr><td rowspan="3"></td><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td># Activated Params</td><td>14B</td><td>27B</td><td>32B</td><td>14B</td><td>3B</td></tr><tr><td># Total Params</td><td>14B</td><td>27B</td><td>32B</td><td>14B</td><td>30B</td></tr><tr><td rowspan="4">General Tasks</td><td>MMLU-Redux</td><td>85.3</td><td>82.6</td><td>83.9</td><td>82.0</td><td>84.1</td></tr><tr><td>GPQA-Diamond</td><td>56.1</td><td>42.4</td><td>49.5</td><td>54.8</td><td>54.8</td></tr><tr><td>C-Eval</td><td>66.9</td><td>66.6</td><td>80.6</td><td>81.0</td><td>82.9</td></tr><tr><td>LiveBench 2024-11-25</td><td>41.6</td><td>49.2</td><td>50.0</td><td>59.6</td><td>59.4</td></tr><tr><td rowspan="5">Alignment Tasks</td><td>IFEval strict prompt</td><td>62.1</td><td>80.6</td><td>79.5</td><td>84.8</td><td>83.7</td></tr><tr><td>Arena-Hard</td><td>75.4</td><td>86.8</td><td>74.5</td><td>86.3</td><td>88.0</td></tr><tr><td>AlignBench v1.1</td><td>7.61</td><td>7.80</td><td>7.71</td><td>8.52</td><td>8.55</td></tr><tr><td>Creative Writing v3</td><td>51.2</td><td>82.0</td><td>54.6</td><td>73.1</td><td>68.1</td></tr><tr><td>WritingBench</td><td>5.73</td><td>7.22</td><td>5.90</td><td>7.24</td><td>7.22</td></tr><tr><td rowspan="5">Math &amp; Text Reasoning</td><td>MATH-500</td><td>80.8</td><td>90.0</td><td>84.6</td><td>90.0</td><td>89.8</td></tr><tr><td>AIME&#x27;24</td><td>22.9</td><td>32.6</td><td>18.8</td><td>31.7</td><td>32.8</td></tr><tr><td>AIME&#x27;25</td><td>17.3</td><td>24.0</td><td>12.8</td><td>23.3</td><td>21.6</td></tr><tr><td>ZebraLogic</td><td>32.3</td><td>24.6</td><td>26.1</td><td>33.0</td><td>33.2</td></tr><tr><td>AutoLogi</td><td>66.2</td><td>64.2</td><td>65.5</td><td>82.0</td><td>81.5</td></tr><tr><td rowspan="3">Agent &amp; Coding</td><td>BFCL v3</td><td>47.0</td><td>59.1</td><td>62.8</td><td>61.5</td><td>58.6</td></tr><tr><td>LiveCodeBench v5</td><td>25.2</td><td>26.9</td><td>26.4</td><td>29.0</td><td>29.8</td></tr><tr><td>CodeForces (Rating / Percentile)</td><td>1280 / 65.3%</td><td>1063 / 49.3%</td><td>903 / 38.2%</td><td>1200 / 58.6%</td><td>1267 / 64.1%</td></tr><tr><td rowspan="6">Multilingual Tasks</td><td>Multi-IF</td><td>49.5</td><td>69.8</td><td>63.2</td><td>72.9</td><td>70.8</td></tr><tr><td>INCLUDE</td><td>65.3</td><td>71.4</td><td>67.5</td><td>67.8</td><td>67.8</td></tr><tr><td>MMMLU 14 languages</td><td>74.7</td><td>76.1</td><td>74.2</td><td>72.6</td><td>73.8</td></tr><tr><td>MT-AIME2024</td><td>13.1</td><td>23.0</td><td>15.3</td><td>23.2</td><td>24.6</td></tr><tr><td>PolyMath</td><td>17.4</td><td>20.3</td><td>18.3</td><td>22.0</td><td>23.3</td></tr><tr><td>MLogiQA</td><td>53.1</td><td>58.5</td><td>58.0</td><td>58.9</td><td>53.3</td></tr></table>


Table 17: Comparison among Qwen3-8B / Qwen3-4B (Thinking) and other reasoning baselines. Thehighest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td></td><td></td><td>DeepSeek-R1
-Distill-Qwen-14B</td><td>DeepSeek-R1
-Distill-Qwen-32B</td><td>Qwen3-4B</td><td>Qwen3-8B</td></tr><tr><td></td><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td></tr><tr><td></td><td># Activated Params</td><td>14B</td><td>32B</td><td>4B</td><td>8B</td></tr><tr><td></td><td># Total Params</td><td>14B</td><td>32B</td><td>4B</td><td>8B</td></tr><tr><td rowspan="4">General Tasks</td><td>MMLU-Redux</td><td>84.1</td><td>88.2</td><td>83.7</td><td>87.5</td></tr><tr><td>GPQA-Diamond</td><td>59.1</td><td>62.1</td><td>55.9</td><td>62.0</td></tr><tr><td>C-Eval</td><td>78.1</td><td>82.2</td><td>77.5</td><td>83.4</td></tr><tr><td>LiveBench 2024-11-25</td><td>52.3</td><td>45.6</td><td>63.6</td><td>67.1</td></tr><tr><td rowspan="5">Alignment Tasks</td><td>IFEval strict prompt</td><td>72.6</td><td>72.5</td><td>81.9</td><td>85.0</td></tr><tr><td>Arena-Hard</td><td>48.0</td><td>60.8</td><td>76.6</td><td>85.8</td></tr><tr><td>AlignBench v1.1</td><td>7.43</td><td>7.25</td><td>8.30</td><td>8.46</td></tr><tr><td>Creative Writing v3</td><td>54.2</td><td>55.0</td><td>61.1</td><td>75.0</td></tr><tr><td>WritingBench</td><td>6.03</td><td>6.13</td><td>7.35</td><td>7.59</td></tr><tr><td rowspan="5">Math &amp; Text Reasoning</td><td>MATH-500</td><td>93.9</td><td>94.3</td><td>97.0</td><td>97.4</td></tr><tr><td>AIME&#x27;24</td><td>69.7</td><td>72.6</td><td>73.8</td><td>76.0</td></tr><tr><td>AIME&#x27;25</td><td>44.5</td><td>49.6</td><td>65.6</td><td>67.3</td></tr><tr><td>ZebraLogic</td><td>59.1</td><td>69.6</td><td>81.0</td><td>84.8</td></tr><tr><td>AutoLogi</td><td>78.6</td><td>74.6</td><td>87.9</td><td>89.1</td></tr><tr><td rowspan="3">Agent &amp; Coding</td><td>BFCL v3</td><td>49.5</td><td>53.5</td><td>65.9</td><td>68.1</td></tr><tr><td>LiveCodeBench v5</td><td>45.5</td><td>54.5</td><td>54.2</td><td>57.5</td></tr><tr><td>CodeForces (Rating / Percentile)</td><td>1574 / 89.1%</td><td>1691 / 93.4%</td><td>1671 / 92.8%</td><td>1785 / 95.6%</td></tr><tr><td rowspan="6">Multilingual Tasks</td><td>Multi-IF</td><td>29.8</td><td>31.3</td><td>66.3</td><td>71.2</td></tr><tr><td>INCLUDE</td><td>59.7</td><td>68.0</td><td>61.8</td><td>67.8</td></tr><tr><td>MMMLU 14 languages</td><td>73.8</td><td>78.6</td><td>69.8</td><td>74.4</td></tr><tr><td>MT-AIME2024</td><td>33.7</td><td>44.6</td><td>60.7</td><td>65.4</td></tr><tr><td>PolyMath</td><td>28.6</td><td>35.1</td><td>40.0</td><td>42.7</td></tr><tr><td>MLogiQA</td><td>53.6</td><td>63.3</td><td>65.9</td><td>69.0</td></tr></table>


Table 18: Comparison among Qwen3-8B / Qwen3-4B (Non-thinking) and other non-reasoning baselines.The highest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td colspan="2"></td><td>LLaMA-3.1-8B-Instruct</td><td>Gemma-3-12B-IT</td><td>Qwen2.5-7B-Instruct</td><td>Qwen2.5-14B-Instruct</td><td>Qwen3-4B</td><td>Qwen3-8B</td></tr><tr><td rowspan="3"></td><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td></tr><tr><td># Activated Params</td><td>8B</td><td>12B</td><td>7B</td><td>14B</td><td>4B</td><td>8B</td></tr><tr><td># Total Params</td><td>8B</td><td>12B</td><td>7B</td><td>14B</td><td>4B</td><td>8B</td></tr><tr><td rowspan="4">General Tasks</td><td>MMLU-Redux</td><td>61.7</td><td>77.8</td><td>75.4</td><td>80.0</td><td>77.3</td><td>79.5</td></tr><tr><td>GPQA-Diamond</td><td>32.8</td><td>40.9</td><td>36.4</td><td>45.5</td><td>41.7</td><td>39.3</td></tr><tr><td>C-Eval</td><td>52.0</td><td>61.1</td><td>76.2</td><td>78.0</td><td>72.2</td><td>77.9</td></tr><tr><td>LiveBench 2024-11-25</td><td>26.0</td><td>43.7</td><td>34.9</td><td>42.2</td><td>48.4</td><td>53.5</td></tr><tr><td rowspan="5">Alignment Tasks</td><td>IFEval strict prompt</td><td>75.0</td><td>80.2</td><td>71.2</td><td>81.0</td><td>81.2</td><td>83.0</td></tr><tr><td>Arena-Hard</td><td>30.1</td><td>82.6</td><td>52.0</td><td>68.3</td><td>66.2</td><td>79.6</td></tr><tr><td>AlignBench v1.1</td><td>6.01</td><td>7.77</td><td>7.27</td><td>7.67</td><td>8.10</td><td>8.38</td></tr><tr><td>Creative Writing v3</td><td>52.8</td><td>79.9</td><td>49.8</td><td>55.8</td><td>53.6</td><td>64.5</td></tr><tr><td>WritingBench</td><td>4.57</td><td>7.05</td><td>5.82</td><td>5.93</td><td>6.85</td><td>7.15</td></tr><tr><td rowspan="5">Math &amp; Text Reasoning</td><td>MATH-500</td><td>54.8</td><td>85.6</td><td>77.6</td><td>83.4</td><td>84.8</td><td>87.4</td></tr><tr><td>AIME&#x27;24</td><td>6.3</td><td>22.4</td><td>9.1</td><td>15.2</td><td>25.0</td><td>29.1</td></tr><tr><td>AIME&#x27;25</td><td>2.7</td><td>18.8</td><td>12.1</td><td>13.6</td><td>19.1</td><td>20.9</td></tr><tr><td>ZebraLogic</td><td>12.8</td><td>17.8</td><td>12.0</td><td>19.7</td><td>35.2</td><td>26.7</td></tr><tr><td>AutoLogi</td><td>30.9</td><td>58.9</td><td>42.9</td><td>57.4</td><td>76.3</td><td>76.5</td></tr><tr><td rowspan="3">Agent &amp; Coding</td><td>BFCL v3</td><td>49.6</td><td>50.6</td><td>55.8</td><td>58.7</td><td>57.6</td><td>60.2</td></tr><tr><td>LiveCodeBench v5</td><td>10.8</td><td>25.7</td><td>14.4</td><td>21.9</td><td>21.3</td><td>22.8</td></tr><tr><td>CodeForces (Rating / Percentile)</td><td>473 / 14.9%</td><td>462 / 14.7%</td><td>191 / 0.0%</td><td>904 / 38.3%</td><td>842 / 33.7%</td><td>1110 / 52.4%</td></tr><tr><td rowspan="6">Multilingual Tasks</td><td>Multi-IF</td><td>52.1</td><td>65.6</td><td>47.7</td><td>55.5</td><td>61.3</td><td>69.2</td></tr><tr><td>INCLUDE</td><td>34.0</td><td>65.3</td><td>53.6</td><td>63.5</td><td>53.8</td><td>62.5</td></tr><tr><td>MMMLU 14 languages</td><td>44.4</td><td>70.0</td><td>61.4</td><td>70.3</td><td>61.7</td><td>66.9</td></tr><tr><td>MT-AIME2024</td><td>0.4</td><td>16.7</td><td>5.5</td><td>8.5</td><td>13.9</td><td>16.6</td></tr><tr><td>PolyMath</td><td>5.8</td><td>17.6</td><td>11.9</td><td>15.0</td><td>16.6</td><td>18.8</td></tr><tr><td>MLogiQA</td><td>41.9</td><td>54.5</td><td>49.5</td><td>51.3</td><td>49.9</td><td>51.4</td></tr></table>


Table 19: Comparison among Qwen3-1.7B / Qwen3-0.6B (Thinking) and other reasoning baselines.The highest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td colspan="2"></td><td>DeepSeek-R1-Distill-Qwen-1.5B</td><td>DeepSeek-R1-Distill-Llama-8B</td><td>Qwen3-0.6B</td><td>Qwen3-1.7B</td></tr><tr><td rowspan="3"></td><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td></tr><tr><td># Activated Params</td><td>1.5B</td><td>8B</td><td>0.6B</td><td>1.7B</td></tr><tr><td># Total Params</td><td>1.5B</td><td>8B</td><td>0.6B</td><td>1.7B</td></tr><tr><td rowspan="4">General Tasks</td><td>MMLU-Redux</td><td>45.4</td><td>66.4</td><td>55.6</td><td>73.9</td></tr><tr><td>GPQA-Diamond</td><td>33.8</td><td>49.0</td><td>27.9</td><td>40.1</td></tr><tr><td>C-Eval</td><td>27.1</td><td>50.4</td><td>50.4</td><td>68.1</td></tr><tr><td>LiveBench 2024-11-25</td><td>24.9</td><td>40.6</td><td>30.3</td><td>51.1</td></tr><tr><td rowspan="5">Alignment Tasks</td><td>IFEval strict prompt</td><td>39.9</td><td>59.0</td><td>59.2</td><td>72.5</td></tr><tr><td>Arena-Hard</td><td>4.5</td><td>17.6</td><td>8.5</td><td>43.1</td></tr><tr><td>AlignBench v1.1</td><td>5.00</td><td>6.24</td><td>6.10</td><td>7.60</td></tr><tr><td>Creative Writing v3</td><td>16.4</td><td>51.1</td><td>30.6</td><td>48.0</td></tr><tr><td>WritingBench</td><td>4.03</td><td>5.42</td><td>5.61</td><td>7.02</td></tr><tr><td rowspan="5">Math &amp; Text Reasoning</td><td>MATH-500</td><td>83.9</td><td>89.1</td><td>77.6</td><td>93.4</td></tr><tr><td>AIME&#x27;24</td><td>28.9</td><td>50.4</td><td>10.7</td><td>48.3</td></tr><tr><td>AIME&#x27;25</td><td>22.8</td><td>27.8</td><td>15.1</td><td>36.8</td></tr><tr><td>ZebraLogic</td><td>4.9</td><td>37.1</td><td>30.3</td><td>63.2</td></tr><tr><td>AutoLogi</td><td>19.1</td><td>63.4</td><td>61.6</td><td>83.2</td></tr><tr><td rowspan="2">Agent &amp; Coding</td><td>BFCL v3</td><td>14.0</td><td>21.5</td><td>46.4</td><td>56.6</td></tr><tr><td>LiveCodeBench v5</td><td>13.2</td><td>42.5</td><td>12.3</td><td>33.2</td></tr><tr><td rowspan="6">Multilingual Tasks</td><td>Multi-IF</td><td>13.3</td><td>27.0</td><td>36.1</td><td>51.2</td></tr><tr><td>INCLUDE</td><td>21.9</td><td>34.5</td><td>35.9</td><td>51.8</td></tr><tr><td>MMMLU 14 languages</td><td>27.3</td><td>40.1</td><td>43.1</td><td>59.1</td></tr><tr><td>MT-AIME2024</td><td>12.4</td><td>13.2</td><td>7.8</td><td>36.1</td></tr><tr><td>PolyMath</td><td>14.5</td><td>10.8</td><td>11.4</td><td>25.2</td></tr><tr><td>MLogiQA</td><td>29.0</td><td>32.8</td><td>40.9</td><td>56.0</td></tr></table>


Table 20: Comparison among Qwen3-1.7B / Qwen3-0.6B (Non-thinking) and other non-reasoningbaselines. The highest and second-best scores are shown in bold and underlined, respectively.


<table><tr><td colspan="2"></td><td>Gemma-3 -1B-IT</td><td>Phi-4-mini</td><td>Qwen2.5-1.5B -Instruct</td><td>Qwen2.5-3B -Instruct</td><td>Qwen3-0.6B</td><td>Qwen3-1.7B</td></tr><tr><td rowspan="3"></td><td>Architecture</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td></tr><tr><td># Activated Params</td><td>1.0B</td><td>3.8B</td><td>1.5B</td><td>3.1B</td><td>0.6B</td><td>1.7B</td></tr><tr><td># Total Params</td><td>1.0B</td><td>3.8B</td><td>1.5B</td><td>3.1B</td><td>0.6B</td><td>1.7B</td></tr><tr><td rowspan="4">General Tasks</td><td>MMLU-Redux</td><td>33.3</td><td>67.9</td><td>50.7</td><td>64.4</td><td>44.6</td><td>64.4</td></tr><tr><td>GPQA-Diamond</td><td>19.2</td><td>25.2</td><td>29.8</td><td>30.3</td><td>22.9</td><td>28.6</td></tr><tr><td>C-Eval</td><td>28.5</td><td>40.0</td><td>53.3</td><td>68.2</td><td>42.6</td><td>61.0</td></tr><tr><td>LiveBench 2024-11-25</td><td>14.4</td><td>25.3</td><td>18.0</td><td>23.8</td><td>21.8</td><td>35.6</td></tr><tr><td rowspan="5">Alignment Tasks</td><td>IFEval strict prompt</td><td>54.5</td><td>68.6</td><td>42.5</td><td>58.2</td><td>54.5</td><td>68.2</td></tr><tr><td>Arena-Hard</td><td>17.8</td><td>32.8</td><td>9.0</td><td>23.7</td><td>6.5</td><td>36.9</td></tr><tr><td>AlignBench v1.1</td><td>5.3</td><td>6.00</td><td>5.60</td><td>6.49</td><td>5.60</td><td>7.20</td></tr><tr><td>Creative Writing v3</td><td>52.8</td><td>10.3</td><td>31.5</td><td>42.8</td><td>28.4</td><td>43.6</td></tr><tr><td>WritingBench</td><td>5.18</td><td>4.05</td><td>4.67</td><td>5.55</td><td>5.13</td><td>6.54</td></tr><tr><td rowspan="5">Math &amp; Text Reasoning</td><td>MATH-500</td><td>46.4</td><td>67.6</td><td>55.0</td><td>67.2</td><td>55.2</td><td>73.0</td></tr><tr><td>AIME&#x27;24</td><td>0.9</td><td>8.1</td><td>0.9</td><td>6.7</td><td>3.4</td><td>13.4</td></tr><tr><td>AIME&#x27;25</td><td>0.8</td><td>5.3</td><td>0.4</td><td>4.2</td><td>2.6</td><td>9.8</td></tr><tr><td>ZebraLogic</td><td>1.9</td><td>2.7</td><td>3.4</td><td>4.8</td><td>4.2</td><td>12.8</td></tr><tr><td>AutoLogi</td><td>16.4</td><td>28.8</td><td>22.5</td><td>29.9</td><td>37.4</td><td>59.8</td></tr><tr><td rowspan="2">Agent &amp; Coding</td><td>BFCL v3</td><td>16.3</td><td>31.3</td><td>47.8</td><td>50.4</td><td>44.1</td><td>52.2</td></tr><tr><td>LiveCodeBench v5</td><td>1.8</td><td>10.4</td><td>5.3</td><td>9.2</td><td>3.6</td><td>11.6</td></tr><tr><td rowspan="6">Multilingual Tasks</td><td>Multi-IF</td><td>32.8</td><td>40.5</td><td>20.2</td><td>32.3</td><td>33.3</td><td>44.7</td></tr><tr><td>INCLUDE</td><td>32.7</td><td>43.8</td><td>33.1</td><td>43.8</td><td>34.4</td><td>42.6</td></tr><tr><td>MMMLU 14 languages</td><td>32.5</td><td>51.4</td><td>40.4</td><td>51.8</td><td>37.1</td><td>48.3</td></tr><tr><td>MT-AIME2024</td><td>0.2</td><td>0.9</td><td>0.7</td><td>1.6</td><td>1.5</td><td>4.9</td></tr><tr><td>PolyMath</td><td>3.5</td><td>6.7</td><td>5.0</td><td>7.3</td><td>4.6</td><td>10.3</td></tr><tr><td>MLogiQA</td><td>31.8</td><td>39.5</td><td>40.9</td><td>39.5</td><td>37.3</td><td>41.1</td></tr></table>

1/10 activated parameters, demonstrating the effectiveness of our Strong-to-Weak Distillationapproach in endowing lightweight models with profound reasoning capabilities.

(2) From Table 16, Qwen3-30B-A3B and Qwen3-14B (Non-thinking) surpass the non-reasoningbaselines in most of the benchmarks. They exceed our previous Qwen2.5-32B-Instruct modelwith significantly fewer activated and total parameters, allowing for more efficient and cost-effective performance.

Qwen3-8B / 4B / 1.7B / 0.6B For Qwen3-8B and Qwen3-4B, we compare them with DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B in the thinking mode, and LLaMA-3.1-8B-Instruct (Dubeyet al., 2024), Gemma-3-12B-IT (Team et al., 2025), Qwen2.5-7B-Instruct, and Qwen2.5-14B-Instruct in thenon-thinking mode, respectively. For Qwen3-1.7B and Qwen3-0.6B, we compare them with DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Llama-8B in the thinking mode, and Gemma-3-1B-IT,Phi-4-mini, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct in the non-thinking mode, respectively. Wepresent the evaluation results of Qwen3-8B and Qwen3-4B in Table 17 and 18 and those of Qwen3-1.7Band Qwen3-0.6B in Table 19 and 20, respectively. Overall, these edge-side models exhibit impressiveperformance and outperform baselines even with more parameters, including our previous Qwen2.5models, in either the thinking or the non-thinking mode. These results, once again, demonstrate theefficacy of our Strong-to-Weak Distillation approach, making it possible for us to build the lightweightQwen3 models with remarkably reduced costs and efforts.

# 4.7 Discussion

The Effectiveness of Thinking Budget To verify that Qwen3 can enhance its intelligence level byleveraging an increased thinking budget, we adjust the allocated thinking budget on four benchmarksacross Mathematics, Coding, and STEM domains. The resulting scaling curves are presented in Figure 2,Qwen3 demonstrates scalable and smooth performance improvements correlated to the allocated thinkingbudget. Moreover, we observe that if we further extend the output length beyond 32K, the model’sperformance is expected to improve further in the future. We leave this exploration as future work.

![](images/0285cdda5fc4e92e5d6ff6d797f8e3e0eae6c38ee10aae4a8a19f98df4bbba04.jpg)


![](images/99f9d62ab4a95a7552a7a54ced9460ec156f8f845fd2ad488adfb6ce96337797.jpg)


![](images/0916f8673e2429f0bc56915b3bfa5914b5d7160f935dadd681e8879b38a2adb5.jpg)


![](images/be345f4fbd1f3236e492334da0279027344697400236a45b422859a241695a9e.jpg)



Figure 2: Performance of Qwen3-235B-A22B with respect to the thinking budget.


The Effectiveness and Efficiency of On-Policy Distillation We evaluate the effectiveness and efficiencyof on-policy distillation by comparing the performance and computational cost—measured in GPUhours—after undergoing distillation versus direct reinforcement learning, both starting from the sameoff-policy distilled 8B checkpoint. For simplicity, we focus solely on math and code-related queries in

this comparison. The results, summarized in Table 21, show that distillation achieves significantly betterperformance than reinforcement learning while requiring approximately only 1/10 of the GPU hours.Furthermore, distillation from teacher logits enables the student model to expand its exploration spaceand enhance its reasoning potential, as evidenced by the improved pass $@ \bar { 6 4 }$ scores on the AIME’24and AIME’25 benchmarks after distillation, compared to the initial checkpoint. In contrast, reinforce-ment learning does not lead to any improvement in pass $@ 6 4$ scores. These observations highlight theadvantages of leveraging a stronger teacher model in guiding student model learning.


Table 21: Comparison of reinforcement learning and on-policy distillation on Qwen3-8B. Numbers inparentheses indicate pass@64 scores.


<table><tr><td>Method</td><td>AIME&#x27;24</td><td>AIME&#x27;25</td><td>MATH500</td><td>LiveCodeBench v5</td><td>MMLU -Redux</td><td>GPQA -Diamond</td><td>GPU Hours</td></tr><tr><td>Off-policy Distillation</td><td>55.0 (90.0)</td><td>42.8 (83.3)</td><td>92.4</td><td>42.0</td><td>86.4</td><td>55.6</td><td>-</td></tr><tr><td>+ Reinforcement Learning</td><td>67.6 (90.0)</td><td>55.5 (83.3)</td><td>94.8</td><td>52.9</td><td>86.9</td><td>61.3</td><td>17,920</td></tr><tr><td>+ On-policy Distillation</td><td>74.4 (93.3)</td><td>65.5 (86.7)</td><td>97.0</td><td>60.3</td><td>88.3</td><td>63.3</td><td>1,800</td></tr></table>

The Effects of Thinking Mode Fusion and General RL To evaluate the effectiveness of Thinking ModeFusion and General Reinforcement Learning (RL) during the post-training, we conduct evaluations onvarious stages of the Qwen-32B model. In addition to the datasets mentioned earlier, we introduce severalin-house benchmarks to monitor other capabilities. These benchmarks include:

• CounterFactQA: Contains counterfactual questions where the model needs to identify that thequestions are not factual and avoid generating hallucinatory answers.

• LengthCtrl: Includes creative writing tasks with length requirements; the final score is based onthe difference between the generated content length and the target length.

• ThinkFollow: Involves multi-turn dialogues with randomly inserted /think and /no thinkflags to test whether the model can correctly switch thinking modes based on user queries.

• ToolUse: Evaluates the stability of the model in single-turn, multi-turn, and multi-step tool callingprocesses. The score includes accuracy in intent recognition, format accuracy, and parameteraccuracy during the tool calling process.


Table 22: Performance of Qwen3-32B after Reasoning RL (Stage 2), Thinking Mode Fusion (Stage 3), andGeneral RL (Stage 4). Benchmarks with * are in-house datasets.


<table><tr><td rowspan="2"></td><td rowspan="2">Benchmark</td><td>Stage 2 Reasoning RL</td><td colspan="2">Stage 3 Thinking Mode Fusion</td><td colspan="2">Stage 4 General RL</td></tr><tr><td>Thinking</td><td>Thinking</td><td>Non-Thinking</td><td>Thinking</td><td>Non-Thinking</td></tr><tr><td rowspan="3">General Tasks</td><td>LiveBench 2024-11-25</td><td>68.6</td><td>70.9+2.3</td><td>57.1</td><td>74.9+4.0</td><td>59.8+2.8</td></tr><tr><td>Arena-Hard</td><td>86.8</td><td>89.4+2.6</td><td>88.5</td><td>93.8+4.4</td><td>92.8+4.3</td></tr><tr><td>CounterFactQA*</td><td>50.4</td><td>61.3+10.9</td><td>64.3</td><td>68.1+6.8</td><td>66.4+2.1</td></tr><tr><td rowspan="4">Instruction &amp; Format Following</td><td>IFEval strict prompt</td><td>73.0</td><td>78.4+5.4</td><td>78.4</td><td>85.0+6.6</td><td>83.2+4.8</td></tr><tr><td>Multi-IF</td><td>61.4</td><td>64.6+3.2</td><td>65.2</td><td>73.0+8.4</td><td>70.7+5.5</td></tr><tr><td>LengthCtrl*</td><td>62.6</td><td>70.6+8.0</td><td>84.9</td><td>73.5+2.9</td><td>87.3+2.4</td></tr><tr><td>ThinkFollow*</td><td>-</td><td colspan="2">88.7</td><td colspan="2">98.9+10.2</td></tr><tr><td rowspan="2">Agent</td><td>BFCL v3</td><td>69.0</td><td>68.4-0.6</td><td>61.5</td><td>70.3+1.9</td><td>63.0+1.5</td></tr><tr><td>ToolUse*</td><td>63.3</td><td>70.4+7.1</td><td>73.2</td><td>85.5+15.1</td><td>86.5+13.3</td></tr><tr><td rowspan="2">Knowledge &amp; STEM</td><td>MMLU-Redux</td><td>91.4</td><td>91.0-0.4</td><td>86.7</td><td>90.9-0.1</td><td>85.7-1.0</td></tr><tr><td>GPQA-Diamond</td><td>68.8</td><td>69.0+0.2</td><td>50.4</td><td>68.4-0.6</td><td>54.6+4.3</td></tr><tr><td rowspan="2">Math &amp; Coding</td><td>AIME&#x27;24</td><td>83.8</td><td>81.9-1.9</td><td>28.5</td><td>81.4-0.5</td><td>31.0+2.5</td></tr><tr><td>LiveCodeBench v5</td><td>68.4</td><td>67.2-1.2</td><td>31.1</td><td>65.7-1.5</td><td>31.3+0.2</td></tr></table>

The results are shown in Table 22, where we can draw the following conclusions:

(1) Stage 3 integrates the non-thinking mode into the model, which already possesses thinkingcapabilities after the first two stages of training. The ThinkFollow benchmark score of 88.7indicates that the model has developed an initial ability to switch between modes, though it stilloccasionally makes errors. Stage 3 also enhances the model’s general and instruction-followingcapabilities in thinking mode, with CounterFactQA improving by 10.9 points and LengthCtrl by8.0 points.

(2) Stage 4 further strengthens the model’s general, instruction-following, and agent capabilitiesin both thinking and non-thinking modes. Notably, the ThinkFollow score improves to 98.9,ensuring accurate mode switching.

(3) For Knowledge, STEM, Math, and Coding tasks, Thinking Mode Fusion and General RL donot bring significant improvements. In contrast, for challenging tasks like AIME’24 and Live-CodeBench, the performance in thinking mode actually decreases after these two training stages.We conjecture this degradation is due to the model being trained on a broader range of generaltasks, which may compromise its specialized capabilities in handling complex problems. Duringthe development of Qwen3, we choose to accept this performance trade-off to enhance themodel’s overall versatility.

# 5 Conclusion

In this technical report, we introduce Qwen3, the latest version of the Qwen series. Qwen3 featuresboth thinking mode and non-thinking mode, allowing users to dynamically manage the number oftokens used for complex thinking tasks. The model was pre-trained on an extensive dataset containing36 trillion tokens, enabling it to understand and generate text in 119 languages and dialects. Through aseries of comprehensive evaluations, Qwen3 has shown strong performance across a range of standardbenchmarks for both pre-trained and post-trained models, including tasks related to code generation,mathematics, reasoning, and agents.

In the near future, our research will focus on several key areas. We will continue to scale up pretraining byusing data that is both higher in quality and more diverse in content. At the same time, we will work onimproving model architecture and training methods for the purposes of effective compression, scaling toextremely long contexts, etc. In addition, we plan to increase computational resources for reinforcementlearning, with a particular emphasis on agent-based RL systems that learn from environmental feedback.This will allow us to build agents capable of tackling complex tasks that require inference time scaling.

# 6 Authors

Core Contributors: An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, BowenYu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, FengHu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, LianghaoDeng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, ShixuanLiu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang,Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang,Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu

Contributors: Bei Chen, Biao Sun, Bin Luo, Bin Zhang, Binghai Wang, Bowen Ping, Boyi Deng, ChangSi, Chaojie Yang, Chen Cheng, Chenfei Wu, Chengpeng Li, Chengyuan Li, Fan Hong, Guobin Zhao,Hang Zhang, Hangrui Hu, Hanyu Zhao, Hao Lin, Hao Xiang, Haoyan Huang, Hongkun Hao, HumenZhong, Jialin Wang, Jiandong Jiang, Jianqiang Wan, Jianyuan Zeng, Jiawei Chen, Jie Zhang, Jin Xu, JinkaiWang, Jinyang Zhang, Jinzheng He, Jun Tang, Kai Zhang, Ke Yi, Keming Lu, Keqin Chen, Langshi Chen,Le Jiang, Lei Zhang, Linjuan Wu, Man Yuan, Mingkun Yang, Minmin Sun, Mouxiang Chen, Na Ni,Nuo Chen, Peng Liu, Peng Wang, Peng Zhu, Pengcheng Zhang, Pengfei Wang, Qiaoyu Tang, Qing Fu,Qiuyue Wang, Rong Zhang, Rui Hu, Runji Lin, Shen Huang, Shuai Bai, Shutong Jiang, Sibo Song, SiqiZhang, Song Chen, Tao He, Ting He, Tingfeng Hui, Wei Ding, Wei Liao, Wei Lin, Wei Zhang, WeijiaXu, Wenbin Ge, Wenmeng Zhou, Wenyuan Yu, Xianyan Jia, Xianzhong Shi, Xiaodong Deng, XiaomingHuang, Xiaoyuan Li, Ximing Zhou, Xinyao Niu, Xipin Wei, Xuejing Liu, Yang Liu, Yang Yao, Yang Zhang,Yanpeng Li, Yantao Liu, Yidan Zhang, Yikai Zhu, Yiming Wang, Yiwen Hu, Yong Jiang, Yong Li, YonganYue, Yu Guan, Yuanzhi Zhu, Yunfei Chu, Yunlong Feng, Yuxin Zhou, Yuxuan Cai, Zeyao Ma, Zhaohai Li,Zheng Li, Zhengyang Tang, Zheren Fu, Zhi Li, Zhibo Yang, Zhifang Guo, Zhipeng Zhang, Zhiying Xu,Zhiyu Yin, Zhongshen Zeng, Zile Qiao, Ziye Meng, Zongmeng Zhang

# A Appendix

# A.1 Additional Evaluation Results

# A.1.1 Long-Context Ability


Table 23: Performance of Qwen3 Models on the RULER benchmark.


<table><tr><td rowspan="2" colspan="2">Model</td><td colspan="7">RULER</td></tr><tr><td>Avg.</td><td>4K</td><td>8K</td><td>16K</td><td>32K</td><td>64K</td><td>128K</td></tr><tr><td></td><td>Qwen2.5-7B-Instruct</td><td>85.4</td><td>96.7</td><td>95.1</td><td>93.7</td><td>89.4</td><td>82.3</td><td>55.1</td></tr><tr><td></td><td>Qwen2.5-14B-Instruct</td><td>91.4</td><td>97.7</td><td>96.8</td><td>95.9</td><td>93.4</td><td>86.7</td><td>78.1</td></tr><tr><td></td><td>Qwen2.5-32B-Instruct</td><td>92.9</td><td>96.9</td><td>97.1</td><td>95.5</td><td>95.5</td><td>90.3</td><td>82.0</td></tr><tr><td></td><td>Qwen2.5-72B-Instruct</td><td>95.1</td><td>97.7</td><td>97.2</td><td>97.7</td><td>96.5</td><td>93.0</td><td>88.4</td></tr><tr><td rowspan="6">Non-thinking Mode</td><td>Qwen3-4B</td><td>85.2</td><td>95.1</td><td>93.6</td><td>91.0</td><td>87.8</td><td>77.8</td><td>66.0</td></tr><tr><td>Qwen3-8B</td><td>89.1</td><td>96.3</td><td>96.0</td><td>91.8</td><td>91.2</td><td>82.1</td><td>77.4</td></tr><tr><td>Qwen3-14B</td><td>94.6</td><td>98.0</td><td>97.8</td><td>96.4</td><td>96.1</td><td>94.0</td><td>85.1</td></tr><tr><td>Qwen3-32B</td><td>93.7</td><td>98.4</td><td>96.0</td><td>96.2</td><td>94.4</td><td>91.8</td><td>85.6</td></tr><tr><td>Qwen3-30B-A3B</td><td>91.6</td><td>96.5</td><td>97.0</td><td>95.3</td><td>92.4</td><td>89.1</td><td>79.2</td></tr><tr><td>Qwen3-235B-A22B</td><td>95.0</td><td>97.7</td><td>97.2</td><td>96.4</td><td>95.1</td><td>93.3</td><td>90.6</td></tr><tr><td rowspan="6">Thinking Mode</td><td>Qwen3-4B</td><td>83.5</td><td>92.7</td><td>88.7</td><td>86.5</td><td>83.2</td><td>83.0</td><td>67.2</td></tr><tr><td>Qwen3-8B</td><td>84.4</td><td>94.7</td><td>94.4</td><td>86.1</td><td>80.8</td><td>78.3</td><td>72.0</td></tr><tr><td>Qwen3-14B</td><td>90.1</td><td>95.4</td><td>93.6</td><td>89.8</td><td>91.9</td><td>90.6</td><td>79.0</td></tr><tr><td>Qwen3-32B</td><td>91.0</td><td>94.7</td><td>93.7</td><td>91.6</td><td>92.5</td><td>90.0</td><td>83.5</td></tr><tr><td>Qwen3-30B-A3B</td><td>86.6</td><td>94.1</td><td>92.7</td><td>89.0</td><td>86.6</td><td>82.1</td><td>75.0</td></tr><tr><td>Qwen3-235B-A22B</td><td>92.2</td><td>95.1</td><td>94.8</td><td>93.0</td><td>92.3</td><td>92.0</td><td>86.0</td></tr></table>

For evaluating long-context processing capabilities, we report the results on the RULER benchmark (Hsiehet al., 2024) in Table 23. To enable length extrapolation, we utilize YARN (Peng et al., 2023) with ascaling factor ${ \tt = } 4$ . In thinking mode, we set the thinking budget to 8192 tokens to mitigate overlyverbose reasoning on the extremely long inputs.

The results show that:

1. In non-thinking mode, Qwen3 outperforms Qwen2.5 models of a similar size in long-contextprocessing tasks.

2. In thinking mode, the model’s performance slightly degrades. We hypothesize that the thinkingcontent does not provide significant benefits for these retrieval tasks, which do not rely onreasoning and may instead interfere with the retrieval process. We are committed to enhancingthe long-context capability in the thinking mode in future versions.

# A.1.2 Multilingual Ability

Table 24-35 presents the detailed benchmark scores across various languages, including Spanish, French,Portuguese, Italian, Arabic, Japanese, Korean, Indonesian, Russian, Vietnamese, German, and Thai. Theresults of these tables demonstrate that the Qwen3 series models achieve competitive performance acrossall evaluated benchmarks, showcasing their strong multilingual capabilities.

To evaluate the performance of Qwen3 across a broader range of languages, we utilize Belebele (Bandarkaret al., 2023), a benchmark for natural language understanding. We conduct evaluations on 80 supportedlanguages from the benchmark, excluding 42 unoptimized languages, as shown in Table 36 (organizedby language family). The performance comparison between Qwen3 and other baseline models onthe Belebele benchmark is presented in Table 37. The results show that Qwen3 achieves comparableperformance to similarly-sized Gemma models while outperforming Qwen2.5 significantly.


Table 24: Benchmark scores for language: Spanish (es). The highest and second-best scores are shownin bold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>Multi-IF</td><td>MLogiQA</td><td>INCLUDE</td><td>MMMLU</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>80.1</td><td>70.0</td><td>96.4</td><td>88.7</td><td>90.0</td><td>54.4</td><td>79.9</td></tr><tr><td>QwQ-32B</td><td>70.0</td><td>75.0</td><td>81.8</td><td>84.5</td><td>76.7</td><td>52.2</td><td>73.4</td></tr><tr><td>Qwen3-235B-A22B</td><td>74.2</td><td>76.2</td><td>89.1</td><td>86.7</td><td>86.7</td><td>57.3</td><td>78.4</td></tr><tr><td>Qwen3-32B</td><td>74.7</td><td>68.8</td><td>90.9</td><td>82.8</td><td>76.7</td><td>51.8</td><td>74.3</td></tr><tr><td>Qwen3-30B-A3B</td><td>74.9</td><td>71.2</td><td>80.0</td><td>81.9</td><td>76.7</td><td>48.5</td><td>72.2</td></tr><tr><td>Qwen3-14B</td><td>76.2</td><td>67.5</td><td>83.6</td><td>81.1</td><td>73.3</td><td>50.3</td><td>72.0</td></tr><tr><td>Qwen3-8B</td><td>74.1</td><td>70.0</td><td>78.2</td><td>79.2</td><td>70.0</td><td>43.7</td><td>69.2</td></tr><tr><td>Qwen3-4B</td><td>69.1</td><td>68.8</td><td>72.7</td><td>75.7</td><td>66.7</td><td>42.3</td><td>65.9</td></tr><tr><td>Qwen3-1.7B</td><td>56.0</td><td>55.0</td><td>72.7</td><td>64.5</td><td>46.7</td><td>30.2</td><td>54.2</td></tr><tr><td>Qwen3-0.6B</td><td>39.2</td><td>42.5</td><td>54.5</td><td>48.8</td><td>13.3</td><td>14.3</td><td>35.4</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>67.5</td><td>52.5</td><td>89.1</td><td>80.6</td><td>10.0</td><td>15.5</td><td>52.5</td></tr><tr><td>Gemma-3-27b-IT</td><td>73.5</td><td>57.5</td><td>89.1</td><td>77.7</td><td>30.0</td><td>22.4</td><td>58.4</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>66.7</td><td>61.3</td><td>80.0</td><td>80.1</td><td>20.0</td><td>18.8</td><td>54.5</td></tr><tr><td>Qwen3-235B-A22B</td><td>71.7</td><td>66.2</td><td>83.6</td><td>83.7</td><td>33.3</td><td>29.5</td><td>61.3</td></tr><tr><td>Qwen3-32B</td><td>72.1</td><td>65.0</td><td>83.6</td><td>80.4</td><td>26.7</td><td>24.7</td><td>58.8</td></tr><tr><td>Qwen3-30B-A3B</td><td>72.1</td><td>53.8</td><td>85.5</td><td>78.3</td><td>33.3</td><td>25.0</td><td>58.0</td></tr><tr><td>Qwen3-14B</td><td>76.2</td><td>63.7</td><td>78.2</td><td>77.4</td><td>40.0</td><td>25.0</td><td>60.1</td></tr><tr><td>Qwen3-8B</td><td>73.1</td><td>50.0</td><td>80.0</td><td>73.7</td><td>16.7</td><td>21.3</td><td>52.5</td></tr><tr><td>Qwen3-4B</td><td>65.8</td><td>50.0</td><td>60.0</td><td>68.3</td><td>13.3</td><td>17.3</td><td>45.8</td></tr><tr><td>Qwen3-1.7B</td><td>47.9</td><td>43.8</td><td>50.9</td><td>54.3</td><td>10.0</td><td>11.6</td><td>36.4</td></tr><tr><td>Qwen3-0.6B</td><td>35.5</td><td>37.5</td><td>43.6</td><td>39.5</td><td>3.3</td><td>5.8</td><td>27.5</td></tr></table>


Table 25: Benchmark scores for language: French (fr). The highest and second-best scores are shown inbold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>Multi-IF</td><td>MLogiQA</td><td>INCLUDE</td><td>MMMLU</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>80.5</td><td>73.8</td><td>85.7</td><td>88.3</td><td>80.0</td><td>52.8</td><td>76.8</td></tr><tr><td>QwQ-32B</td><td>72.4</td><td>78.8</td><td>76.2</td><td>84.0</td><td>80.0</td><td>49.4</td><td>73.5</td></tr><tr><td>Qwen3-235B-A22B</td><td>77.3</td><td>78.8</td><td>85.7</td><td>86.6</td><td>86.7</td><td>57.4</td><td>78.8</td></tr><tr><td>Qwen3-32B</td><td>76.7</td><td>81.2</td><td>76.2</td><td>82.1</td><td>83.3</td><td>47.1</td><td>74.4</td></tr><tr><td>Qwen3-30B-A3B</td><td>75.2</td><td>67.5</td><td>83.3</td><td>81.0</td><td>76.7</td><td>46.9</td><td>71.8</td></tr><tr><td>Qwen3-14B</td><td>77.6</td><td>71.2</td><td>73.8</td><td>80.4</td><td>73.3</td><td>44.2</td><td>70.1</td></tr><tr><td>Qwen3-8B</td><td>73.8</td><td>66.2</td><td>85.7</td><td>77.9</td><td>70.0</td><td>45.3</td><td>69.8</td></tr><tr><td>Qwen3-4B</td><td>71.3</td><td>63.7</td><td>71.4</td><td>74.5</td><td>66.7</td><td>40.2</td><td>64.6</td></tr><tr><td>Qwen3-1.7B</td><td>52.6</td><td>56.2</td><td>54.8</td><td>64.8</td><td>60.0</td><td>28.7</td><td>52.8</td></tr><tr><td>Qwen3-0.6B</td><td>36.1</td><td>48.8</td><td>47.6</td><td>48.4</td><td>6.7</td><td>14.0</td><td>33.6</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>67.8</td><td>56.2</td><td>85.7</td><td>81.8</td><td>10.0</td><td>15.3</td><td>52.8</td></tr><tr><td>Gemma-3-27b-IT</td><td>73.9</td><td>57.5</td><td>73.8</td><td>78.3</td><td>23.3</td><td>21.5</td><td>54.7</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>72.1</td><td>55.0</td><td>81.0</td><td>80.2</td><td>26.7</td><td>15.7</td><td>55.1</td></tr><tr><td>Qwen3-235B-A22B</td><td>73.2</td><td>65.0</td><td>88.1</td><td>81.1</td><td>36.7</td><td>28.1</td><td>62.0</td></tr><tr><td>Qwen3-32B</td><td>75.8</td><td>60.0</td><td>73.8</td><td>79.5</td><td>30.0</td><td>23.0</td><td>57.0</td></tr><tr><td>Qwen3-30B-A3B</td><td>75.6</td><td>52.5</td><td>69.0</td><td>77.9</td><td>26.7</td><td>27.3</td><td>54.8</td></tr><tr><td>Qwen3-14B</td><td>78.4</td><td>63.7</td><td>73.8</td><td>75.1</td><td>33.3</td><td>24.4</td><td>58.1</td></tr><tr><td>Qwen3-8B</td><td>71.9</td><td>52.5</td><td>71.4</td><td>71.7</td><td>20.0</td><td>21.4</td><td>51.5</td></tr><tr><td>Qwen3-4B</td><td>64.2</td><td>47.5</td><td>61.9</td><td>67.6</td><td>20.0</td><td>19.2</td><td>46.7</td></tr><tr><td>Qwen3-1.7B</td><td>46.1</td><td>43.8</td><td>64.3</td><td>53.2</td><td>3.3</td><td>11.6</td><td>37.0</td></tr><tr><td>Qwen3-0.6B</td><td>32.8</td><td>35.0</td><td>38.1</td><td>39.4</td><td>6.7</td><td>4.6</td><td>26.1</td></tr></table>


Table 26: Benchmark scores for language: Portuguese (pt). The highest and second-best scores areshown in bold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>Multi-IF</td><td>MLogiQA</td><td>INCLUDE</td><td>MMMLU</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>80.5</td><td>73.8</td><td>83.9</td><td>88.9</td><td>73.3</td><td>52.2</td><td>75.4</td></tr><tr><td>QwQ-32B</td><td>70.5</td><td>70.0</td><td>80.4</td><td>84.0</td><td>80.0</td><td>48.7</td><td>72.3</td></tr><tr><td>Qwen3-235B-A22B</td><td>73.6</td><td>78.8</td><td>78.6</td><td>86.2</td><td>86.7</td><td>58.3</td><td>77.0</td></tr><tr><td>Qwen3-32B</td><td>74.1</td><td>76.2</td><td>76.8</td><td>82.6</td><td>80.0</td><td>52.4</td><td>73.7</td></tr><tr><td>Qwen3-30B-A3B</td><td>76.1</td><td>71.2</td><td>71.4</td><td>81.0</td><td>76.7</td><td>49.3</td><td>71.0</td></tr><tr><td>Qwen3-14B</td><td>77.3</td><td>68.8</td><td>75.0</td><td>81.6</td><td>83.3</td><td>46.7</td><td>72.1</td></tr><tr><td>Qwen3-8B</td><td>73.9</td><td>67.5</td><td>75.0</td><td>78.6</td><td>56.7</td><td>44.8</td><td>66.1</td></tr><tr><td>Qwen3-4B</td><td>70.6</td><td>62.5</td><td>71.4</td><td>75.1</td><td>73.3</td><td>44.2</td><td>66.2</td></tr><tr><td>Qwen3-1.7B</td><td>55.6</td><td>60.0</td><td>53.6</td><td>64.6</td><td>46.7</td><td>28.2</td><td>51.4</td></tr><tr><td>Qwen3-0.6B</td><td>38.7</td><td>33.8</td><td>42.9</td><td>47.5</td><td>10.0</td><td>12.7</td><td>30.9</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>66.8</td><td>57.5</td><td>78.6</td><td>80.7</td><td>10.0</td><td>15.0</td><td>51.4</td></tr><tr><td>Gemma-3-27b-IT</td><td>72.9</td><td>55.0</td><td>75.0</td><td>77.1</td><td>33.3</td><td>20.9</td><td>55.7</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>68.8</td><td>55.0</td><td>71.4</td><td>82.2</td><td>23.3</td><td>11.3</td><td>52.0</td></tr><tr><td>Qwen3-235B-A22B</td><td>72.5</td><td>67.5</td><td>82.1</td><td>83.5</td><td>33.3</td><td>28.3</td><td>61.2</td></tr><tr><td>Qwen3-32B</td><td>71.1</td><td>61.3</td><td>73.2</td><td>80.6</td><td>30.0</td><td>23.9</td><td>56.7</td></tr><tr><td>Qwen3-30B-A3B</td><td>72.3</td><td>47.5</td><td>67.9</td><td>77.8</td><td>26.7</td><td>24.0</td><td>52.7</td></tr><tr><td>Qwen3-14B</td><td>75.5</td><td>58.8</td><td>75.0</td><td>76.5</td><td>26.7</td><td>25.8</td><td>56.4</td></tr><tr><td>Qwen3-8B</td><td>71.9</td><td>56.2</td><td>71.4</td><td>72.9</td><td>20.0</td><td>19.7</td><td>52.0</td></tr><tr><td>Qwen3-4B</td><td>66.1</td><td>50.0</td><td>73.2</td><td>66.7</td><td>10.0</td><td>18.1</td><td>47.4</td></tr><tr><td>Qwen3-1.7B</td><td>49.5</td><td>33.8</td><td>39.3</td><td>52.9</td><td>6.7</td><td>12.8</td><td>32.5</td></tr><tr><td>Qwen3-0.6B</td><td>36.6</td><td>37.5</td><td>42.9</td><td>37.5</td><td>3.3</td><td>5.7</td><td>27.2</td></tr></table>


Table 27: Benchmark scores for language: Italian (it). The highest and second-best scores are shown inbold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>Multi-IF</td><td>INCLUDE</td><td>MMMLU</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>80.9</td><td>100.0</td><td>87.2</td><td>90.0</td><td>54.1</td><td>82.4</td></tr><tr><td>QwQ-32B</td><td>71.2</td><td>96.4</td><td>84.9</td><td>76.7</td><td>49.3</td><td>75.7</td></tr><tr><td>Qwen3-235B-A22B</td><td>73.7</td><td>96.4</td><td>85.7</td><td>80.0</td><td>57.4</td><td>78.6</td></tr><tr><td>Qwen3-32B</td><td>76.6</td><td>90.9</td><td>81.6</td><td>80.0</td><td>49.7</td><td>75.8</td></tr><tr><td>Qwen3-30B-A3B</td><td>75.9</td><td>94.5</td><td>81.9</td><td>80.0</td><td>48.1</td><td>76.1</td></tr><tr><td>Qwen3-14B</td><td>79.0</td><td>94.5</td><td>80.2</td><td>70.0</td><td>47.0</td><td>74.1</td></tr><tr><td>Qwen3-8B</td><td>74.6</td><td>89.1</td><td>77.5</td><td>76.7</td><td>46.1</td><td>72.8</td></tr><tr><td>Qwen3-4B</td><td>69.8</td><td>83.6</td><td>74.4</td><td>76.7</td><td>44.5</td><td>69.8</td></tr><tr><td>Qwen3-1.7B</td><td>54.6</td><td>74.5</td><td>64.2</td><td>53.3</td><td>29.6</td><td>55.2</td></tr><tr><td>Qwen3-0.6B</td><td>37.8</td><td>45.5</td><td>45.9</td><td>6.7</td><td>13.3</td><td>29.8</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>67.6</td><td>98.2</td><td>80.7</td><td>13.3</td><td>15.2</td><td>55.0</td></tr><tr><td>Gemma-3-27b-IT</td><td>74.6</td><td>90.9</td><td>78.4</td><td>23.3</td><td>20.5</td><td>57.5</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>67.2</td><td>94.5</td><td>80.7</td><td>16.7</td><td>16.7</td><td>55.2</td></tr><tr><td>Qwen3-235B-A22B</td><td>72.9</td><td>92.7</td><td>82.6</td><td>33.3</td><td>28.6</td><td>62.0</td></tr><tr><td>Qwen3-32B</td><td>71.4</td><td>92.7</td><td>79.5</td><td>30.0</td><td>23.0</td><td>59.3</td></tr><tr><td>Qwen3-30B-A3B</td><td>73.9</td><td>87.3</td><td>77.7</td><td>33.3</td><td>24.8</td><td>59.4</td></tr><tr><td>Qwen3-14B</td><td>75.8</td><td>89.1</td><td>75.7</td><td>26.7</td><td>27.6</td><td>59.0</td></tr><tr><td>Qwen3-8B</td><td>72.1</td><td>85.5</td><td>72.9</td><td>13.3</td><td>23.8</td><td>53.5</td></tr><tr><td>Qwen3-4B</td><td>63.0</td><td>78.2</td><td>67.8</td><td>23.3</td><td>19.3</td><td>50.3</td></tr><tr><td>Qwen3-1.7B</td><td>46.1</td><td>70.9</td><td>53.4</td><td>6.7</td><td>11.9</td><td>37.8</td></tr><tr><td>Qwen3-0.6B</td><td>35.1</td><td>43.6</td><td>39.0</td><td>0.0</td><td>4.5</td><td>24.4</td></tr></table>


Table 28: Benchmark scores for language: Arabic (ar). The highest and second-best scores are shown inbold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>MLogiQA</td><td>INCLUDE</td><td>MMMLU</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>75.0</td><td>89.3</td><td>87.8</td><td>76.7</td><td>52.6</td><td>76.3</td></tr><tr><td>QwQ-32B</td><td>75.0</td><td>67.9</td><td>81.8</td><td>80.0</td><td>41.3</td><td>69.2</td></tr><tr><td>Qwen3-235B-A22B</td><td>80.0</td><td>71.4</td><td>83.6</td><td>76.7</td><td>53.7</td><td>73.1</td></tr><tr><td>Qwen3-32B</td><td>66.2</td><td>73.2</td><td>80.1</td><td>86.7</td><td>47.0</td><td>70.6</td></tr><tr><td>Qwen3-30B-A3B</td><td>66.2</td><td>66.1</td><td>77.2</td><td>83.3</td><td>47.3</td><td>68.0</td></tr><tr><td>Qwen3-14B</td><td>71.2</td><td>67.9</td><td>77.4</td><td>83.3</td><td>46.6</td><td>69.3</td></tr><tr><td>Qwen3-8B</td><td>65.0</td><td>67.9</td><td>74.4</td><td>76.7</td><td>44.9</td><td>65.8</td></tr><tr><td>Qwen3-4B</td><td>62.5</td><td>55.4</td><td>67.7</td><td>66.7</td><td>41.2</td><td>58.7</td></tr><tr><td>Qwen3-1.7B</td><td>55.0</td><td>44.6</td><td>53.2</td><td>36.7</td><td>25.8</td><td>43.1</td></tr><tr><td>Qwen3-0.6B</td><td>40.0</td><td>41.1</td><td>38.9</td><td>10.0</td><td>11.7</td><td>28.3</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>51.2</td><td>78.6</td><td>80.9</td><td>13.3</td><td>12.9</td><td>47.4</td></tr><tr><td>Gemma-3-27b-IT</td><td>56.2</td><td>62.5</td><td>74.4</td><td>26.7</td><td>22.8</td><td>48.5</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>56.2</td><td>66.1</td><td>77.2</td><td>6.7</td><td>14.7</td><td>44.2</td></tr><tr><td>Qwen3-235B-A22B</td><td>66.2</td><td>67.9</td><td>79.5</td><td>40.0</td><td>28.2</td><td>56.4</td></tr><tr><td>Qwen3-32B</td><td>55.0</td><td>69.6</td><td>75.7</td><td>23.3</td><td>25.4</td><td>49.8</td></tr><tr><td>Qwen3-30B-A3B</td><td>48.8</td><td>64.3</td><td>71.6</td><td>30.0</td><td>22.6</td><td>47.5</td></tr><tr><td>Qwen3-14B</td><td>52.5</td><td>60.7</td><td>69.5</td><td>23.3</td><td>23.5</td><td>45.9</td></tr><tr><td>Qwen3-8B</td><td>45.0</td><td>58.9</td><td>64.6</td><td>13.3</td><td>16.4</td><td>39.6</td></tr><tr><td>Qwen3-4B</td><td>52.5</td><td>42.9</td><td>56.7</td><td>13.3</td><td>15.3</td><td>36.1</td></tr><tr><td>Qwen3-1.7B</td><td>31.2</td><td>37.5</td><td>43.6</td><td>3.3</td><td>9.4</td><td>25.0</td></tr><tr><td>Qwen3-0.6B</td><td>40.0</td><td>39.3</td><td>35.4</td><td>0.0</td><td>3.8</td><td>23.7</td></tr></table>


Table 29: Benchmark scores for language: Japanese (ja). The highest and second-best scores are shownin bold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>MLogiQA</td><td>INCLUDE</td><td>MMMLU</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>72.5</td><td>74.5</td><td>83.8</td><td>83.3</td><td>55.4</td><td>73.9</td></tr><tr><td>QwQ-32B</td><td>73.8</td><td>86.3</td><td>82.3</td><td>53.3</td><td>39.9</td><td>67.1</td></tr><tr><td>Qwen3-235B-A22B</td><td>75.0</td><td>94.1</td><td>84.8</td><td>73.3</td><td>52.7</td><td>76.0</td></tr><tr><td>Qwen3-32B</td><td>70.0</td><td>90.2</td><td>80.2</td><td>76.7</td><td>47.7</td><td>73.0</td></tr><tr><td>Qwen3-30B-A3B</td><td>66.2</td><td>88.2</td><td>79.9</td><td>73.3</td><td>47.4</td><td>71.0</td></tr><tr><td>Qwen3-14B</td><td>68.8</td><td>88.2</td><td>79.4</td><td>66.7</td><td>45.7</td><td>69.8</td></tr><tr><td>Qwen3-8B</td><td>71.2</td><td>86.3</td><td>74.9</td><td>73.3</td><td>44.7</td><td>70.1</td></tr><tr><td>Qwen3-4B</td><td>63.7</td><td>80.4</td><td>72.5</td><td>53.3</td><td>40.7</td><td>62.1</td></tr><tr><td>Qwen3-1.7B</td><td>53.8</td><td>74.5</td><td>61.8</td><td>36.7</td><td>28.5</td><td>51.1</td></tr><tr><td>Qwen3-0.6B</td><td>47.5</td><td>47.1</td><td>45.1</td><td>13.3</td><td>14.5</td><td>33.5</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>60.0</td><td>92.2</td><td>81.9</td><td>10.0</td><td>12.5</td><td>51.3</td></tr><tr><td>Gemma-3-27b-IT</td><td>66.2</td><td>86.3</td><td>76.5</td><td>20.0</td><td>17.3</td><td>53.3</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>55.0</td><td>94.1</td><td>77.7</td><td>16.7</td><td>17.7</td><td>52.2</td></tr><tr><td>Qwen3-235B-A22B</td><td>67.5</td><td>92.2</td><td>80.9</td><td>26.7</td><td>26.9</td><td>58.8</td></tr><tr><td>Qwen3-32B</td><td>58.8</td><td>92.2</td><td>78.0</td><td>20.0</td><td>20.5</td><td>53.9</td></tr><tr><td>Qwen3-30B-A3B</td><td>51.2</td><td>82.4</td><td>74.9</td><td>30.0</td><td>20.6</td><td>51.8</td></tr><tr><td>Qwen3-14B</td><td>55.0</td><td>84.3</td><td>73.8</td><td>33.3</td><td>19.8</td><td>53.2</td></tr><tr><td>Qwen3-8B</td><td>47.5</td><td>82.4</td><td>69.9</td><td>20.0</td><td>18.5</td><td>47.7</td></tr><tr><td>Qwen3-4B</td><td>46.2</td><td>76.5</td><td>64.8</td><td>13.3</td><td>15.1</td><td>43.2</td></tr><tr><td>Qwen3-1.7B</td><td>40.0</td><td>68.6</td><td>46.3</td><td>3.3</td><td>11.6</td><td>34.0</td></tr><tr><td>Qwen3-0.6B</td><td>37.5</td><td>37.3</td><td>37.9</td><td>3.3</td><td>3.7</td><td>23.9</td></tr></table>


Table 30: Benchmark scores for language: Korean (ko). The highest and second-best scores are shown inbold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>MLogiQA</td><td>INCLUDE</td><td>MMMLU</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>75.0</td><td>88.0</td><td>85.9</td><td>76.7</td><td>50.0</td><td>75.1</td></tr><tr><td>QwQ-32B</td><td>76.2</td><td>72.0</td><td>81.8</td><td>60.0</td><td>40.0</td><td>66.0</td></tr><tr><td>Qwen3-235B-A22B</td><td>71.2</td><td>80.0</td><td>84.7</td><td>80.0</td><td>55.7</td><td>74.3</td></tr><tr><td>Qwen3-32B</td><td>71.2</td><td>74.0</td><td>79.2</td><td>80.0</td><td>48.5</td><td>70.6</td></tr><tr><td>Qwen3-30B-A3B</td><td>68.8</td><td>72.0</td><td>78.6</td><td>76.7</td><td>46.6</td><td>68.5</td></tr><tr><td>Qwen3-14B</td><td>67.5</td><td>74.0</td><td>79.6</td><td>76.7</td><td>46.0</td><td>68.8</td></tr><tr><td>Qwen3-8B</td><td>60.0</td><td>80.0</td><td>74.7</td><td>76.7</td><td>42.3</td><td>66.7</td></tr><tr><td>Qwen3-4B</td><td>66.2</td><td>74.0</td><td>68.8</td><td>70.0</td><td>40.6</td><td>63.9</td></tr><tr><td>Qwen3-1.7B</td><td>53.8</td><td>66.0</td><td>57.8</td><td>43.3</td><td>25.2</td><td>49.2</td></tr><tr><td>Qwen3-0.6B</td><td>33.8</td><td>52.0</td><td>41.5</td><td>13.3</td><td>11.8</td><td>30.5</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>63.7</td><td>80.0</td><td>80.5</td><td>13.3</td><td>12.9</td><td>50.1</td></tr><tr><td>Gemma-3-27b-IT</td><td>58.8</td><td>76.0</td><td>75.9</td><td>20.0</td><td>18.3</td><td>49.8</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>58.8</td><td>68.0</td><td>76.7</td><td>6.7</td><td>17.7</td><td>45.6</td></tr><tr><td>Qwen3-235B-A22B</td><td>63.7</td><td>76.0</td><td>79.8</td><td>33.3</td><td>27.9</td><td>56.1</td></tr><tr><td>Qwen3-32B</td><td>60.0</td><td>74.0</td><td>77.2</td><td>26.7</td><td>21.2</td><td>51.8</td></tr><tr><td>Qwen3-30B-A3B</td><td>52.5</td><td>72.0</td><td>72.5</td><td>16.7</td><td>20.7</td><td>46.9</td></tr><tr><td>Qwen3-14B</td><td>52.5</td><td>68.0</td><td>73.3</td><td>20.0</td><td>18.7</td><td>46.5</td></tr><tr><td>Qwen3-8B</td><td>52.5</td><td>76.0</td><td>66.5</td><td>23.3</td><td>16.3</td><td>46.9</td></tr><tr><td>Qwen3-4B</td><td>46.2</td><td>74.0</td><td>59.9</td><td>13.3</td><td>16.6</td><td>42.0</td></tr><tr><td>Qwen3-1.7B</td><td>48.8</td><td>58.0</td><td>46.0</td><td>6.7</td><td>9.0</td><td>33.7</td></tr><tr><td>Qwen3-0.6B</td><td>40.0</td><td>52.0</td><td>36.9</td><td>0.0</td><td>5.5</td><td>26.9</td></tr></table>


Table 31: Benchmark scores for language: Indonesian (id). The highest and second-best scores areshown in bold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>INCLUDE</td><td>MMMLU</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>80.0</td><td>86.3</td><td>83.3</td><td>51.3</td><td>75.2</td></tr><tr><td>QwQ-32B</td><td>76.4</td><td>83.7</td><td>73.3</td><td>47.3</td><td>70.2</td></tr><tr><td>Qwen3-235B-A22B</td><td>80.0</td><td>87.2</td><td>80.0</td><td>53.5</td><td>75.2</td></tr><tr><td>Qwen3-32B</td><td>80.0</td><td>82.0</td><td>76.7</td><td>45.6</td><td>71.1</td></tr><tr><td>Qwen3-30B-A3B</td><td>81.8</td><td>80.4</td><td>80.0</td><td>44.9</td><td>71.8</td></tr><tr><td>Qwen3-14B</td><td>78.2</td><td>79.6</td><td>70.0</td><td>45.3</td><td>68.3</td></tr><tr><td>Qwen3-8B</td><td>72.7</td><td>77.7</td><td>70.0</td><td>43.8</td><td>66.0</td></tr><tr><td>Qwen3-4B</td><td>70.9</td><td>72.3</td><td>66.7</td><td>41.2</td><td>62.8</td></tr><tr><td>Qwen3-1.7B</td><td>63.6</td><td>61.2</td><td>36.7</td><td>26.8</td><td>47.1</td></tr><tr><td>Qwen3-0.6B</td><td>36.4</td><td>46.6</td><td>10.0</td><td>12.6</td><td>26.4</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>80.0</td><td>81.1</td><td>10.0</td><td>14.7</td><td>46.4</td></tr><tr><td>Gemma-3-27b-IT</td><td>76.4</td><td>75.9</td><td>13.3</td><td>22.6</td><td>47.0</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>74.5</td><td>78.8</td><td>10.0</td><td>16.6</td><td>45.0</td></tr><tr><td>Qwen3-235B-A22B</td><td>81.8</td><td>81.9</td><td>33.3</td><td>27.5</td><td>56.1</td></tr><tr><td>Qwen3-32B</td><td>81.8</td><td>77.2</td><td>23.3</td><td>24.3</td><td>51.6</td></tr><tr><td>Qwen3-30B-A3B</td><td>70.9</td><td>76.4</td><td>30.0</td><td>25.9</td><td>50.8</td></tr><tr><td>Qwen3-14B</td><td>70.9</td><td>74.1</td><td>26.7</td><td>24.6</td><td>49.1</td></tr><tr><td>Qwen3-8B</td><td>78.2</td><td>69.6</td><td>20.0</td><td>21.6</td><td>47.4</td></tr><tr><td>Qwen3-4B</td><td>67.3</td><td>66.5</td><td>13.3</td><td>19.0</td><td>41.5</td></tr><tr><td>Qwen3-1.7B</td><td>52.7</td><td>49.0</td><td>3.3</td><td>10.8</td><td>29.0</td></tr><tr><td>Qwen3-0.6B</td><td>52.7</td><td>40.0</td><td>3.3</td><td>5.1</td><td>25.3</td></tr></table>


Table 32: Benchmark scores for language: Russian (ru). The highest and second-best scores are shownin bold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>Multi-IF</td><td>INCLUDE</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>68.1</td><td>80.4</td><td>70.0</td><td>52.3</td><td>67.7</td></tr><tr><td>QwQ-32B</td><td>61.2</td><td>73.2</td><td>76.7</td><td>43.6</td><td>63.7</td></tr><tr><td>Qwen3-235B-A22B</td><td>62.2</td><td>80.4</td><td>80.0</td><td>53.1</td><td>68.9</td></tr><tr><td>Qwen3-32B</td><td>62.5</td><td>73.2</td><td>63.3</td><td>46.5</td><td>61.4</td></tr><tr><td>Qwen3-30B-A3B</td><td>60.7</td><td>76.8</td><td>73.3</td><td>45.4</td><td>64.0</td></tr><tr><td>Qwen3-14B</td><td>63.6</td><td>80.4</td><td>66.7</td><td>46.4</td><td>64.3</td></tr><tr><td>Qwen3-8B</td><td>62.9</td><td>69.6</td><td>63.3</td><td>37.7</td><td>58.4</td></tr><tr><td>Qwen3-4B</td><td>52.8</td><td>69.6</td><td>56.7</td><td>36.6</td><td>53.9</td></tr><tr><td>Qwen3-1.7B</td><td>37.8</td><td>46.4</td><td>20.0</td><td>22.8</td><td>31.8</td></tr><tr><td>Qwen3-0.6B</td><td>26.4</td><td>46.4</td><td>3.3</td><td>7.0</td><td>20.8</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>52.0</td><td>80.4</td><td>20.0</td><td>13.7</td><td>41.5</td></tr><tr><td>Gemma-3-27b-IT</td><td>57.3</td><td>71.4</td><td>23.3</td><td>21.6</td><td>43.4</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>54.1</td><td>67.9</td><td>20.0</td><td>13.3</td><td>38.8</td></tr><tr><td>Qwen3-235B-A22B</td><td>56.7</td><td>75.0</td><td>40.0</td><td>26.1</td><td>49.4</td></tr><tr><td>Qwen3-32B</td><td>58.6</td><td>71.4</td><td>30.0</td><td>23.3</td><td>45.8</td></tr><tr><td>Qwen3-30B-A3B</td><td>58.0</td><td>73.2</td><td>30.0</td><td>21.1</td><td>45.6</td></tr><tr><td>Qwen3-14B</td><td>60.3</td><td>71.4</td><td>26.7</td><td>24.2</td><td>45.6</td></tr><tr><td>Qwen3-8B</td><td>59.3</td><td>58.9</td><td>20.0</td><td>22.8</td><td>40.2</td></tr><tr><td>Qwen3-4B</td><td>46.1</td><td>58.9</td><td>13.3</td><td>17.8</td><td>34.0</td></tr><tr><td>Qwen3-1.7B</td><td>34.8</td><td>41.1</td><td>3.3</td><td>13.2</td><td>23.1</td></tr><tr><td>Qwen3-0.6B</td><td>25.5</td><td>46.4</td><td>0.0</td><td>5.8</td><td>19.4</td></tr></table>


Table 33: Benchmark scores for language: Vietnamese (vi). The highest and second-best scores areshown in bold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>MLogiQA</td><td>INCLUDE</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>72.5</td><td>89.1</td><td>70.0</td><td>52.1</td><td>70.9</td></tr><tr><td>QwQ-32B</td><td>71.2</td><td>69.1</td><td>70.0</td><td>49.2</td><td>64.9</td></tr><tr><td>Qwen3-235B-A22B</td><td>75.0</td><td>87.3</td><td>83.3</td><td>55.1</td><td>75.2</td></tr><tr><td>Qwen3-32B</td><td>67.5</td><td>81.8</td><td>83.3</td><td>44.0</td><td>69.2</td></tr><tr><td>Qwen3-30B-A3B</td><td>68.8</td><td>78.2</td><td>76.7</td><td>46.1</td><td>67.4</td></tr><tr><td>Qwen3-14B</td><td>72.5</td><td>72.7</td><td>73.3</td><td>45.8</td><td>66.1</td></tr><tr><td>Qwen3-8B</td><td>65.0</td><td>72.7</td><td>73.3</td><td>42.9</td><td>63.5</td></tr><tr><td>Qwen3-4B</td><td>68.8</td><td>63.6</td><td>60.0</td><td>42.2</td><td>58.6</td></tr><tr><td>Qwen3-1.7B</td><td>52.5</td><td>61.8</td><td>30.0</td><td>26.9</td><td>42.8</td></tr><tr><td>Qwen3-0.6B</td><td>33.8</td><td>38.2</td><td>6.7</td><td>9.8</td><td>22.1</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>57.5</td><td>81.8</td><td>10.0</td><td>13.0</td><td>40.6</td></tr><tr><td>Gemma-3-27b-IT</td><td>52.5</td><td>74.5</td><td>33.3</td><td>20.6</td><td>45.2</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>61.3</td><td>72.7</td><td>26.7</td><td>18.6</td><td>44.8</td></tr><tr><td>Qwen3-235B-A22B</td><td>70.0</td><td>83.6</td><td>36.7</td><td>27.1</td><td>54.4</td></tr><tr><td>Qwen3-32B</td><td>60.0</td><td>81.8</td><td>23.3</td><td>21.8</td><td>46.7</td></tr><tr><td>Qwen3-30B-A3B</td><td>52.5</td><td>81.8</td><td>20.0</td><td>24.7</td><td>44.8</td></tr><tr><td>Qwen3-14B</td><td>63.7</td><td>67.3</td><td>20.0</td><td>21.6</td><td>43.2</td></tr><tr><td>Qwen3-8B</td><td>48.8</td><td>65.5</td><td>20.0</td><td>19.1</td><td>38.4</td></tr><tr><td>Qwen3-4B</td><td>48.8</td><td>65.5</td><td>20.0</td><td>19.0</td><td>38.3</td></tr><tr><td>Qwen3-1.7B</td><td>36.2</td><td>60.0</td><td>3.3</td><td>10.9</td><td>27.6</td></tr><tr><td>Qwen3-0.6B</td><td>30.0</td><td>36.4</td><td>3.3</td><td>3.9</td><td>18.4</td></tr></table>


Table 34: Benchmark scores for language: German (de). The highest and second-best scores are shownin bold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>INCLUDE</td><td>MMMLU</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>50.0</td><td>85.6</td><td>86.7</td><td>53.8</td><td>69.0</td></tr><tr><td>QwQ-32B</td><td>57.1</td><td>83.8</td><td>76.7</td><td>51.0</td><td>67.2</td></tr><tr><td>Qwen3-235B-A22B</td><td>71.4</td><td>86.0</td><td>83.3</td><td>55.4</td><td>74.0</td></tr><tr><td>Qwen3-32B</td><td>64.3</td><td>81.9</td><td>86.7</td><td>48.1</td><td>70.2</td></tr><tr><td>Qwen3-30B-A3B</td><td>64.3</td><td>81.9</td><td>80.0</td><td>46.6</td><td>68.2</td></tr><tr><td>Qwen3-14B</td><td>57.1</td><td>80.9</td><td>70.0</td><td>48.1</td><td>64.0</td></tr><tr><td>Qwen3-8B</td><td>64.3</td><td>78.1</td><td>66.7</td><td>43.6</td><td>63.2</td></tr><tr><td>Qwen3-4B</td><td>57.1</td><td>74.0</td><td>73.3</td><td>43.1</td><td>61.9</td></tr><tr><td>Qwen3-1.7B</td><td>64.3</td><td>63.4</td><td>36.7</td><td>26.8</td><td>47.8</td></tr><tr><td>Qwen3-0.6B</td><td>57.1</td><td>47.6</td><td>10.0</td><td>13.7</td><td>32.1</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>57.1</td><td>80.4</td><td>10.0</td><td>13.5</td><td>40.2</td></tr><tr><td>Gemma-3-27b-IT</td><td>57.1</td><td>76.1</td><td>26.7</td><td>20.2</td><td>45.0</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>64.3</td><td>79.9</td><td>16.7</td><td>19.3</td><td>45.0</td></tr><tr><td>Qwen3-235B-A22B</td><td>71.4</td><td>81.7</td><td>40.0</td><td>25.9</td><td>54.8</td></tr><tr><td>Qwen3-32B</td><td>57.1</td><td>77.2</td><td>30.0</td><td>21.9</td><td>46.6</td></tr><tr><td>Qwen3-30B-A3B</td><td>57.1</td><td>77.7</td><td>23.3</td><td>25.2</td><td>45.8</td></tr><tr><td>Qwen3-14B</td><td>57.1</td><td>76.0</td><td>30.0</td><td>24.5</td><td>46.9</td></tr><tr><td>Qwen3-8B</td><td>64.3</td><td>70.8</td><td>20.0</td><td>19.9</td><td>43.8</td></tr><tr><td>Qwen3-4B</td><td>64.3</td><td>66.0</td><td>26.7</td><td>16.4</td><td>43.4</td></tr><tr><td>Qwen3-1.7B</td><td>42.9</td><td>53.2</td><td>10.0</td><td>10.6</td><td>29.2</td></tr><tr><td>Qwen3-0.6B</td><td>42.9</td><td>37.8</td><td>3.3</td><td>5.7</td><td>22.4</td></tr></table>


Table 35: Benchmark scores for language: Thai (th). The highest and second-best scores are shown inbold and underlined, respectively.


<table><tr><td></td><td>Model</td><td>MLogiQA</td><td>MT-AIME24</td><td>PolyMath</td><td>Average</td></tr><tr><td rowspan="10">Thinking Mode</td><td>Gemini2.5-Pro</td><td>73.8</td><td>80.0</td><td>50.7</td><td>68.2</td></tr><tr><td>QwQ-32B</td><td>75.0</td><td>60.0</td><td>41.3</td><td>58.8</td></tr><tr><td>Qwen3-235B-A22B</td><td>73.8</td><td>86.7</td><td>53.6</td><td>71.4</td></tr><tr><td>Qwen3-32B</td><td>73.8</td><td>76.7</td><td>46.9</td><td>65.8</td></tr><tr><td>Qwen3-30B-A3B</td><td>63.7</td><td>80.0</td><td>45.2</td><td>63.0</td></tr><tr><td>Qwen3-14B</td><td>65.0</td><td>76.7</td><td>44.4</td><td>62.0</td></tr><tr><td>Qwen3-8B</td><td>68.8</td><td>70.0</td><td>41.3</td><td>60.0</td></tr><tr><td>Qwen3-4B</td><td>60.0</td><td>60.0</td><td>39.4</td><td>53.1</td></tr><tr><td>Qwen3-1.7B</td><td>48.8</td><td>33.3</td><td>23.7</td><td>35.3</td></tr><tr><td>Qwen3-0.6B</td><td>33.8</td><td>13.3</td><td>11.4</td><td>19.5</td></tr><tr><td rowspan="11">Non-thinking Mode</td><td>GPT-4o-2024-1120</td><td>52.5</td><td>10.0</td><td>11.9</td><td>24.8</td></tr><tr><td>Gemma-3-27b-IT</td><td>50.0</td><td>16.7</td><td>19.0</td><td>28.6</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>58.8</td><td>6.7</td><td>17.4</td><td>27.6</td></tr><tr><td>Qwen3-235B-A22B</td><td>61.3</td><td>23.3</td><td>27.6</td><td>37.4</td></tr><tr><td>Qwen3-32B</td><td>61.3</td><td>13.3</td><td>22.2</td><td>32.3</td></tr><tr><td>Qwen3-30B-A3B</td><td>50.0</td><td>30.0</td><td>22.3</td><td>34.1</td></tr><tr><td>Qwen3-14B</td><td>47.5</td><td>23.3</td><td>22.1</td><td>31.0</td></tr><tr><td>Qwen3-8B</td><td>42.5</td><td>10.0</td><td>17.2</td><td>23.2</td></tr><tr><td>Qwen3-4B</td><td>43.8</td><td>13.3</td><td>16.1</td><td>24.4</td></tr><tr><td>Qwen3-1.7B</td><td>42.5</td><td>6.7</td><td>9.5</td><td>19.6</td></tr><tr><td>Qwen3-0.6B</td><td>37.5</td><td>0.0</td><td>3.6</td><td>13.7</td></tr></table>


Table 36: Language families and language codes supported by Qwen3 in Belebele Benchmark


<table><tr><td>Language family</td><td># Langs</td><td>Language code (ISO 639-3_ISO 15924)</td></tr><tr><td>Indo-European</td><td>40</td><td>por_Latn, deu_Latn, ttk_Cyrl, ces_Latn, nob_Latn, dan_Latn, snd_Arab, spa_Latn, isl_Latn, slv_Latn, eng_Latn, ory_Orya, hrv_Latn, ell_Grek, ukr_Cyrl, pan_Guru, srp_Cyrl, npi_Deva, mkd_Cyrl, guj_Gujr, nld_Latn, swe_Latn, hin_Deva, rus_Cyrl, asm_Beng, cat_Latn, als_Latn, sin_Sinh, urd_Arab, mar_Deva, lit_Latn, skl_Latn, ita_Latn, pol_Latn, bul_Cyrl, afr_Latn, ron_Latn, fra_Latn, ben_Beng, hye_Armn</td></tr><tr><td>Sino-Tibetan</td><td>3</td><td>zho_Hans, mya_Mymr, zho_Hant</td></tr><tr><td>Afro-Asiatic</td><td>8</td><td>heb_Hebr, apc_Arab, acm_Arab, ary_Arab, ars_Arab, arb_Arab, mlt_Latn, erz_Arab</td></tr><tr><td>Austronesian</td><td>7</td><td>ilo_Latn, ceb_Latn, tgl_Latn, sun_Latn, jav_Latn, war_Latn, ind_Latn</td></tr><tr><td>Dravidian</td><td>4</td><td>mal_Mlym, kan_Knda, tel_Telu, tam_Taml</td></tr><tr><td>Turkic</td><td>4</td><td>kaz_Cyrl, azj_Latn, tur_Latn, uzn_Latn</td></tr><tr><td>Tai-Kadai</td><td>2</td><td>tha_Thai, lao_Laoo</td></tr><tr><td>Uralic</td><td>3</td><td>fin_Latn, hun_Latn, est_Latn</td></tr><tr><td>Austroasiatic</td><td>2</td><td>vie_Latn, khm_Khmr</td></tr><tr><td>Other</td><td>7</td><td>eus_Latn, kor_Hang, hat_Latn, swh_Latn, kea_Latn, jpn_Jpan, kat_Geor</td></tr></table>


Table 37: Comparison of Belebele Benchmark performance between Qwen3 and other baseline models.Scores are highlighted with the highest in bold and the second-best underlined.


<table><tr><td>Model</td><td>Indo-European</td><td>Sino-Tibetan</td><td>Afro-Asiatic</td><td>Austronesian</td><td>Dravidian</td><td>Turkic</td><td>Tai-Kadai</td><td>Uralic</td><td>Austroasiatic</td><td>Other</td></tr><tr><td>Gemma-3-27B-IT</td><td>89.2</td><td>86.3</td><td>85.9</td><td>84.1</td><td>83.5</td><td>86.8</td><td>81.0</td><td>91.0</td><td>86.5</td><td>87.0</td></tr><tr><td>Qwen2.5-32B-Instruct</td><td>85.5</td><td>82.3</td><td>80.4</td><td>70.6</td><td>67.8</td><td>80.8</td><td>74.5</td><td>87.0</td><td>79.0</td><td>72.6</td></tr><tr><td>QwQ-32B</td><td>86.1</td><td>83.7</td><td>81.9</td><td>71.3</td><td>69.3</td><td>80.3</td><td>77.0</td><td>88.0</td><td>83.0</td><td>74.0</td></tr><tr><td>Qwen3-32B (Thinking)</td><td>90.7</td><td>89.7</td><td>84.8</td><td>86.7</td><td>84.5</td><td>89.3</td><td>83.5</td><td>91.3</td><td>88.0</td><td>83.1</td></tr><tr><td>Qwen3-32B (Non-thinking)</td><td>89.1</td><td>88.0</td><td>82.3</td><td>83.7</td><td>84.0</td><td>85.0</td><td>85.0</td><td>88.7</td><td>88.0</td><td>81.3</td></tr><tr><td>Gemma-3-12B-IT</td><td>85.8</td><td>83.3</td><td>83.4</td><td>79.3</td><td>79.0</td><td>82.8</td><td>77.5</td><td>89.0</td><td>83.0</td><td>81.6</td></tr><tr><td>Qwen2.5-14B-Instruct</td><td>82.7</td><td>78.9</td><td>80.4</td><td>69.1</td><td>66.2</td><td>74.2</td><td>72.2</td><td>83.9</td><td>77.9</td><td>70.4</td></tr><tr><td>Qwen3-14B (Thinking)</td><td>88.6</td><td>87.3</td><td>82.4</td><td>82.4</td><td>81.0</td><td>83.8</td><td>83.5</td><td>91.0</td><td>82.5</td><td>81.7</td></tr><tr><td>Qwen3-14B (Non-thinking)</td><td>87.4</td><td>82.7</td><td>80.1</td><td>80.7</td><td>78.0</td><td>81.8</td><td>80.5</td><td>87.7</td><td>81.5</td><td>77.0</td></tr><tr><td>Gemma-3-4B-IT</td><td>71.8</td><td>72.0</td><td>63.5</td><td>61.7</td><td>64.8</td><td>64.0</td><td>61.5</td><td>70.7</td><td>71.0</td><td>62.6</td></tr><tr><td>Qwen2.5-3B-Instruct</td><td>58.0</td><td>62.3</td><td>57.2</td><td>47.9</td><td>36.9</td><td>45.1</td><td>49.8</td><td>50.6</td><td>56.8</td><td>48.4</td></tr><tr><td>Qwen3-4B (Thinking)</td><td>82.2</td><td>77.7</td><td>74.1</td><td>73.0</td><td>74.3</td><td>76.3</td><td>68.5</td><td>83.0</td><td>74.5</td><td>67.9</td></tr><tr><td>Qwen3-4B (Non-thinking)</td><td>76.0</td><td>77.0</td><td>65.6</td><td>65.6</td><td>65.5</td><td>64.0</td><td>60.5</td><td>74.0</td><td>74.0</td><td>61.0</td></tr><tr><td>Gemma-3-1B-IT</td><td>36.5</td><td>36.0</td><td>30.0</td><td>29.1</td><td>28.8</td><td>27.3</td><td>28.0</td><td>32.7</td><td>33.0</td><td>30.9</td></tr><tr><td>Qwen2.5-1.5B-Instruct</td><td>41.5</td><td>43.0</td><td>39.6</td><td>34.8</td><td>28.6</td><td>29.7</td><td>39.4</td><td>33.8</td><td>42.0</td><td>36.0</td></tr><tr><td>Qwen3-1.7B (Thinking)</td><td>69.7</td><td>66.0</td><td>59.4</td><td>58.6</td><td>52.8</td><td>57.8</td><td>53.5</td><td>70.3</td><td>63.5</td><td>53.4</td></tr><tr><td>Qwen3-1.7B (Non-thinking)</td><td>58.8</td><td>62.7</td><td>50.8</td><td>53.0</td><td>43.3</td><td>48.0</td><td>46.0</td><td>54.3</td><td>54.0</td><td>43.9</td></tr></table>

# References



Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael ´Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXivpreprint arXiv:2412.08905, 2024.





AIME. AIME problems and solutions, 2025. URL https://artofproblemsolving.com/wiki/index.php/AIME Problems and Solutions.





Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit ´Sanghai. GQA: Training generalized multi-query Transformer models from multi-head checkpoints. InEMNLP, pp. 4895–4901. Association for Computational Linguistics, 2023.





Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.Training-free long-context scaling of large language models. CoRR, abs/2402.17463, 2024.





Anthropic. Claude 3.7 Sonnet, 2025. URL https://www.anthropic.com/news/claude-3-7-sonnet.





Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan,Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with largelanguage models. CoRR, abs/2108.07732, 2021.





Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, XingxuanZhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and TianhangZhu. Qwen technical report. CoRR, abs/2309.16609, 2023.





Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, ShijieWang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025.





Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa,Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The Belebele benchmark:A parallel reading comprehension dataset in 122 language variants. CoRR, abs/2308.16884, 2023.





Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and DarioAmodei. Language models are few-shot learners. In NeurIPS, 2020.





Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg,and Abhinav Jangda. MultiPL-E: A scalable and polyglot approach to benchmarking neural codegeneration. IEEE Trans. Software Eng., 49(7):3675–3691, 2023.





Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, ´Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, NickRyder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, PhilippeTillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes,Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, IgorBabuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr,Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR,abs/2107.03374, 2021.





Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.





Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, WangdingZeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, andWenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts languagemodels. CoRR, abs/2401.06066, 2024.





Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gatedconvolutional networks. In ICML, volume 70 of Proceedings of Machine Learning Research, pp. 933–941.PMLR, 2017.





Google DeepMind. Gemini 2.5, 2025. URL https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/.





Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, An-dreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton,Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer,Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, AravindhMahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko,Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, FilipPavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen,and Neil Houlsby. Scaling vision transformers to 22 billion parameters. In ICML, volume 202 ofProceedings of Machine Learning Research, pp. 7480–7512. PMLR, 2023.





Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang,Xiaolong Jin, Zhenlin Wei, et al. SuperGPQA: Scaling LLM evaluation across 285 graduate disciplines.arXiv preprint arXiv:2502.14739, 2025.





Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, AieshaLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn,Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, AstonZhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi ´ ere, Bethany Biron, Binh `Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell,Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, CyrusNikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, DhruvChoudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin,Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang,Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan ´Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, JanGeffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock,Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, andet al. The Llama 3 herd of models. CoRR, abs/2407.21783, 2024.





Simin Fan, Matteo Pagliardini, and Martin Jaggi. DoGE: Domain reweighting with generalizationestimation. arXiv preprint arXiv:2310.15393, 2023.





Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino,Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are wedone with MMLU? CoRR, abs/2406.04127, 2024.





Alex Gu, Baptiste Roziere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. `Wang. CRUXEval: A benchmark for code reasoning, understanding and execution. arXiv preprintarXiv:2401.03065, 2024.





Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcementlearning. arXiv preprint arXiv:2501.12948, 2025.





Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, TengyuXu, Hongjiang Lv, et al. Multi-IF: Benchmarking LLMs on multi-turn and multilingual instructionsfollowing. arXiv preprint arXiv:2410.15553, 2024.





Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and JacobSteinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net, 2021a.





Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPSDatasets and Benchmarks, 2021b.





Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang,and Boris Ginsburg. RULER: What’s the real context size of your long-context language models? CoRR,abs/2404.06654, 2024.





Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. In NeurIPS, 2023.





Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, BowenYu, Keming Lu, et al. Qwen2.5-Coder technical report. CoRR, abs/2409.12186, 2024.





Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free evaluation oflarge language models for code. CoRR, abs/2403.07974, 2024.





Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm Transform-ers: Equivalent and efficient pre-LN Transformers. CoRR, abs/2305.14858, 2023.





Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman,Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf,Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A.Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open ¨language model post-training. CoRR, abs/2411.15124, 2024.





Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez,and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and BenchBuilderpipeline. CoRR, abs/2406.11939, 2024.





Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, JohnSchulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. CoRR, abs/2305.20050, 2023.





Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark,and Yejin Choi. ZebraLogic: On the scaling limits of LLMs for logical reasoning. CoRR, abs/2502.01100,2025.





Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, ChengqiDeng, Chenyu Zhang, Chong Ruan, et al. DeepSeek-V3 technical report. arXiv preprint arXiv:2412.19437,2024a.





Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by ChatGPTreally correct? Rigorous evaluation of large language models for code generation. In NeurIPS, 2023a.





Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang,and Min Lin. RegMix: Data mixture as regression for language model pre-training. arXiv preprintarXiv:2407.01492, 2024b.





Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, YifanXu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, YuxiaoDong, and Jie Tang. AlignBench: Benchmarking Chinese alignment of large language models. CoRR,abs/2311.18743, 2023b.





Meta-AI. The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation, 2025.URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/.





OpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/.





OpenAI. Multilingual massive multitask language understanding, 2024. URL https://huggingface.co/datasets/openai/MMMLU.





OpenAI. Learning to reason with LLMs, 2024. URL https://openai.com/index/learning-to-reason-with-llms/.





OpenAI. Introducing openai o3 and o4-mini, 2025. URL https://openai.com/index/introducing-o3-and-o4-mini/.





Samuel J. Paech. Creative writing v3, 2024. URL https://eqbench.com/creative writing.html.





Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context windowextension of large language models. CoRR, abs/2309.00071, 2023.





Zihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu,Jingren Zhou, and Junyang Lin. Demons in the detail: On implementing load balancing loss fortraining specialized mixture-of-expert models. CoRR, abs/2501.11873, 2025.





Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, BofeiGao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, BinyuanHui, and Junyang Lin. CodeElo: Benchmarking competition-level code generation of LLMs withhuman-comparable Elo ratings. CoRR, abs/2501.01257, 2025.





Qwen Team. QwQ: Reflect deeply on the boundaries of the unknown, November 2024. URL https://qwenlm.github.io/blog/qwq-32b-preview/.





Qwen Team. QwQ-32B: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/.





David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level Google-proof Q&A benchmark.CoRR, abs/2311.12022, 2023.





Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, ShivalikaSingh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas,Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen,Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Flo-rez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari,Borje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzeminski, Gabriel Adri- ¨ano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir ObandoCeron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell,Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia SoltaniMoakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, SerhanYilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara Hooker, and Antoine Bosselut. INCLUDE:evaluating multilingual language understanding with regional knowledge. CoRR, abs/2411.19799,2024.





Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words withsubword units. In ACL (1). The Association for Computer Linguistics, 2016.





Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, andDaya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models.CoRR, abs/2402.03300, 2024.





Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung WonChung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models aremultilingual chain-of-thought reasoners. In ICLR. OpenReview.net, 2023.





Guijin Son, Jiwoo Hong, Hyunwoo Ko, and James Thorne. Linguistic generalizability of test-time scalingin mathematical reasoning. CoRR, abs/2502.17407, 2025.





Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:Enhanced Transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.





Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, ¨Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-Benchtasks and whether chain-of-thought can solve them. In ACL (Findings), pp. 13003–13051. Associationfor Computational Linguistics, 2023.





Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, SarahPerrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi ´ ere, et al. Gemma 3 technical report. `arXiv preprint arXiv:2503.19786, 2025.





Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level subwords.In AAAI, pp. 9154–9160. AAAI Press, 2020.





Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, FeitongSun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, andJingren Zhou. PolyMath: Evaluating mathematical reasoning in multilingual contexts, 2025.





Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren,Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, XiangYue, and Wenhu Chen. MMLU-Pro: A more robust and challenging multi-task language understandingbenchmark. CoRR, abs/2406.01574, 2024.





Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, WillieNeiswanger, and Micah Goldblum. LiveBench: A challenging, contamination-free LLM benchmark.CoRR, abs/2406.19314, 2024.





Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, MengyueWu, Qin Jin, and Fei Huang. WritingBench: A comprehensive benchmark for generative writing. CoRR,abs/2503.05244, 2025.





xAI. Grok 3 beta — the age of reasoning agents, 2025. URL https://x.ai/news/grok-3.





Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc VLe, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language modelpretraining. Advances in Neural Information Processing Systems, 36:69798–69818, 2023.





Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, RashiRungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad,Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang,and Hao Ma. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023.





Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E.Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8 berkeley function calling leaderboard.html, 2024.





An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang,Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He,Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang,Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu,Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang,Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu,Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR,abs/2407.10671, 2024a.





An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, DayihengLiu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b.





An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu,Jingren Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert modelvia self-improvement. CoRR, abs/2409.12122, 2024c.





Yidan Zhang, Boyi Deng, Yu Wan, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, andJingren Zhou. P-MMEval: A parallel multilingual multitask benchmark for consistent evaluation ofLLMs. CoRR, abs/2411.09116, 2024.





Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, andLe Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911, 2023.





Qin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang,and Junyang Lin. AutoLogi: Automated generation of logic puzzles for evaluating reasoning abilitiesof large language models. CoRR, abs/2502.16906, 2025.

