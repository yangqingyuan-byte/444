# Large Language Models for Time Series: A Survey

Xiyuan Zhang , Ranak Roy Chowdhury , Rajesh K. Gupta and Jingbo Shang

University of California, San Diego

{xiyuanzh, rrchowdh, rgupta, jshang}@ucsd.edu

# Abstract

Large Language Models (LLMs) have seen signif-icant use in domains such as natural language pro-cessing and computer vision. Going beyond text,image and graphics, LLMs present a significantpotential for analysis of time series data, benefit-ing domains such as climate, IoT, healthcare, traf-fic, audio and finance. This survey paper providesan in-depth exploration and a detailed taxonomyof the various methodologies employed to harnessthe power of LLMs for time series analysis. Weaddress the inherent challenge of bridging the gapbetween LLMs’ original text data training and thenumerical nature of time series data, and explorestrategies for transferring and distilling knowledgefrom LLMs to numerical time series analysis. Wedetail various methodologies, including (1) directprompting of LLMs, (2) time series quantization,(3) aligning techniques, (4) utilization of the vi-sion modality as a bridging mechanism, and (5) thecombination of LLMs with tools. Additionally, thissurvey offers a comprehensive overview of the ex-isting multimodal time series and text datasets anddelves into the challenges and future opportunitiesof this emerging field. We maintain an up-to-dateGithub repository1 which includes all the papersand datasets discussed in the survey.

# 1 Introduction

Time series analysis plays a critical role in a variety of fields,including climate modeling, traffic management, healthcaremonitoring and finance analytics. Time series analysis com-prises a wide range of tasks such as classification [Liu etal., 2023d], forecasting [Gruver et al., 2023], anomaly detec-tion [Zhang et al., 2023f], and imputation [Chen et al., 2023].Traditionally, these tasks have been tackled using classicalsignal processing techniques such as frequency analysis anddecomposition-based approaches. More recently, deep learn-ing approaches like Convolutional Neural Networks (CNNs),Long Short-Term Memory networks (LSTMs), and Trans-formers [Wen et al., 2022] have revolutionized this field and

![](images/e199c9b9ffedec52066bc0a21085c3882d37b0f61020db364f6d6f66891c5347.jpg)



Figure 1: Large language models have recently been applied for var-ious time series tasks in diverse application domains.


proved effective in extracting meaningful patterns from timeseries data, making them the primary approaches of time se-ries analysis in various domains.

In recent years, Large Language Models (LLMs) havegained substantial attention particularly in the fields of Natu-ral Language Processing (NLP) and Computer Vision (CV).Prominent models such as GPT-4 [OpenAI, 2023] have trans-formed the landscape of text processing by offering unprece-dented accuracy in tasks such as text generation, translation,sentiment analysis, question answering and summarization.In the CV domain, Large Multimodal Models (LMMs) havealso facilitated advancements in image recognition, objectdetection, and generative tasks, leading to more intelligentand capable visual systems [Song et al., 2023]. Inspired bythese successes, researchers are now exploring the potentialof LLMs in the realm of time series analysis, expecting fur-ther breakthroughs, as shown in Figure 1. While several sur-veys offer a broad perspective on large models for time seriesin general [Jin et al., 2023b; Ma et al., 2023], these do notspecifically focus on LLMs or the key challenge of bridg-ing modality gap, which stems from LLMs being originallytrained on discrete textual data, in contrast to the continuousnumerical nature of time series.

Our survey uniquely contributes to the existing literatureby emphasizing how to bridge such modality gap and trans-fer knowledge from LLMs for time series analysis. Our sur-vey also covers more diverse application domains, rangingfrom climate, Internet of Things (IoT), to healthcare, trafficmanagement, and finance. Moreover, certain intrinsic prop-erties of time series, like continuity, auto-regressiveness, anddependency on the sampling rate, are also shared by audio,speech, and music data. Therefore, we also present represen-

![](images/1e8a725faa204fd4804a2d16bec5c4b58a75714b64dc03aaa674fec1b358d519.jpg)



Figure 2: Left: Taxonomy of LLMs for time series analysis (prompting, quantization, aligning which is further categorized into two groupsas detailed in Figure 4, vision as bridge, tool integration). For each category, key distinctions are drawn in comparison to the standard LLMpipeline shown at the top of the figure. Right: We present representative works for each category, sorted by their publication dates. The useof arrows indicates that later works build upon earlier studies. Dark(light)-colored boxes represent billion(million)-parameter models. Iconsto the left of the text boxes represent the application domains of domain-specific models, with icons’ meanings illustrated in Figure 1.


tative LLM-based works from these domains to explore howwe can use LLMs for other types of time series. We presenta comprehensive taxonomy by categorizing these methodolo-gies into five distinct groups, as shown in Figure 2. If weoutline typical LLM-driven NLP pipelines in five stages - in-put text, tokenization, embedding, LLM, output - then eachcategory of our taxonomy targets one specific stage in thispipeline. Specifically, (i) Prompting (input stage) treats timeseries data as raw text and directly prompts LLMs with timeseries; (ii) Time Series Quantization (tokenization stage) dis-cretizes time series as special tokens for LLMs to process;(iii) Aligning (embedding stage) designs time series encoderto align time series embeddings with language space; (iv) Vi-sion as Bridge (LLM stage) connects time series with Vision-Lanuage Models (VLM) by employing visual representationsas a bridge; (v) Tool Integration (output stage) adopts lan-guage models to output tools to benefit time series analysis.Beyond this taxonomy, our survey also compiles an extensivelist of existing multimodal datasets that incorporate both timeseries and text. We conclude our paper by discussing futureresearch directions in this emerging and promising field.

# 2 Background and Problem Formulation

Large language models are characterized by their vast num-ber of parameters and extensive training data. They excel inunderstanding, generating, and interpreting human languageand recently represent a significant advancement in artificialintelligence. The inception of LLMs can be traced back tomodels like GPT-2 [Radford et al., 2019], BERT [Devlin etal., 2018], BART [Lewis et al., 2019], and T5 [Raffel et al.,2020], which laid the foundational architecture. Over time,the evolution of these models has been marked by increas-ing complexity and capabilities, such as LLAMA-2 [Touvronet al., 2023], PaLM [Chowdhery et al., 2023], and GPT-4.More recently, researchers have developed multimodal largelanguage models to integrate and interpret multiple forms ofdata, such as text, images, and time series, to achieve a morecomprehensive understanding of information.

This survey focuses on how LLMs could benefit time seriesanalysis. We first define the mathematical formulation for theinput and output, which may contain time series or (and) textdepending on the downstream tasks, as well as the models.

Input: denoted as x, composed of time series $\mathbf { x } _ { s } \in \mathbb { R } ^ { T \times c }$and optional text data $\mathbf { x } _ { t }$ represented as strings, where $T , c$represent the sequence length and the number of features.

Output: denoted as y and may represent time series, textor numbers depending on the specific downstream task. Fortime series generation or forecasting task, y represents gen-e series . For te $\mathbf { y } _ { s }$ or prgener icted n tas $k$ -step future time series such as report genera-${ \bf y } _ { s } ^ { T + 1 : T + k }$$\mathbf { y } _ { t }$or regression task, y represents numbers indicating the pre-dicted classes or numerical values.

Model: We use $f _ { \theta }$ parameterized by $\theta$ , $g _ { \phi }$ parameterized by$\phi$ , and $h _ { \psi }$ parameterized by $\psi$ to represent language, time se-ries and vision models, where $f _ { \theta }$ is typically initialized frompre-trained large language models. We optimize parameters$\theta$ , $\phi$ and $\psi$ through loss function $\mathcal { L }$ .

# 3 Taxonomy

In this section, we detail our taxonomy of applying LLMsfor time series analysis, categorized by five groups. We sum-marize the representative works, mathematical formulation,advantages and limitations of each category in Table 2.

# 3.1 Prompting

Number-Agnostic Tokenization: The method treats numer-ical time series as raw textual data and directly prompts ex-isting LLMs. For example, PromptCast [Xue and Salim,2022] proposes prompt-based time series forecasting by con-verting numerical time series into text prompts and forecast-ing time series in a sentence-to-sentence manner. The inputprompts are composed of context and questions followingpre-defined templates. An illustrative prompt template fortemperature forecasting, along with examples from other rep-resentative works, are showcased in Table 1. Similar prompt-


Table 1: Examples of representative direct prompting methods.


<table><tr><td>Method</td><td>Example</td></tr><tr><td>PromptCast [Xue and Salim, 2022]</td><td>“From {t1} to {tobs}, the average temperature of region {Um} was {xtm} degree on each day. What is the temperature going to be on {tobs}?”</td></tr><tr><td>Liu et al. [2023d]</td><td>“Classify the following accelerometer data in meters per second squared as either walking or running: 0.052,0.052,0.052,0.051,0.052,0.055,0.051,0.056,0.06,0.064”</td></tr><tr><td>TabLLM [Hegselmann et al., 2023]</td><td>“The person is 42 years old and has a Master&#x27;s degree. She gained $594. Does this person earn more than 50000 dollars? Yes or no? Answer:”</td></tr><tr><td>LLMTime [Gruver et al., 2023]</td><td>“0.123, 1.23, 12.3, 123.0” → “1 2, 1 2 3, 1 2 3 0, 1 2 3 0 0”</td></tr></table>

ing methods have been applied to forecast Place-of-Interest(POI) customer flows (AuxMobLCast [Xue et al., 2022]), en-ergy load [Xue and Salim, 2023], and user’s next location(LLM-Mob [Wang et al., 2023b]). Liu et al. [2023d] promptPaLM-24B for health-related tasks such as activity recogni-tion and daily stress estimate. TabLLM [Hegselmann et al.,2023] prompts large language models with a serialization ofthe tabular data to a natural-language string for few-shot andzero-shot tabular data classification. Zhang et al. [2023f]prompt large language models to detect anomalous behaviorsfrom mobility data. Xie et al. [2023a] extract historical pricefeatures such as open, close, high, and low prices to promptChatGPT in a zero-shot fashion.

Number-Specific Tokenization: More recently, LLM-Time [Gruver et al., 2023] pointed out that Byte Pair Encod-ing (BPE) tokenization has the limitation of breaking a sin-gle number into tokens that don’t align with the digits, lead-ing to inconsistent tokenization across different floating pointnumbers and complicating arithmetic operations [Spathis andKawsar, 2023]. Therefore, following LLMs such as LLaMAand PaLM, they propose to insert spaces between digits to en-sure distinct tokenization of each digit and use a comma (“,”)to separate each time step in a time series. They also scaletime series to optimize token usage and keep fixed precision(e.g., two digits of precision) to efficiently manage contextlength. Meanwhile, BloomberGPT [Wu et al., 2023] trainson financial data with text and numerical data and places eachdigit in its own chunk to better handle numbers. Using similarspace-prefixed tokenization, Mirchandani et al. [2023] showthat LLMs are general pattern machines capable of sequencetransformation, completion and improvement.

# 3.2 Quantization

Quantization based method [Rabanser et al., 2020] convertsnumerical data into discrete representations as input to LLMs.This approach can be further divided into two main categoriesbased on the discretization technique employed.

Discrete Indices from VQ-VAE: The first type of quan-tization method transforms continuous time series into dis-crete indices as tokens. Among them one of the most popularmethods is training a Vector Quantized-Variational AutoEn-coder (VQ-VAE) [Van Den Oord et al., 2017], which learnsa codebook $\mathcal { C } \stackrel { } { = } \{ \mathbf { c } _ { i } \} _ { i = 1 } ^ { K }$ of K D-dimensional codewords$\mathbf { c } _ { i } \in \mathbb { R } ^ { D }$ to capture the latent representations, as illustrated

![](images/c72c8f1e1d83e06003c361af0d5b5c20d805282fcf008c1cebeb1592d4bb1c2b.jpg)



(a) VQ-VAE based quantization method.


![](images/104510c485c10e85ca04df4ab1430d582d1c8958b9627d85c311303b4df32ea9.jpg)



(b) K-Means based quantization method.



Figure 3: Two types of index-based quantization methods.


in Figure 3a. The method identifies the nearest neighbor$k _ { i }$ of each step $i$ of the encoded time series representation$g _ { \phi } ( \mathbf { x } _ { s } ) \ \in \ \mathbb { R } ^ { \frac { T } { S } \times D }$ in the codebook $S$ denotes the cumula-tive stride of VQ-VAE encoder), and uses the correspondingindices k as the quantized input to language models:

$$
\mathbf {q} _ {i} = \mathbf {c} _ {k _ {i}}, k _ {i} = \underset {j} {\arg \min } \| g _ {\phi} (\mathbf {x} _ {s}) _ {i} - \mathbf {c} _ {j} \| _ {2}, \mathbf {k} = [ k _ {i} ] _ {i = 1} ^ {\frac {T}{S}}. \tag {1}
$$

Based on VQ-VAE, Auto-TTE [Chung et al., 2023] quan-tizes ECGs into discrete formats and generates 12-lead ECGsignals conditioned on text reports. DeWave [Duan et al.,2023] adapts VQ-VAE to derive discrete codex encoding andaligns it with pre-trained BART for open-vocabulary EEG-to-text translation tasks. TOTEM [Talukder and Gkioxari, 2023]also quantizes time series through VQ-VAE as input to Trans-formers for multiple downstream applications such as fore-casting, classification, and translation. In the audio domain,UniAudio [Yang et al., 2023] tokenizes different types of tar-get audio using Residual Vector Quantization (RVQ) [Zeghi-dour et al., 2021] (a hierarchy of multiple vector quantizers)and supports 11 audio generation tasks. VioLA [Wang etal., 2023a] unifies various crossmodal tasks involving speechand text by converting speech utterances to discrete tokensthrough RVQ. AudioGen [Kreuk et al., 2022] learns discreteaudio representations using vector quantization layers andgenerates audio samples conditioned on text inputs.

Discrete Indices from K-Means: Apart from employingVQ-VAE, researchers have also explored K-Means cluster-

![](images/1d213062645861532ebbfdb9b5dfe69fb91459b3e61056e3046e92156a667df0.jpg)



(a) Aligning by similarity matching (Type one).


![](images/45edede9143dcb7c49a1b427d489580754e9fb82e840cff9b27fe8d131db788d.jpg)



(b) Aligning with large language models as backbones (Typetwo), where the output could be time series (e.g., forecasting)or text (e.g., EEG-to-text) depending on the downstream tasks.



Figure 4: Two types of aligning based methods.


ing for index-based tokenization, which uses the centroid in-dices as discretized tokens [Hsu et al., 2021], as shown inFigure 3b. Such methods are mostly applied in the audio do-main. For example, SpeechGPT [Zhang et al., 2023a] showscapability to perceive and generate multi-modal contents us-ing K-Means based discrete unit extractor. AudioLM [Bor-sos et al., 2023] discretizes codes produced by a neural audiocodec using K-means clustering to achieve high-quality syn-thesis. It also combines discretized activations of languagemodels pre-trained on audio using RVQ to capture long-termstructure. Following the same quantization procedure, Au-dioPaLM [Rubenstein et al., 2023] fuses PaLM-2 [Anil et al.,2023] and AudioLM with a joint vocabulary that can repre-sent speech and text with discrete tokens.

Discrete Indices from Other Techniques: Apart fromthe aforementioned time-domain quantization, FreqTST [Liet al., 2023b] utilizes frequency spectrum as a common dic-tionary to discretize time series into frequency units withweights for downstream forecasting task. Chronos [Ansari etal., 2024] quantizes real-valued time series into discrete bins,and optimizes existing language model architectures on thesetokenized time series via the cross-entropy loss.

Text Categories: The second type of quantization convertsnumerical data into pre-defined text categories, which is pri-marily adopted in financial domain. For example, TDML [Yuet al., 2023] categorizes the weekly price fluctuations into12 bins represented as $\mathbf { \ddot { \rho } } _ { \mathrm { Ḋ } i }  \mathbf { \vec { \rho } } _ { \mathrm { Ḋ } i Ḍ }$ or “Ui”, where “D” indicatesa decrease in price and “U” means an increase, and $i \ =$$1 , 2 , 3 , 4 , 5 , 5 +$ represents the level of price change.

# 3.3 Aligning

The third type of works trains a separate encoder for time se-ries, and aligns the encoded time series to the semantic spaceof language models. These works can be further categorizedinto two groups based on their specific aligning strategies, asillustrated in Figure 4.

Similarity Matching through Contrastive Loss: The firsttype of method aligns the time series embeddings with textembeddings through similarity matching, such as minimizing

the contrastive loss:

$$
\mathcal {L} = - \frac {1}{B} \sum_ {i = 1} ^ {B} \log \frac {\exp \left(\sin \left(g _ {\phi} \left(\mathbf {x} _ {s i}\right) , f _ {\theta} \left(\mathbf {x} _ {t i}\right)\right)\right) ^ {\frac {1}{\gamma}}}{\sum_ {k = 1} ^ {B} \exp \left(\sin \left(g _ {\phi} \left(\mathbf {x} _ {s i}\right) , f _ {\theta} \left(\mathbf {x} _ {t k}\right)\right)\right) ^ {\frac {1}{\gamma}}}, \tag {2}
$$

where $B , \gamma$ represent batch size and temperature parameterthat controls distribution concentrations, and sim representssimilarity score, typically computed as inner product:

$$
\sin \left(g _ {\phi} \left(\mathbf {x} _ {s i}\right), f _ {\theta} \left(\mathbf {x} _ {t i}\right)\right) = \left\langle g _ {\phi} \left(\mathbf {x} _ {s i}\right), f _ {\theta} \left(\mathbf {x} _ {t i}\right) \right\rangle . \tag {3}
$$

For instance, ETP [Liu et al., 2023a; Li et al., 2023a] inte-grates contrastive learning based pre-training to align electro-cardiography (ECG) signals with textual reports. King et al.[2023] use similar contrastive framework to align 17 clinicalmeasurements collected in Intensive Care Unit (ICU) to theircorresponding clinical notes. TEST [Sun et al., 2023] usescontrastive learning to generate instance-wise, feature-wise,and text-prototype-aligned time series embeddings to alignwith text embeddings. TENT [Zhou et al., 2023b] aligns textembeddings with IoT sensor signals through a unified seman-tic feature space using contrastive learning. JoLT [Cai et al.,2023] utilizes Querying Transformer (Q-Former) [Li et al.,2023c] optimized with contrastive loss to align the time se-ries and text representations.

Similarity Matching through Other Losses: Apart fromcontrastive loss, other loss functions are also employed to op-timize similarity matching between time series embeddingsand text embeddings. ECG-LLM [Qiu et al., 2023] alignsthe distribution between ECG and language embedding fromECG statements with an Optimal Transport based loss func-tion to train an ECG report generation model. MTAM [Han etal., 2022] uses various aligning techniques, such as CanonicalCorrelation Analysis and Wasserstein Distance, as loss func-tions to align electroencephalography (EEG) features withtheir corresponding language descriptions.

LLMs as Backbones: The second type of aligning methoddirectly uses large language models as backbones follow-ing time series embedding layers. EEG-to-Text [Wang andJi, 2022] feeds EEG embeddings to pre-trained BART foropen vocabulary EEG-To-Text decoding and EEG-based sen-timent classification. GPT4TS [Zhou et al., 2023a] usespatching embeddings [Nie et al., 2022] as input to frozenpre-trained GPT-2 where the positional embedding layersand self-attention blocks are retained during time series fine-tuning. The method provides a unified framework for seventime series tasks, including few-shot or zero-shot learn-ing. Following GPT4TS, researchers further incorporatedseasonal-trend decomposition (TEMPO [Cao et al., 2023]),two-stage fine-tuning (LLM4TS [Chang et al., 2023]), do-main descriptions (UniTime [Liu et al., 2023e]), graph atten-tion mechanism (GATGPT [Chen et al., 2023]), and spatial-temporal embedding module (ST-LLM [Liu et al., 2024]).Time-LLM [Jin et al., 2023a] reprograms time series data intotext prototypes as input to LLaMA-7B. It also provides nat-ural language prompts such as domain expert knowledge andtask instructions to augment input context. Lag-Llama [Rasulet al., 2023] builds univariate probabilistic time series fore-casting model based on LLaMA architecture. In the audio,speech and music domains, researchers have also designed


Table 2: Summary of five major categories of applying LLMs for time series analysis, including their respective subcategories, representativeworks, mathematical formulations, advantages and limitations. $q$ and $\mathbf { x } _ { v }$ represent text-based quantization process and image data.


<table><tr><td>Method</td><td>Subcategory</td><td>Representative Works</td><td>Equations</td><td>Advantages</td><td>Limitations</td></tr><tr><td rowspan="2">Prompting</td><td>Number-Agnostic</td><td>PromptCast [Xue and Salim, 2022]</td><td rowspan="2">y = fθ(xs, xt)</td><td rowspan="2">easy to implement; zero-shot capability</td><td rowspan="2">lose semantics; not efficient</td></tr><tr><td>Number-Specific</td><td>LLMTime [Gruver et al., 2023]</td></tr><tr><td rowspan="3">Quantization</td><td>VQ-VAE</td><td>DeWave [Duan et al., 2023]</td><td rowspan="2">ki=arg minj||gφ(xs)i-cj||2k=[ki]T/Σi=1, y=fθ(k, xt)</td><td rowspan="3">flexibility of index and time series conversion</td><td rowspan="3">may require two-stage training</td></tr><tr><td>K-Means</td><td>AudioLM [Borsos et al., 2023]</td></tr><tr><td>Text Categories</td><td>TDML [Yu et al., 2023]</td><td>y = fθ(q(xs), xt)</td></tr><tr><td rowspan="3">Aligning</td><td rowspan="2">Similarity Match</td><td>ETP [Liu et al., 2023a]</td><td>y = gφ(xs)</td><td rowspan="3">align semantics of different modalities; end-to-end training</td><td rowspan="3">complicated design and fine-tuning</td></tr><tr><td>MATM [Han et al., 2022]</td><td>L = sim(gφ(xs), fθ(xt))</td></tr><tr><td>LLM Backbone</td><td>GPT4TS [Zhou et al., 2023a]</td><td>y = fθ(gφ(xs), xt)</td></tr><tr><td rowspan="2">Vision as Bridge</td><td>Paired Data</td><td>ImageBind [Girdhar et al., 2023]</td><td>L = sim(gφ(xs), hψ(xs))</td><td rowspan="2">additional visual knowledge</td><td rowspan="2">not hold for all data</td></tr><tr><td>TS Plots as Images</td><td>Wimmer and Rekabsaz [2023]</td><td>y = hψ(xs)</td></tr><tr><td rowspan="2">Tool</td><td>Code</td><td>CTG++ [Zhong et al., 2023]</td><td>z = fθ(xs)</td><td rowspan="2">empower LLM with more abilities</td><td rowspan="2">optimization not end-to-end</td></tr><tr><td>API</td><td>ToolLLM [Qin et al., 2023]</td><td>y = z(xs)</td></tr></table>

dedicated encoders to embed speech (WavPrompt [Gao etal., 2022], Speech LLaMA [Lakomkin et al., 2023])), music(MU-LLaMA [Liu et al., 2023c]), and general audio inputs(LTU [Gong et al., 2023], SALMONN [Tang et al., 2023]),and feed the embeddings to large language models.

# 3.4 Vision as Bridge

Time series data can be effectively interpreted or associatedwith visual representations, which align closer with textualdata and have demonstrated successful integrations with largelanguage models. Therefore, researchers have also leveragedvision modality as a bridge to connect time series with LLMs.

Paired Data: ImageBind [Girdhar et al., 2023] usesimage-paired data to bind six modalities (images, text, audio,depth, thermal, and Inertial Measurement Unit (IMU) timeseries) and learn a joint embedding space, enabling new emer-gent capabilities. PandaGPT [Su et al., 2023] further com-bines the multimodal encoders from ImageBind and LLMsto enable visual and auditory instruction-following capabili-ties. IMU2CLIP [Moon et al., 2022] aligns IMU time serieswith video and text, by projecting them into the joint repre-sentation space of Contrastive Language-Image Pre-training(CLIP) [Radford et al., 2021]. AnyMAL [Moon et al., 2023]builds upon IMU2CLIP by training a lightweight adapter toproject the IMU embeddings into the text token embeddingspace of LLaMA-2-70B. It is also capable of transformingdata from other modalities, such as images, videos, audio,into the same text embedding space.

Physics Relationships: IMUGPT [Leng et al., 2023] gen-erates IMU data from ChatGPT-augmented text descriptions.It first generates 3D human motion from text using pre-trained motion synthesis model T2M-GPT [Zhang et al.,2023b]. Then it derives IMU data from 3D motion based onphysics relationships of motion kinetics.

Time Series Plots as Images: CLIP-LSTM [Wimmer andRekabsaz, 2023] transforms stock market data into sequences

of texts and images of price charts, and leverages pre-trainedCLIP vision-language model to generate features for down-stream forecasting. Insight Miner [Zhang et al., 2023e] con-verts time series windows into images using lineplot, andfeeds images into vision language model LLaVA [Liu et al.,2023b] to generate time series trend descriptions.

# 3.5 Tool

This type of method does not directly use large languagemodels to process time series. Instead, it applies large lan-guage models to generate indirect tools $z ( \cdot )$ , such as codeand API calls, to benefit time series related tasks.

Code: $\mathrm { C T G } { + } { + }$ [Zhong et al., 2023] applies GPT-4 to gen-erate differentiable loss functions in a code format from textdescriptions to guide the diffusion model to generate traffictrajectories. With this two-step translation, the LLM and dif-fusion model efficiently bridge the gap between user intentand traffic simulation.

API Call: ToolLLM [Qin et al., 2023] introduces a generaltool-use framework composed of data construction, modeltraining, and evaluation. This framework includes API callsfor time series tasks such as weather and stock forecasting.

Text Domain Knowledge: SHARE [Zhang et al., 2023d]exploits the shared structures in human activity label namesand proposes a sequence-to-sequence structure to generate la-bel names as token sequences to preserve the shared labelstructures. It applies GPT-4 to augment semantics of labelnames. GG-LLM [Graule and Isler, 2023] leverages LLaMA-2 to encode world knowledge of common human behavioralpatterns to predict human actions without further training.SCRL-LG [Ding et al., 2023] leverages LLaMA-7B as stockfeature selectors to extract meaningful representations fromnews headlines, which are subsequently employed in rein-forcement learning for precise feature alignments.


Table 3: Summary of representative time series and text multimodal datasets.


<table><tr><td>Domain</td><td>Dataset</td><td>Size</td><td>Major Modalities</td><td>Task</td></tr><tr><td rowspan="2">Internet of Things</td><td>Ego4D2 [Grauman et al., 2022]</td><td>3,670h data, 3.85M narrations</td><td>text, IMU, video, audio, 3D</td><td>classification, forecasting</td></tr><tr><td>DeepSQA3 [Xing et al., 2021]</td><td>25h data, 91K questions</td><td>text, imu</td><td>classification, question answering</td></tr><tr><td rowspan="2">Finance</td><td>PIXIU4 [Xie et al., 2023b]</td><td>136K instruction data</td><td>text, tables</td><td>5 NLP tasks, forecasting</td></tr><tr><td>MoAT5 [Lee et al., 2023]</td><td>6 datasets, 2K timesteps in total</td><td>text, time series</td><td>forecasting</td></tr><tr><td rowspan="3">Healthcare</td><td>Zuco 2.06 [Hollenstein et al., 2019]</td><td>739 sentences</td><td>text, eye-tracking, EEG</td><td>classification, text generation</td></tr><tr><td>PTB-XL7 [Wagner et al., 2020]</td><td>60h data, 71 unique statements</td><td>text, ECG</td><td>classification</td></tr><tr><td>ECG-QA8 [Oh et al., 2023]</td><td>70 question templates</td><td>text, ECG</td><td>classification, question answering</td></tr><tr><td>Audio</td><td>OpenAQA-5M9 [Gong et al., 2023]</td><td>5.6M (audio, question, answer) tuples</td><td>text, audio</td><td>tagging, classification</td></tr><tr><td>Music</td><td>MusicCaps10 [Agostinelli et al., 2023]</td><td>5.5K music clips</td><td>text, music</td><td>captioning, generation</td></tr><tr><td>Speech</td><td>CommonVoice11 [Ardila et al., 2019]</td><td>7,335 speech hours in 60 languages</td><td>text, speech</td><td>ASR, translation</td></tr></table>

# 4 Comparison within the Taxonomy

We compare the five categories of our taxonomy and pro-vide general guidelines for which category to choose basedon considerations of data, model, efficiency and optimization.

Data: When no training data is available and the objec-tive is to apply LLM for time series in an zero-shot fashion,it is preferable to use prompting-based methods. This is be-cause direct prompting enables the utilization of pre-trainedlanguage models’ inherent capabilities without fine-tuning.However, representing numbers as strings can diminish thesemantic value intrinsically tied to numerical data. There-fore, with adequate training data, quantization or aligning-based methods become more advantageous. As shown in Fig-ure 2, these two categories are the most extensively studiedones in existing literature. Furthermore, if time series datacan be interpreted or associated with visual representations,these representations can be incorporated to utilize the intrin-sic knowledge embedded in the vision modality or pre-trainedvision-language models.

Model: Prompting and tool integration methods tend toapply billion-parameter models as they often apply off-the-self LLMs without architectural modifications. By con-trast, aligning and quantization methods vary from millionto billion-parameter models, depending on the specific appli-cation requirements and available computational resources.

Efficiency: Prompting-based methods are not efficient fornumerical data with high precision, as well as multivariatetime series as it requires transforming each dimension intoseparate univariate time series, resulting in extremely long in-put. They are also less efficient for long-term predictions dueto the computational demands of generating long sequences.These methods are more effective when dealing with simplenumerical data that is richly interwoven with textual infor-mation, such as opening and closing stock prices in financialnews articles. By contrast, quantization and aligning meth-ods are more efficient to handle long sequences, as time seriesare typically down-sampled or segmented into patches beforefeeding into large language models.

Optimization: Depending on the specific discretizationtechnique, quantization based method may require a two-stage training process (such as first training the VQ-VAE

model), which may result in sub-optimal performance com-pared with that achieved through end-to-end training in align-ing methods. Using large language models as indirect toolsempowers LLMs with more capabilities to manage numeri-cal data, but also raises the level of complexity to optimizeboth LLMs and other components in an end-to-end fashion.Therefore, existing works of tool integration typically employoff-the-shelf LLMs without further fine-tuning.

# 5 Multimodal Datasets

Applying LLMs for time series benefits from the availabil-ity of multimodal time series and text data. In this section,we introduce representative multimodal time series and textdatasets organized by their respective domains (Table 3). Welist additional multimodal datasets in our Github repository12.Internet of Things (IoT): Human activity recognition is animportant task in IoT domain, which identifies human activi-ties given time series collected with IoT devices (such as IMUsensors). The corresponding text data are the labels or text de-scriptions of these activities. Ego4D [Grauman et al., 2022]presents 3,670 hours of daily-life activity data across hun-dreds of scenarios, including household, outdoor, workplace,and leisure. The dataset is rich in modalities, including theIMU time series measurement, and dense temporally-alignedtextual descriptions of the activities and object interactions,totaling 3.85 million sentences. Ego-Exo4D [Grauman etal., 2023] further offers three kinds of paired natural lan-guage datasets including expert commentary, narrate-and-actdescriptions provided by the participants, and atomic action

descriptions similar as Ego4D. DeepSQA [Xing et al., 2021]presents a generalized Sensory Question Answering (SQA)framework to facilitate querying raw sensory data related tohuman activities using natural language.

Finance: PIXIU [Xie et al., 2023b] presents multi-task andmulti-modal instruction tuning data in the financial domainwith 136K data samples. It contains both financial natu-ral language understanding and prediction tasks, and covers9 datasets of multiple modalities such as text and time se-ries. MoAT [Lee et al., 2023] constructs multimodal datasetswith textual information paired with time series for eachtimestep, such as news articles extracted with relevant key-words, mostly covering finance related domains such as fuel,metal, stock and bitcoin.

Healthcare: Zuco 1.0 [Hollenstein et al., 2018] and Zuco2.0 [Hollenstein et al., 2019] datasets contain simultaneouseye-tracking and EEG during natural reading and during an-notation. PTB-XL [Wagner et al., 2020] offers comprehen-sive metadata regarding ECG annotated by expert cardiolo-gists, covering information such as ECG reports, diagnosticstatements, diagnosis likelihoods, and signal-specific proper-ties. Based on PTB-XL, ECG-QA [Oh et al., 2023] intro-duces the first Question Answering (QA) dataset for ECGanalysis, containg 70 question templates that cover a widerange of clinically relevant ECG topics.

Audio/Music/Speech: AudioSet [Gemmeke et al., 2017] isa collection of 2 million 10-second audio clips excised fromYouTube videos and labeled with the sounds that the clip con-tains from a set of 527 labels. OpenAQA-5M [Gong et al.,2023] dataset consists of 1.9 million closed-ended and 3.7million open-ended (audio, question, answer) tuples. Music-Caps [Agostinelli et al., 2023] is a high-quality music captiondataset, including 5.5K music clips. MTG-Jamendo [Bog-danov et al., 2019] is a dataset with 55,000 audio songs in var-ious languages. Libri-Light [Kahn et al., 2020] is an Englishdataset encompassing 60,000 hours of speech data. Common-Voice [Ardila et al., 2019] is a multilingual speech datasetconsisting of 7,335 validated hours in 60 languages.

These datasets offer valuable benchmarks for multimodaltime series and text analysis. These contain both time se-ries focused tasks, including classification, which is evaluatedusing accuracy and macro-F1 scores, and forecasting, whichutilizes metrics such as MSE, MAE, RMSE, and MAPE, aswell as NLP focused tasks such as captioning, question an-swering, and translation, assessed through BLEU, ROUGE,METEOR, and EM scores, among others.

# 6 Challenges and Future Directions

In this section, we introduce the challenges and promisingfuture directions of applying LLMs for time series analysis.

# 6.1 Theoretical Understanding

Existing works empirically show the benefits of applyingLLMs for time series analysis. For example, LIFT [Dinh etal., 2022] empirically shows that language model fine-tuningcan work for non-language tasks without changing the archi-tecture or loss function; Gurnee and Tegmark [2023] empir-ically show that LLMs learn linear representations of space

and time across multiple scales that are robust to promptingvariations. Despite these empirical findings, there remainsa gap in theoretical understanding of how models, primar-ily trained on textual data, can effectively interpret numeri-cal time series. As a preliminary theoretical analysis, Yun etal. [2019] prove that Transformer models can universally ap-proximate arbitrary continuous sequence-to-sequence func-tions on a compact domain. Additionally, GPT4TS [Zhou etal., 2023a] theoretically shows that such generic capability oflarge language models can be related to Principal ComponentAnalysis (PCA), as minimizing the gradient with respect tothe self-attention layer shares similarities with PCA. Furtherinvestigations on the generalizability of LLMs on numericaldata is essential to establish solid understanding of the syn-ergy between LLMs and time series analysis.

# 6.2 Multimodal and Multitask Analysis

Existing papers that apply LLMs for time series analysismostly focus on single modality and single task at a time,such as forecasting, classification, text generation, and do notsupport simultaneous multimodal and multitask analysis. Incomputer vision and audio domains, models such as Unified-IO [Lu et al., 2022] and UniAudio [Yang et al., 2023] haveunified multiple input modalities into a sequence of discretevocabulary tokens to support multiple tasks within a singletransformer-based architecture. More research into leverag-ing LLMs for multimodal and multitask analysis would leadto more powerful time series foundation models.

# 6.3 Efficient Algorithms

Time series, especially those that are multivariate or pos-sess long history information may increase the computationalcomplexity for existing large language models. Patching [Nieet al., 2022] has been a widely adopted strategy to improveperformance as well as reduce complexity, but large patchesmay obscure the semantic information of time series and neg-atively impact the performance. Therefore, developing moreefficient algorithms is crucial for facilitating large-scale timeseries analysis and enhancing interactions with end users.

# 6.4 Combining Domain Knowledge

Combining existing statistical domain knowledge with LLMsmay further boost the model’s capability for time series anal-ysis. For example, TEMPO [Cao et al., 2023] applies timeseries seasonal-trend decomposition and treats decomposedcomponents as different semantic inductive biases as input tothe pre-trained transformer. FreqTST [Li et al., 2023b] lever-ages insights from the frequency domain by tokenizing singletime series into frequency units with weights for downstreamforecasting. Further incorporating domain knowledge, suchas wavelet decomposition, auto-correlation analysis, and em-pirical mode decomposition may augment LLMs’ capabilitiesin analyzing time series data.

# 6.5 Customization and Privacy

Existing works on large language models and time seriesanalysis typically train a global model for all end users. Train-ing customized models for different users based on the globalmodel may bring further benefits and flexibility. Another

important consideration is privacy, especially as many timeseries data are collected in private settings for clinical pur-poses or smart home applications. As an initial attempt,FedAlign [Zhang et al., 2023c] leverages federated learningframeworks and uses the expressive natural language classnames as a common ground to align the latent spaces acrossdifferent clients. Advancing research into model customiza-tion and user privacy preservation would broaden the scopeand utility of LLM-empowered time series analysis.

# 7 Conclusion

We present the first survey that systematically analyzes thecategorization of transferring knowledge from large languagemodels for numerical time series analysis: direct prompt-ing, time series quantization, aligning, the use of the visionmodality to connect text and time series, and the integrationof large language models with other analytical tools. For eachcategory, we introduce their mathematical formulation, rep-resentative works, and compare their advantages and limita-tions. We also introduce representative multimodal text andtime series datasets in various domains such as healthcare,IoT, finance, and audio. Concluding the paper, we outlinethe challenges and emerging directions for potential futureresearch of LLM-empowered time series analysis.

# 8 Acknowledgements

Our work is supported in part by ACE, one of the sevencenters in JUMP 2.0, a Semiconductor Research Corpora-tion (SRC) program sponsored by DARPA. Our work is alsosponsored by NSF CAREER Award 2239440, NSF Proto-OKN Award 2333790, NIH Bridge2AI Center Program un-der award 1U54HG012510-01, Cisco-UCSD Sponsored Re-search Project, as well as generous gifts from Google, Adobe,and Teradata. Any opinions, findings, and conclusions orrecommendations expressed herein are those of the authorsand should not be interpreted as necessarily representing theviews, either expressed or implied, of the U.S. Government.The U.S. Government is authorized to reproduce and dis-tribute reprints for government purposes not withstanding anycopyright annotation hereon.

# References



Andrea Agostinelli, Timo I Denk, Zalan Borsos, Jesse En-´gel, Mauro Verzetti, Antoine Caillon, Qingqing Huang,Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al.Musiclm: Generating music from text. arXiv preprintarXiv:2301.11325, 2023.





Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2technical report. arXiv preprint arXiv:2305.10403, 2023.





Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, XiyuanZhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur,Syama Sundar Rangapuram, Sebastian Pineda Arango,Shubham Kapoor, et al. Chronos: Learning the languageof time series. arXiv preprint arXiv:2403.07815, 2024.





Rosana Ardila, Megan Branson, Kelly Davis, Michael Hen-retty, Michael Kohler, Josh Meyer, Reuben Morais, Lind-say Saunders, Francis M Tyers, and Gregor Weber. Com-mon voice: A massively-multilingual speech corpus. arXivpreprint arXiv:1912.06670, 2019.





Dmitry Bogdanov, Minz Won, Philip Tovstogan, AlastairPorter, and Xavier Serra. The mtg-jamendo dataset for au-tomatic music tagging. ICML, 2019.





Zalan Borsos, Rapha ´ el Marinier, Damien Vincent, Eugene ¨Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Rob-lek, Olivier Teboul, David Grangier, Marco Tagliasacchi,et al. Audiolm: a language modeling approach to audiogeneration. IEEE/ACM Transactions on Audio, Speech,and Language Processing, 2023.





Yifu Cai, Mononito Goswami, Arjun Choudhry, ArvindSrinivasan, and Artur Dubrawski. Jolt: Jointly learned rep-resentations of language and time-series. In Deep Genera-tive Models for Health Workshop NeurIPS 2023, 2023.





Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, YixiangZheng, Wen Ye, and Yan Liu. Tempo: Prompt-based gen-erative pre-trained transformer for time series forecasting.arXiv preprint arXiv:2310.04948, 2023.





Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts:Two-stage fine-tuning for time-series forecasting with pre-trained llms. arXiv preprint arXiv:2308.08469, 2023.





Yakun Chen, Xianzhi Wang, and Guandong Xu. Gatgpt:A pre-trained large language model with graph attentionnetwork for spatiotemporal imputation. arXiv preprintarXiv:2311.14332, 2023.





Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, SebastianGehrmann, et al. Palm: Scaling language modelingwith pathways. Journal of Machine Learning Research,24(240):1–113, 2023.





Hyunseung Chung, Jiho Kim, Joon-myoung Kwon, Ki-HyunJeon, Min Sung Lee, and Edward Choi. Text-to-ecg: 12-lead electrocardiogram synthesis conditioned on clinicaltext reports. In ICASSP 2023-2023 IEEE InternationalConference on Acoustics, Speech and Signal Processing(ICASSP), pages 1–5. IEEE, 2023.





Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprintarXiv:1810.04805, 2018.





Yujie Ding, Shuai Jia, Tianyi Ma, Bingcheng Mao, XiuzeZhou, Liuliu Li, and Dongming Han. Integrating stockfeatures and global information via large language mod-els for enhanced stock return prediction. arXiv preprintarXiv:2310.05627, 2023.





Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, MichaelGira, Shashank Rajput, Jy-yong Sohn, Dimitris Pa-pailiopoulos, and Kangwook Lee. Lift: Language-interfaced fine-tuning for non-language machine learningtasks. Advances in Neural Information Processing Sys-tems, 35:11763–11784, 2022.





Yiqun Duan, Charles Zhou, Zhen Wang, Yu-Kai Wang, andChin-teng Lin. Dewave: Discrete encoding of eeg wavesfor eeg to text translation. In Thirty-seventh Conference onNeural Information Processing Systems, 2023.





Heting Gao, Junrui Ni, Kaizhi Qian, Yang Zhang, ShiyuChang, and Mark Hasegawa-Johnson. Wavprompt:Towards few-shot spoken language understanding withfrozen language models. arXiv preprint arXiv:2203.15863,2022.





Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, ArenJansen, Wade Lawrence, R Channing Moore, ManojPlakal, and Marvin Ritter. Audio set: An ontology andhuman-labeled dataset for audio events. In 2017 IEEEinternational conference on acoustics, speech and signalprocessing (ICASSP), pages 776–780. IEEE, 2017.





Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, MannatSingh, Kalyan Vasudev Alwala, Armand Joulin, and IshanMisra. Imagebind: One embedding space to bind themall. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 15180–15190,2023.





Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlin-sky, and James Glass. Listen, think, and understand. arXivpreprint arXiv:2305.10790, 2023.





Moritz A Graule and Volkan Isler. Gg-llm: Geometricallygrounding large language models for zero-shot human ac-tivity forecasting in human-aware task planning. arXivpreprint arXiv:2310.20034, 2023.





Kristen Grauman, Andrew Westbury, Eugene Byrne, ZacharyChavis, Antonino Furnari, Rohit Girdhar, Jackson Ham-burger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:Around the world in 3,000 hours of egocentric video. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 18995–19012, 2022.





Kristen Grauman, Andrew Westbury, Lorenzo Torresani,Kris Kitani, Jitendra Malik, Triantafyllos Afouras, KumarAshutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote,et al. Ego-exo4d: Understanding skilled human activityfrom first-and third-person perspectives. arXiv preprintarXiv:2311.18259, 2023.





Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew GordonWilson. Large language models are zero-shot time seriesforecasters. arXiv preprint arXiv:2310.07820, 2023.





Wes Gurnee and Max Tegmark. Language models representspace and time. arXiv preprint arXiv:2310.02207, 2023.





William Han, Jielin Qiu, Jiacheng Zhu, Mengdi Xu, DouglasWeber, Bo Li, and Ding Zhao. An empirical exploration ofcross-domain alignment between language and electroen-cephalogram. arXiv preprint arXiv:2208.06348, 2022.





Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Mon-ica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm:Few-shot classification of tabular data with large languagemodels. In International Conference on Artificial Intelli-gence and Statistics, pages 5549–5581. PMLR, 2023.





Nora Hollenstein, Jonathan Rotsztejn, Marius Troendle, An-dreas Pedroni, Ce Zhang, and Nicolas Langer. Zuco, a si-multaneous eeg and eye-tracking resource for natural sen-tence reading. Scientific data, 5(1):1–13, 2018.





Nora Hollenstein, Marius Troendle, Ce Zhang, and Nico-las Langer. Zuco 2.0: A dataset of physiological record-ings during natural reading and annotation. arXiv preprintarXiv:1912.00903, 2019.





Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,Kushal Lakhotia, Ruslan Salakhutdinov, and AbdelrahmanMohamed. Hubert: Self-supervised speech representationlearning by masked prediction of hidden units. IEEE/ACMTransactions on Audio, Speech, and Language Processing,29:3451–3460, 2021.





Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James YZhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series fore-casting by reprogramming large language models. arXivpreprint arXiv:2310.01728, 2023.





Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang,Siqiao Xue, Xue Wang, James Zhang, Yi Wang, HaifengChen, Xiaoli Li, et al. Large models for time series andspatio-temporal data: A survey and outlook. arXiv preprintarXiv:2310.10196, 2023.





Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny `Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazare,´Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert,Christian Fuegen, et al. Libri-light: A benchmark forasr with limited or no supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), pages 7669–7673. IEEE,2020.





Ryan King, Tianbao Yang, and Bobak J Mortazavi. Mul-timodal pretraining of medical time series and notes. InMachine Learning for Health (ML4H), pages 244–255.PMLR, 2023.





Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,Alexandre Defossez, Jade Copet, Devi Parikh, Yaniv Taig- ´man, and Yossi Adi. Audiogen: Textually guided audiogeneration. arXiv preprint arXiv:2209.15352, 2022.





Egor Lakomkin, Chunyang Wu, Yassir Fathullah, OzlemKalinli, Michael L Seltzer, and Christian Fuegen. End-to-end speech recognition contextualization with large lan-guage models. arXiv preprint arXiv:2309.10917, 2023.





Geon Lee, Wenchao Yu, Wei Cheng, and Haifeng Chen.Moat: Multi-modal augmented time series forecasting.2023.





Zikang Leng, Hyeokhyen Kwon, and Thomas Plotz. Generat- ¨ing virtual on-body accelerometer data from virtual textualdescriptions for human activity recognition. arXiv preprintarXiv:2305.03187, 2023.





Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-jad, Abdelrahman Mohamed, Omer Levy, Ves Stoy-anov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language genera-





tion, translation, and comprehension. arXiv preprintarXiv:1910.13461, 2019.





Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and ShendaHong. Frozen language model helps ecg zero-shot learn-ing. arXiv preprint arXiv:2303.12311, 2023.





Junkai Li, Weizhi Ma, and Yang Liu. Modeling time seriesas text sequence a frequency-vectorization transformer fortime series forecasting. 2023.





Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozenimage encoders and large language models. arXiv preprintarXiv:2301.12597, 2023.





Che Liu, Zhongwei Wan, Sibo Cheng, Mi Zhang, andRossella Arcucci. Etp: Learning transferable ecg rep-resentations via ecg-text pre-training. arXiv preprintarXiv:2309.07145, 2023.





Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. Visual instruction tuning. arXiv preprintarXiv:2304.08485, 2023.





Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, andYing Shan. Music understanding llama: Advancing text-to-music generation with question answering and caption-ing. arXiv preprint arXiv:2308.11276, 2023.





Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, ShunLiao, Paolo Di Achille, and Shwetak Patel. Large lan-guage models are few-shot health learners. arXiv preprintarXiv:2305.15525, 2023.





Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang,Bryan Hooi, and Roger Zimmermann. Unitime: Alanguage-empowered unified model for cross-domain timeseries forecasting. arXiv preprint arXiv:2310.09751, 2023.





Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, ChengLong, Ziyue Li, and Rui Zhao. Spatial-temporal largelanguage model for traffic prediction. arXiv preprintarXiv:2401.10134, 2024.





Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-taghi, and Aniruddha Kembhavi. Unified-io: A unifiedmodel for vision, language, and multi-modal tasks. arXivpreprint arXiv:2206.08916, 2022.





Qianli Ma, Zhen Liu, Zhenjing Zheng, Ziyang Huang, Siy-ing Zhu, Zhongzhong Yu, and James T Kwok. A sur-vey on time-series pre-trained models. arXiv preprintarXiv:2305.10716, 2023.





Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter,Danny Driess, Montserrat Gonzalez Arenas, KanishkaRao, Dorsa Sadigh, and Andy Zeng. Large languagemodels as general pattern machines. arXiv preprintarXiv:2307.04721, 2023.





Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, AlirezaDirafzoon, Aparajita Saraf, Amy Bearman, and BabakDamavandi. Imu2clip: Multimodal contrastive learning forimu motion sensors from egocentric videos and text. arXivpreprint arXiv:2210.14395, 2022.





Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, TusharNagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh,Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Any-mal: An efficient and scalable any-modality augmentedlanguage model. arXiv preprint arXiv:2309.16058, 2023.





Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, andJayant Kalagnanam. A time series is worth 64 words:Long-term forecasting with transformers. arXiv preprintarXiv:2211.14730, 2022.





Jungwoo Oh, Seongsu Bae, Gyubok Lee, Joon-myoungKwon, and Edward Choi. Ecg-qa: A comprehensive ques-tion answering dataset combined with electrocardiogram.arXiv preprint arXiv:2306.15681, 2023.





OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774,2023.





Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian,et al. Toolllm: Facilitating large language models to master$1 6 0 0 0 +$ real-world apis. arXiv preprint arXiv:2307.16789,2023.





Jielin Qiu, William Han, Jiacheng Zhu, Mengdi Xu, MichaelRosenberg, Emerson Liu, Douglas Weber, and Ding Zhao.Transfer knowledge from natural language to electrocar-diography: Can we detect cardiovascular disease throughlanguage models? arXiv preprint arXiv:2301.09017, 2023.





Stephan Rabanser, Tim Januschowski, Valentin Flunkert,David Salinas, and Jan Gasthaus. The effectiveness ofdiscretization in forecasting: An empirical study on neu-ral time series models. arXiv preprint arXiv:2005.10111,2020.





Alec Radford, Jeffrey Wu, Rewon Child, David Luan, DarioAmodei, Ilya Sutskever, et al. Language models are unsu-pervised multitask learners. OpenAI blog, 1(8):9, 2019.





Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language su-pervision. In International conference on machine learn-ing, pages 8748–8763. PMLR, 2021.





Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, andPeter J Liu. Exploring the limits of transfer learning witha unified text-to-text transformer. The Journal of MachineLearning Research, 21(1):5485–5551, 2020.





Kashif Rasul, Arjun Ashok, Andrew Robert Williams, ArianKhorasani, George Adamopoulos, Rishika Bhagwatkar,Marin Bilos, Hena Ghonia, Nadhir Vincent Hassen, An-ˇderson Schneider, et al. Lag-llama: Towards founda-tion models for time series forecasting. arXiv preprintarXiv:2310.08278, 2023.





Paul K Rubenstein, Chulayuth Asawaroengchai, Duc DungNguyen, Ankur Bapna, Zalan Borsos, F ´ elix de Chaumont ´Quitry, Peter Chen, Dalia El Badawy, Wei Han, EugeneKharitonov, et al. Audiopalm: A large language model thatcan speak and listen. arXiv preprint arXiv:2306.12925,2023.





Shezheng Song, Xiaopeng Li, and Shasha Li. How tobridge the gap between modalities: A comprehensive sur-vey on multimodal large language model. arXiv preprintarXiv:2311.07594, 2023.





Dimitris Spathis and Fahim Kawsar. The first step isthe hardest: Pitfalls of representing and tokenizing tem-poral data for large language models. arXiv preprintarXiv:2309.06236, 2023.





Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, andDeng Cai. Pandagpt: One model to instruction-follow themall. arXiv preprint arXiv:2305.16355, 2023.





Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test:Text prototype aligned embedding to activate llm’s abilityfor time series. arXiv preprint arXiv:2308.08241, 2023.





Sabera J Talukder and Georgia Gkioxari. Time series mod-eling at scale: A universal representation across tasks anddomains. 2023.





Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen,Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang.Salmonn: Towards generic hearing abilities for large lan-guage models. arXiv preprint arXiv:2310.13289, 2023.





Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023.





Aaron Van Den Oord, Oriol Vinyals, et al. Neural discreterepresentation learning. Advances in neural informationprocessing systems, 30, 2017.





Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Di-eter Kreiseler, Fatima I Lunze, Wojciech Samek, and To-bias Schaeffter. Ptb-xl, a large publicly available electro-cardiography dataset. Scientific data, 7(1):154, 2020.





Zhenhailong Wang and Heng Ji. Open vocabularyelectroencephalography-to-text decoding and zero-shotsentiment classification. In Proceedings of the AAAI Con-ference on Artificial Intelligence, volume 36, pages 5350–5358, 2022.





Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shu-jie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and FuruWei. Viola: Unified codec language models for speechrecognition, synthesis, and translation. arXiv preprintarXiv:2305.16107, 2023.





Xinglei Wang, Meng Fang, Zichao Zeng, and Tao Cheng.Where would i go next? large language models as hu-man mobility predictors. arXiv preprint arXiv:2308.15197,2023.





Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen,Ziqing Ma, Junchi Yan, and Liang Sun. Transformers intime series: A survey. arXiv preprint arXiv:2202.07125,2022.





Christopher Wimmer and Navid Rekabsaz. Leveragingvision-language models for granular market change predic-tion. arXiv preprint arXiv:2301.10166, 2023.





Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, MarkDredze, Sebastian Gehrmann, Prabhanjan Kambadur,David Rosenberg, and Gideon Mann. Bloomberggpt:A large language model for finance. arXiv preprintarXiv:2303.17564, 2023.





Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, andJimin Huang. The wall street neophyte: A zero-shot analy-sis of chatgpt over multimodal stock movement predictionchallenges. arXiv preprint arXiv:2304.05351, 2023.





Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai,Min Peng, Alejandro Lopez-Lira, and Jimin Huang. Pixiu:A large language model, instruction data and evaluationbenchmark for finance. arXiv preprint arXiv:2306.05443,2023.





Tianwei Xing, Luis Garcia, Federico Cerutti, Lance Kaplan,Alun Preece, and Mani Srivastava. Deepsqa: Understand-ing sensor data via question answering. In Proceedings ofthe International Conference on Internet-of-Things Designand Implementation, pages 106–118, 2021.





Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. 2022.





Hao Xue and Flora D Salim. Utilizing language modelsfor energy load forecasting. In Proceedings of the 10thACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, pages 224–227, 2023.





Hao Xue, Bhanu Prakash Voutharoja, and Flora D Salim.Leveraging language foundation models for human mobil-ity forecasting. In Proceedings of the 30th InternationalConference on Advances in Geographic Information Sys-tems, pages 1–9, 2022.





Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao,Jiang Bian, Xixin Wu, et al. Uniaudio: An audio foun-dation model toward universal audio generation. arXivpreprint arXiv:2310.00704, 2023.





Xinli Yu, Zheng Chen, Yuan Ling, Shujing Dong, ZongyiLiu, and Yanbin Lu. Temporal data meets llm–explainable financial time series forecasting. arXivpreprint arXiv:2306.11025, 2023.





Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat,Sashank J Reddi, and Sanjiv Kumar. Are transformers uni-versal approximators of sequence-to-sequence functions?arXiv preprint arXiv:1912.10077, 2019.





Neil Zeghidour, Alejandro Luebs, Ahmed Omran, JanSkoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Au-dio, Speech, and Language Processing, 30:495–507, 2021.





Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang,Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empoweringlarge language models with intrinsic cross-modal conver-sational abilities. arXiv preprint arXiv:2305.11000, 2023.





Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, ShaoliHuang, Yong Zhang, Hongwei Zhao, Hongtao Lu, andXi Shen. T2m-gpt: Generating human motion from textual





descriptions with discrete representations. arXiv preprintarXiv:2301.06052, 2023.





Jiayun Zhang, Xiyuan Zhang, Xinyang Zhang, Dezhi Hong,Rajesh K. Gupta, and Jingbo Shang. Navigating Align-ment for Non-identical Client Class Sets: A Label Name-Anchored Federated Learning Framework. In Proceedingsof the 29th ACM SIGKDD Conference on Knowledge Dis-covery and Data Mining. ACM, aug 2023.





Xiyuan Zhang, Ranak Roy Chowdhury, Jiayun Zhang, DezhiHong, Rajesh K. Gupta, and Jingbo Shang. Unleashingthe power of shared label structures for human activityrecognition. In Proceedings of the 32nd ACM InternationalConference on Information and Knowledge Management,CIKM ’23, page 3340–3350. Association for ComputingMachinery, 2023.





Yunkai Zhang, Yawen Zhang, Ming Zheng, Kezhen Chen,Chongyang Gao, Ruian Ge, Siyuan Teng, Amine Jelloul,Jinmeng Rao, Xiaoyuan Guo, et al. Insight miner: A timeseries analysis dataset for cross-domain alignment withnatural language. In NeurIPS 2023 AI for Science Work-shop, 2023.





Zheng Zhang, Hossein Amiri, Zhenke Liu, Andreas Zufle,¨and Liang Zhao. Large language models for spatial tra-jectory patterns mining. arXiv preprint arXiv:2310.04942,2023.





Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic,Yulong Cao, Danfei Xu, Marco Pavone, and BaishakhiRay. Language-guided traffic simulation via scene-leveldiffusion. arXiv preprint arXiv:2306.06344, 2023.





Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and RongJin. One fits all: Power general time series analysis bypretrained lm. arXiv preprint arXiv:2302.11939, 2023.





Yunjiao Zhou, Jianfei Yang, Han Zou, and Lihua Xie. Tent:Connect language models with iot sensors for zero-shot ac-tivity recognition. arXiv preprint arXiv:2311.08245, 2023.

