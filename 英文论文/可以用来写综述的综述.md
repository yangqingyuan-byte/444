# A Comprehensive Survey of Deep Learning for Time SeriesForecasting: Architectural Diversity and Open Challenges

Jongseon Kim $^ { 1 , 3 }$ †, Hyungjoon Kim $^ { 1 , 4 }$ †, HyunGi Kim2, Dongjun Lee1,Sungroh Yoon1, 2*

1Interdisciplinary Program in Artificial Intelligence, Seoul National University.2Department of Electrical and Computer Engineering, Seoul National University.3R&D Department, LG Chem.$^ 4$ R&D Department, Samsung SDI.

*Corresponding author(s). E-mail(s): sryoon@snu.ac.kr;Contributing authors: hallooawooye@snu.ac.kr; khjn81@snu.ac.kr;rlagusrl0128@snu.ac.kr; elite1717@snu.ac.kr;†These authors contributed equally to this work.

# Abstract

Time series forecasting is a critical task that provides key information for decision-making acrossvarious fields, such as economic planning, supply chain management, and medical diagnosis.After the use of traditional statistical methodologies and machine learning in the past, variousfundamental deep learning architectures such as MLPs, CNNs, RNNs, and GNNs have been devel-oped and applied to solve time series forecasting problems. However, the structural limitationscaused by the inductive biases of each deep learning architecture constrained their performance.Transformer models, which excel at handling long-term dependencies, have become significantarchitectural components for time series forecasting. However, recent research has shown thatalternatives such as simple linear layers can outperform Transformers. These findings have openedup new possibilities for using diverse architectures, ranging from fundamental deep learning mod-els to emerging architectures and hybrid approaches. In this context of exploration into variousmodels, the architectural modeling of time series forecasting has now entered a renaissance. Thissurvey not only provides a historical context for time series forecasting but also offers comprehen-sive and timely analysis of the movement toward architectural diversification. By comparing andre-examining various deep learning models, we uncover new perspectives and presents the lat-est trends in time series forecasting, including the emergence of hybrid models, diffusion models,Mamba models, and foundation models. By focusing on the inherent characteristics of time seriesdata, we also address open challenges that have gained attention in time series forecasting, suchas channel dependency, distribution shift, causality, and feature extraction. This survey exploresvital elements that can enhance forecasting performance through diverse approaches. These con-tributions help lower entry barriers for newcomers by providing a systematic understanding ofthe diverse research areas in time series forecasting (TSF), while offering seasoned researchersbroader perspectives and new opportunities through in-depth exploration of TSF challenges.

Keywords: Time series forecasting, Deep learning, Foundation model, Distribution shift, Causality

# 1 Introduction

Time series forecasting (TSF) is a task that predicts future values based on sequential historicaldata (Cryer, 1986). It is utilized as a key decision-making tool in various fields, such as economicsand finance, supply chain management, transportation, energy, weather, and healthcare (Danese and

# A Comprehensive Survey of Deep Learning for Time Series Forecasting:Architectural Diversity and Open Challenges

# 1 Introduction

(Section 1)

• Importance of TSF

(Fig.2) Number of Top-tier AI and ML Conference Papers

• History of TSF Model Development

(Fig.3) Evolution of Time Series Forecasting Models

• Limitations of Existing Surveys and Contributions

(Fig.4) Overview of Latest Open Challenges in Time Series Forecasting

(Tab.1) Summary of Survey Papers on Time Series Forecasting

# 2 Background

(Section 2)

[2.1] Time Series Data

(Fig.5) Properties of Time Series Data

[2.2] Time-Series Forecasting

(Fig.6) Properties of Multivariate Time Series Data

[2.3] Time Series Forecasting Datasets.

(Tab.2) Datasets Frequently Used in Time Series Forecasting

[2.4] Evaluation Metrics

(Tab.3) Summary of Datasets Across Domains from Monash Archive

# 3 Historical TSF Models

(Section 3)

[3.1] Conventional Methods

[3.2] Fundamental Deep Learning Models

(Fig.8) Remarkable Historical TSF Models

[3.3] The Prominence of Transformer-based Models

. (Fig.9) Comparison of Full and Sparse Self-Attention Patterns

[3.4] Uprising of Non-Transformer-based Models

# 4 New Exploration of TSF Models

(Section 4)

[4.1] Overcoming Limitations of Transformer

(Fig.10) Patching Technique in Self-Attention for TSF

(Fig.11) Cross-Dimensional Self-Attention for Modeling Variable Relationships

(Tab.5) Taxonomy and Methodologies of Transformer Models for TSF

(Tab.6) Comparison of Other Deep Learning Models with Transformersin Terms of Criteria

[4.2] Growth of Fundamental Deep Learning Models

(Tab.7) Taxonomy and Methodologies of Traditional Deep LearningArchitectures for TSF

[4.3] Emergence of Foundation Models

(Tab.8) Taxonomy and Methodologies of Foundation Models for TSF

[4.4] Advance of Diffusion Models

(Fig.12) Conditional Diffusion Process for Time Series Data

[4.5] Debut of the Mamba

(Tab.9) Taxonomy and Methodologies of Diffusion Models for TSF(Fig.13) Diagram of Discretized State Space Model

(Fig.14) Structure of the Mamba Block

(Tab.10) Taxonomy and Methodologies of Mamba Models for TSF

# 5 TSF Latest Open Challenges & Handling Methods

(Section 5)

[5.1] Channel Dependency Comprehension

(Fig.15) Comparison of CI and CD Strategies in Channel Correlations

[5.2] Alleviation of Distribution Shift

(Fig.16) Recent Approaches to Channel Strategies

(Tab.11) Normalization-Denormalization-based Approaches to AlleviateDistribution Shifts

[5.3] Enhancing Causality

[5.4] Time Series Feature Extraction

(Fig.17) Key Techniques in Time Series Feature Extraction

[5.5] Model Combination Techniques

(Fig.18) Key Ensemble Techniques

[5.6] Interpretability & Explainability

[5.7] Spatio-temporal Time Series Forecasting

# 6 Conclusion

(Section 6)

• Conclusion

• Limitations and Future Work

Fig. 1: Overview of This Survey

Kalchschmidt, 2011; Abu-Mostafa and Atiya, 1996; Alghamdi et al, 2019; Nti et al, 2020; Dimriet al, 2020; Soyiri and Reidpath, 2013). Such applications offer various opportunities, including costreduction, increased efficiency, and enhanced competitiveness (Danese and Kalchschmidt, 2011). Thediversity and complexity of time series data make forecasting challenging. In addition to apparentinformation, hidden patterns and irregular values pose challenges in learning temporal dependencies.(Chen et al, 2024d)

In multivariate problems, additional factors such as channel correlation make the task even moredifficult (Section 2.1). Furthermore, time series data exhibits different characteristics depending onthe domain, and the various times and environments in which the series is collected result in sig-nificantly different patterns (Section 2.3). Because of this, TSF problems exhibit limited modelgeneralizability, requiring diverse architectures and approaches. The increasingly complicated TSFproblems are presenting researchers with growing challenges, which has recently led to the activedevelopment of new methodologies and algorithms to address these issues (Lim and Zohren, 2021).

TSF is closely connected to core AI challenges, and solving these problems plays a crucial role inadvancing the field of AI. For instance, the generalization of AI models is essential for ensuring stableperformance on new data. However, time series data presents unique characteristics and domain-specific patterns that make generalization particularly challenging (Han et al, 2023b). Solutionsto generalization in TSF provide new insights that are applicable to other fields, such as NaturalLanguage Processing (NLP) and Computer Vision (CV). The distribution of real-world time seriesdata can change over time, and models must effectively handle these changes (Section 5.2). TSFplays a crucial role in addressing distribution shift issues, and the techniques developed to solve theseproblems can extend to other AI domains, such as image, video, and NLP. Moreover, attempts tounderstand models are crucial for improving their trustworthiness in real-world applications (Hanet al, 2023a). In mission-critical fields such as healthcare and finance, the explainability of time seriesprediction models is essential. For example, explaining how stock price predictions or patient vitalsign forecasts are made can help users better understand and trust the model’s decisions. Thus,TSF research in explainability provides practical solutions for other AI applications (Section 5.6).Recently, TSF has evolved beyond simple forecasting, establishing itself as a practical platform foraddressing key AI challenges. Solving these problems within TSF highlights the potential for AI todevelop in a complementary and synergistic way.

Fig. 2 shows the number of papers on time series forecasting presented at major AI and machinelearning conferences. This explosive increase demonstrates the growing importance of TSF researchin the AI and machine learning fields. As various studies addressing time series forecasting problemsare being actively conducted, survey papers are also being frequently published. Over time, numeroussurvey papers have systematically organized the vast landscape of TSF, offering in-depth researchthat has provided valuable guidance and direction for researchers. However, existing survey papersstill have room for improvement, particularly in addressing the inevitable increase in model diversityand the open challenges in the field, as summarized in Table 1. This survey aims to reorganize theextensive discourse on TSF by providing essential and comprehensive information. At the same time,it explores model diversity to deliver the latest TSF trends and uniquely highlights open challenges,clearly presenting them to readers.

![](images/15abf7e86b6e8d42f007fa93b45885a69963a43095308f0aacf137907c609a24.jpg)



Fig. 2: Number of Top-tier AI and ML Conference Papers on Time Series Forecasting


Models for TSF have undergone various stages of development over an extended period. In thepast, statistical methods based on moving averages were predominantly used, which later evolved intotraditional approaches such as Exponential Smoothing and ARIMA (Box et al, 1970). Machine learn-ing techniques such as Tree Models (Quinlan, 1986) and Support Vector Machines (SVM) (Cortes,1995) have also been frequently used, but they had limitations in learning complex nonlinear pat-terns (Section 3.1). With the increase in available data and advancements in hardware computingpower, various deep learning architectures such as MLPs (Rumelhart et al, 1986), RNNs (Hopfield,1982), CNNs (LeCun et al, 1998), and GNNs (Scarselli et al, 2008) were developed, enabling thelearning of more complex patterns. However, the performance of these early deep learning architec-tures was constrained by their intrinsic designs. To overcome these structural limitations, variantssuch as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Temporal Con-volutional Networks (TCN) (Bai et al, 2018) have been widely utilized (Section 3.2). Transformers(Vaswani et al, 2017), known for their ability to handle long-term dependencies, have demonstratedexcellent performance in natural language processing and have been naturally extended to time seriesdata as well. While Transformers have shown good performance in TSF and become widely popu-lar, recent cases have shown that simple linear models can outperform Transformer models (Section3.3). As a result, there has been a significant increase in reconsidering fundamental deep learningmethodologies, along with growing interest in various architectures such as foundation models, dif-fusion models, and Mamba models (Section 4.2, 4.3, 4.4, 4.5). The Transformer model continues toimprove in performance and still plays a significant role (Section 4.1). In this way, TSF has entereda renaissance of modeling, with various methodologies actively competing without being dominatedby any single approach (Fig. 3). In this context, this survey offers two major strengths that set itapart from previous TSF survey papers.

First, we focus on the inevitable diversification of architectures, providing a timely and compre-hensive view to understand the current trend of architecture diversification. Existing TSF surveypapers, such as (Wen et al, 2023; Zhang et al, 2024d; Lin et al, 2024a; Meijer and Chen, 2024; Patroand Agneeswaran, 2024a), focus on providing detailed analyses of specific architectures but havelimitations when it comes to broadly comparing diversified architectures, including newly emerg-ing ones. This paper systematically compares the developmental progress of various architectures(MLPs, CNNs, RNNs, GNNs, Transformer, Diffusion, foundation models, Mamba) and analyzes thestrengths, weaknesses, and contributions of each. In addition, it addresses the performance of hybridmodels that combine the strengths of multiple architectures, clearly highlighting key trends in TSF.With these contributions, readers can effectively understand the continuously evolving trends anddirections of advancement in the field. Through this, it lowers the entry barrier for newcomers to TSFand provides a comprehensive roadmap that opens up new research opportunities for establishedresearchers.

Second, we explore from the perspective of open challenges. Although numerous advanced archi-tectures have resolved many issues, the core challenges in TSF continue to persist. In particular,issues such as channel correlation, distribution shifts, causality, and feature extraction (Section 5)remain significant challenges that need to be addressed (Fig. 4). This survey explores the latestmethodologies aimed at addressing these challenges and provides readers with valuable insights forproblem-solving. While some existing surveys address these TSF challenges, they often focus on onlya few problems. In contrast, this paper takes an issue-driven approach, deeply examining seven keyTSF problems and organizing new solutions.

This survey covers the fundamental concepts of time series data and the problem definition offorecasting in Section 2, followed by an examination of the evolution of past methodologies in Section3. Section 4 analyzes the key features of the latest models, and finally, Section 5, explores the openchallenges in TSF and their solutions. Through this, readers will gain a broad understanding of thepast and present of TSF research and acquire new ideas for future studies.

# 2 Background

In this section, before exploring time series forecasting models, we explain the definition and keycharacteristics of time series data to provide the necessary background. We also define the problemof time series forecasting tasks, discussing related datasets and evaluation metrics. This establishesthe basic concepts of time series forecasting and provides preliminary information to help understandthe models discussed in the following sections.


volution of Time Series Forecasting


![](images/118cac2ec7649dffc7881c47b458ff23868b9440dd689ded477d61175d20abb5.jpg)



ummary of Survey Papers on Time Series F


<table><tr><td>Articles</td><td>Focus</td><td>Broad Architecture Review</td><td>TSF Challenges</td><td>Recent Work</td><td>Reference</td></tr><tr><td rowspan="2">Time-series forecasting with deep learning: a survey</td><td>Encoder-Decoder structures and Hybrid models</td><td rowspan="2">✓</td><td rowspan="2">✓</td><td></td><td rowspan="2">Lim and Zohren (2021)</td></tr><tr><td>Interpretability and causal inference for decision support</td><td></td></tr><tr><td rowspan="2">Forecast Methods for Time Series Data: A Survey</td><td>Categorization of various forecasting methods: statistical, ML, and DL</td><td rowspan="2">✓</td><td rowspan="2"></td><td></td><td rowspan="2">Liu et al (2021b)</td></tr><tr><td>Challenges in preprocessing, modeling, and parallel computation</td><td></td></tr><tr><td rowspan="2">Deep Learning for Time Series Forecasting: Tutorial and Literature Survey</td><td>Key components of DL TSF</td><td rowspan="2">✓</td><td rowspan="2"></td><td></td><td rowspan="2">Benidis et al (2022)</td></tr><tr><td>Practical applicability</td><td></td></tr><tr><td rowspan="2">A Review on Deep Sequential Models for Forecasting Time Series Data</td><td>Common deep sequential models</td><td rowspan="2">✓</td><td rowspan="2"></td><td></td><td rowspan="2">Ahmed et al (2022)</td></tr><tr><td>Guidelines on implementation, application, and optimization</td><td></td></tr><tr><td>Transformers in Time Series: A Survey</td><td>Transformer structure variations and architectural improvements</td><td></td><td>✓</td><td></td><td>Wen et al (2023)</td></tr><tr><td rowspan="2">Long Sequence Time-Series Forecasting with Deep Learning: A Survey</td><td>Definition of long sequence forecasting challenges from various perspectives</td><td rowspan="2">✓</td><td rowspan="2">✓</td><td></td><td rowspan="2">Chen et al (2023)</td></tr><tr><td>New classification system and performance evaluation methods</td><td></td></tr><tr><td rowspan="2">Machine Learning Advances for Time Series Forecasting</td><td>Economic/financial TSF</td><td rowspan="2">✓</td><td rowspan="2"></td><td></td><td rowspan="2">Masini et al (2023)</td></tr><tr><td>Categorized into linear and nonlinear</td><td></td></tr><tr><td>Diffusion Models for Time-Series Applications: A Survey</td><td>Diffusion models in time series analysis</td><td></td><td></td><td></td><td>Lin et al (2024a)</td></tr><tr><td>The Rise of Diffusion Models in Time-Series Forecasting</td><td>Diffusion-based forecasting models with case studies</td><td></td><td></td><td>✓</td><td>Meijer and Chen (2024)</td></tr><tr><td>Foundation Models for Time Series Analysis: A Tutorial and Survey</td><td>Application of foundation models in time series analysis</td><td></td><td></td><td>✓</td><td>Liang et al (2024c)</td></tr><tr><td rowspan="2">Large Language Models for Time Series: A Survey</td><td>Categorization of methodologies for LLMs in time series</td><td rowspan="2">✓</td><td rowspan="2"></td><td>✓</td><td rowspan="2">Zhang et al (2024d)</td></tr><tr><td>Explanation of bridging modality gaps</td><td></td></tr><tr><td>A Survey of Time Series Foundation Models</td><td>Approaches to Foundation Models and LLMs in time series analysis</td><td></td><td></td><td>✓</td><td>Ye et al (2024)</td></tr><tr><td rowspan="2">Mamba-360: Survey of State Space Models</td><td>Emphasis on SSM advantages in long time series</td><td rowspan="2">✓</td><td rowspan="2">✓</td><td>✓</td><td rowspan="2">Patro and Agneeswaran (2024a)</td></tr><tr><td>Comparison of domain-specific applications and performance</td><td></td></tr><tr><td rowspan="2">This Survey</td><td>A comprehensive overview of the evolution of TSF models, introducing innovative architectures</td><td rowspan="2">✓</td><td rowspan="2">✓</td><td>✓</td><td rowspan="2"></td></tr><tr><td>Discussion of critical open challenges in forecasting tasks, along with advanced solutions to address them</td><td></td></tr></table>

![](images/6458bc889df8b5c75cb2c31a0e2a54d206e6f73592c6076213eccf1e36515d8e.jpg)



Fig. 4: Overview of Latest Open Challenges in Time Series Forecasting


# 2.1 Time-Series Data

Time series data is a collection of sequential data points gathered at regular time intervals, represent-ing a series of observations of phenomena that change over time. This temporal continuity allows forthe understanding and analysis of phenomena that evolve according to time order. Each data pointrepresents the state or value at a specific moment, and through the observation of these data points,various patterns such as long-term trends, seasonality, cyclicality, and irregularities can be recog-nized. These patterns provide valuable information for predicting future values or detecting changesat critical moments. By extracting and studying the meaningful information provided by time seriesdata, researchers can develop practical applications to address challenges in various disciplines.

# Characteristics of Time Series Data

Time series data encapsulates various characteristics that play a critical role in explaining the diversepatterns and fluctuations within time series. Understanding these characteristic elements is essentialfor analyzing and predicting data. The key properties are explained in Fig. 5.

In time series data, the above features frequently appear in a mixed form. Therefore, decom-position is commonly used to separate the components for detailed analysis, or distribution shiftalleviation methods are widely applied. Many time series datasets provide information on multivari-ate variables. Sometimes, these data provide additional information that univariate data cannot, andit is important to understand this for many problems. The main properties are explained in Fig. 6.

These additional features convey the complexity of the real world in greater detail and sometimesprovide reasons for significant pattern changes. Considering this information can lead to more precisetime series analysis and forecasting.

# 2.2 Time-Series Forecasting

Time series forecasting involves analyzing historical time-series data to predict future values. Thisprocess entails analyzing inherent temporal patterns in the data, such as trends and seasonality,

![](images/dbd594e28fd846f19393142aeec860b55f91ffb8c5e8b204103d08d0324fd47c.jpg)



Fig. 5: Properties of Time Series Data


using past observations to estimate future values. The goal of time series forecasting is to sup-port decision-making by providing information about an uncertain future, such as predicting futuredemand changes or fluctuations in prices. Time series forecasting can employ various methodologies,including statistical methods, machine learning, and deep learning. These methodologies focus oncapturing the characteristics of data as it changes over time.

Univariate Time Series Forecasting (UTSF). Univariate forecasting refers to making predic-tions using only one variable. For example, predicting the next day’s temperature based solely onpast temperature data from a weather station is a univariate forecast. The advantage of univariateforecasting is that the models are simple and computationally efficient. Since the data consists of asingle variable, the model is relatively straightforward and easy to understand, making data collec-tion and management easier. However, it may only utilize limited information as it cannot account

![](images/69026a433f617acea033609565ad7bc5009aecea7fbba10b2a3a65263f73637c.jpg)



Fig. 6: Properties of Multivariate Time Series Data


for important external factors or interactions between different variables.

$$
\hat {x} _ {t + 1: t + h} = f \left(x _ {t - p: t}\right) \tag {1}
$$

The formula representing UTSF is shown in Eq. (1). The model takes the past $p + 1$ time steps,specifically the data from $t - p$ to $t$ , as input to make predictions. Here, $\hat { x } _ { t + 1 : t + h }$ represents thepredicted values from $t + 1$ to $t + h$ . Single-step forecasting predicts only one value when $h = 1$ , whilemulti-step forecasting predicts multiple values when $h > 1$ .

Multivariate Time Series Forecasting (MTSF). Multivariate forecasting involves makingpredictions using multiple variables simultaneously. For instance, in weather forecasting, predictingthe next day’s temperature by considering various variables such as temperature, humidity, andwind speed is an example of multivariate forecasting. By incorporating interactions and correlationsbetween multiple variables, multivariate forecasting can capture complex relationships, offering higherpredictive accuracy. However, these models tend to be more complex, require more data, and can bemore challenging to handle, increasing the risk of overfitting.

To incorporate other influencing variables, we can extend the model to a multivariate approach.

$$
\hat {\mathbf {X}} _ {t + 1: t + h} = f \left(\mathbf {X} _ {t - p: t}\right) \tag {2}
$$

The formula representing MTSF is shown in Eq. (2). $\mathbf { X } _ { t - p : t }$ is a set of vectors over the past $p { + 1 }$ timesteps, where each vector contains the values of multiple variables at time $t$ . This model takes multiplevariables from the past $p + 1$ time steps, specifically $\mathbf { X } _ { t - p : t }$ , as input to generate the multivariatepredicted values from $t + 1$ to $t + h$ , denoted as $\hat { \mathbf { X } } _ { t + 1 : t + h }$ . This multivariate approach allows for morecomprehensive modeling by considering both the target variable’s history and the effects of otherrelevant factors. Fig. 7 illustrates how a multivariate forecasting model uses a past lookback windowto predict future intervals.

![](images/1eb70c9dd5511fcfa7a16eadc77b2fcd9f0af2ec31bffa759acecf377a80c07d.jpg)



Fig. 7: Diagram of Multivariate Time Series Forecasting


Short-Term Time Series Forecasting (STSF). Short-term time series forecasting focuses onpredictions for the near future, making it suitable for tasks that require quick responses, such asimmediate operational planning or short-term decision-making. The models are simple, making themeasy to train and implement, and they often demonstrate relatively high accuracy. However, becausethe forecast range is short, it cannot capture long-term trends or complex variations, which limitsits applicability.

Long-Term Time Series Forecasting (LTSF). Long-term time series forecasting deals withpredictions for the distant future, with forecast horizons increasingly extending to several months,years, or beyond. It is valuable for long-term strategy planning, investment decisions, and policy-making, addressing many real-world problems. By identifying long-term trends and cycles, organiza-tions can prepare accordingly, highlighting its significance. However, predicting the distant future ischallenging, and extensive research is being conducted to improve accuracy.

# 2.3 Time Series Forecasting Datasets

Models for Time Series Forecasting (TSF) are primarily trained and evaluated using open bench-mark datasets. Open datasets offer data that has already been collected across various fields,allowing researchers to use them without going through separate data collection processes. Addi-tionally, researchers can objectively compare the performance of different models and convincinglydemonstrate the contributions of their developed models.

Datasets frequently used by time series forecasting models in various domains, such as energyconsumption, traffic, weather, exchange rates, and disease outbreaks, are listed in Table 2. Thesetime series datasets are typically collected in real-time via sensors, recorded as transaction logs infinancial markets, accumulated as server logs, or gathered through periodic surveys.

Time series data exist across a wide range of domains, each exhibiting unique characteristics.This diversity is due to the nature of time series data, which addresses real-world problems invarious fields and can display highly varied patterns depending on time and environmental factors.For example, stock price data often show long-term upward or downward trends. Stock marketfluctuations tend to move in specific directions over the long term due to complex influences such aseconomic conditions, corporate performance, and policy changes. In contrast, electricity consumptiondata exhibit pronounced periodicity throughout the day, with higher consumption during daytimehours and lower consumption at night. There are also seasonal patterns, with different consumptionrates during summer and winter.

Monash Time Series Forecasting Archive (Godahewa et al, 2021l) points out that there is nocomprehensive time series dataset archive and thus provides a thorough time series repository. Thisrepository offers 30 datasets for real-world data and competition purposes, covering various domainssuch as energy, economics, nature, and sales. Details are specified in Table 3.

Time series data exhibit a greater variety of domain-specific characteristics compared to NaturalLanguage Processing (NLP). These differences in dataset characteristics make time series forecasting


Table 2: Datasets Frequently Used in Time Series Forecasting Models


<table><tr><td>Dataset</td><td>Channel</td><td>Length</td><td>Description</td><td>Frequency</td><td>Source</td></tr><tr><td>ETTm1, ETTm2</td><td>7</td><td>69680</td><td>Electricity Transformer Temperature data from two counties in China for 2 years</td><td>15 mins</td><td>ETDataset</td></tr><tr><td>ETTh1, ETTh2</td><td>7</td><td>17420</td><td>Electricity Transformer Temperature data from two counties in China for 2 years</td><td>Hourly</td><td>ETDataset</td></tr><tr><td>Electricity</td><td>321</td><td>26304</td><td>Hourly electricity consumption data of clients from 2012 to 2014</td><td>Hourly</td><td>UCI Archive</td></tr><tr><td>Traffic</td><td>862</td><td>17544</td><td>Hourly road occupancy rates on San Francisco Bay area freeways for 48 months (2015-2016)</td><td>Hourly, Weekly</td><td>PeMS</td></tr><tr><td>Weather</td><td>21</td><td>52696</td><td>Meteorological data (e.g., humidity, temperature) in Germany</td><td>10 mins</td><td>BGC Jena</td></tr><tr><td>Exchange</td><td>8</td><td>7588</td><td>Daily exchange rates of eight countries from 1990 to 2016</td><td>Daily</td><td>Multivariate Time Series Data</td></tr><tr><td>ILI</td><td>7</td><td>966</td><td>Weekly Influenza-Like Illness data from the U.S. CDC</td><td>Weekly</td><td>CDC FluView</td></tr></table>

problems more diverse and complex, reducing the generalizability of time series forecasting models.Consequently, developing foundation models for TSF is more challenging compared to NLP. Timeseries data lack the well-designed vocabulary and grammar found in NLP, and obtaining vast amountsof data is more difficult than in fields like Computer Vision (CV) or NLP. In the CV field, thereare general benchmark datasets such as MNIST and ImageNet, while in NLP, there are datasets likeGLUE (General Language Understanding Evaluation) and SQuAD (Stanford Question AnsweringDataset). In contrast, the time series analysis field lacks large-scale general benchmark datasets andlarge-scale challenges like those in image and language domains. The most famous time series fore-casting competition is the M-Competitions, which started in 1982 and has only been held six timesto date. The paper “Time Series Dataset Survey for Forecasting with Deep Learning” (Hahn et al,2023) highlights the lack of general benchmark datasets for TSF and analyzes the statistical charac-teristics of datasets across various domains, clustering them to provide easily accessible informationfor researchers. We do not delve into the characteristics of the datasets in detail, but instead pro-vide a brief overview of the key datasets. Recently, the development of foundation models for timeseries has begun, leading to the gradual emergence of general datasets for TSF. Details about TSFfoundation models will be discussed in Section 4.3.

# 2.4 Evaluation Metrics

The evaluation metrics used in TSF play a crucial role in objectively comparing and assessing modelperformance. Common metrics for deterministic models include Mean Absolute Error (MAE) andMean Squared Error (MSE), which are easy to use because they allow intuitive comparisons ofprediction errors. On the other hand, probabilistic models that can reflect uncertainty typically useContinuous Ranked Probability Score (CRPS), which measures the accuracy of distribution forecastsby evaluating the difference between predicted and actual distributions (Matheson and Winkler,1976). However, the limitations of these traditional evaluation metrics have been noted, leading tothe proposal of various performance metrics (Thompson, 1990).

The distinct characteristics of time series data necessitate the use of diverse metrics in TSF. It isessential to assess not only the magnitude of errors but also how well the model learns the temporalpatterns and various features of time series data. Table 4 categorizes evaluation metrics by type andexplains each metric.

Error-based metrics for deterministic models measure the difference between predicted and actualvalues. Explained variance metrics assess how well the model explains the variance of the data (Chiccoet al, 2021). These metrics evaluate how well the model captures key patterns such as seasonality ortrends in the data, thereby aiding in the structural understanding of the predictions. Error metricsfocus on accuracy, but they do not assess overfitting, whereas explained variance evaluates the model’sgeneralization ability. Model selection metrics help identify the optimal model (Portet, 2020). Thesemetrics take into account factors such as sample size and the number of parameters, playing a key rolein selecting the optimal model. Thus, they are particularly useful for choosing the most appropriatemodel for a given dataset. For instance, in noisy environments, MAE is preferred over MSE due toits reduced sensitivity to outliers. Additionally, when evaluating non-stationary data, it is important


ary of Datasets Across Domains from Monash Archive (Godahe


<table><tr><td>Dataset</td><td>Domain</td><td>No. Series</td><td>Min. Length</td><td>Max. Length</td><td>Competition</td><td>Multivariate</td><td>Source</td></tr><tr><td>M1</td><td>Multiple</td><td>1001</td><td>15</td><td>150</td><td>Yes</td><td>No</td><td>Makridakis et al (1982)</td></tr><tr><td>M3</td><td>Multiple</td><td>3003</td><td>20</td><td>144</td><td>Yes</td><td>No</td><td>Makridakis and Hibon (2000)</td></tr><tr><td>M4</td><td>Multiple</td><td>100000</td><td>19</td><td>9933</td><td>Yes</td><td>No</td><td>Makridakis et al (2020)</td></tr><tr><td>Tourism</td><td>Tourism</td><td>1311</td><td>11</td><td>333</td><td>Yes</td><td>No</td><td>Athanasopoulos et al (2011)</td></tr><tr><td>CIF 2016</td><td>Banking</td><td>72</td><td>34</td><td>120</td><td>Yes</td><td>No</td><td>Štepnicka and Burda (2017)</td></tr><tr><td>London Smart Meters</td><td>Energy</td><td>5560</td><td>288</td><td>39648</td><td>No</td><td>No</td><td>Jean-Michel, 2019</td></tr><tr><td>Aus. Electricity Demand</td><td>Energy</td><td>5</td><td>230736</td><td>232272</td><td>No</td><td>No</td><td>Godahewa et al (2021e)</td></tr><tr><td>Wind Farms</td><td>Energy</td><td>339</td><td>6345</td><td>527040</td><td>No</td><td>No</td><td>Godahewa et al (2021b,c)</td></tr><tr><td>Dominick</td><td>Sales</td><td>115704</td><td>28</td><td>393</td><td>No</td><td>No</td><td>Center (2020)</td></tr><tr><td>Bitcoin</td><td>Economic</td><td>18</td><td>2659</td><td>4581</td><td>No</td><td>No</td><td>Godahewa et al (2021f,g)</td></tr><tr><td>Pedestrian Counts</td><td>Transport</td><td>66</td><td>576</td><td>96424</td><td>No</td><td>No</td><td>City of Melbourne, 2020</td></tr><tr><td>Vehicle Trips</td><td>Transport</td><td>329</td><td>70</td><td>243</td><td>No</td><td>No</td><td>fivethirtyeight, 2015</td></tr><tr><td>KDD Cup 2018</td><td>Nature</td><td>270</td><td>9504</td><td>10920</td><td>Yes</td><td>No</td><td>KDD Cup, 2018</td></tr><tr><td>Weather</td><td>Nature</td><td>3010</td><td>1332</td><td>65981</td><td>No</td><td>No</td><td>Sparks et al (2020)</td></tr><tr><td>NN5</td><td>Banking</td><td>111</td><td>791</td><td>791</td><td>Yes</td><td>Yes</td><td>Taieb et al (2012)</td></tr><tr><td>Web Traffic</td><td>Web</td><td>145063</td><td>803</td><td>803</td><td>Yes</td><td>Yes</td><td>Google, 2017</td></tr><tr><td>Solar</td><td>Energy</td><td>137</td><td>52560</td><td>52560</td><td>No</td><td>Yes</td><td>Solar, 2020</td></tr><tr><td>Electricity</td><td>Energy</td><td>321</td><td>26304</td><td>26304</td><td>No</td><td>Yes</td><td>UCI, 2020</td></tr><tr><td>Car Parts</td><td>Sales</td><td>2674</td><td>51</td><td>51</td><td>No</td><td>Yes</td><td>Hyndman, 2015</td></tr><tr><td>FRED-MD</td><td>Economic</td><td>107</td><td>728</td><td>728</td><td>No</td><td>Yes</td><td>McCracken and Ng (2016)</td></tr><tr><td>San Francisco Traffic</td><td>Transport</td><td>862</td><td>17544</td><td>17544</td><td>No</td><td>Yes</td><td>Caltrans, 2020</td></tr><tr><td>Rideshare</td><td>Transport</td><td>2304</td><td>541</td><td>541</td><td>No</td><td>Yes</td><td>Godahewa et al (2021h,i)</td></tr><tr><td>Hospital</td><td>Health</td><td>767</td><td>84</td><td>84</td><td>No</td><td>Yes</td><td>Hyndman, 2015</td></tr><tr><td>COVID Deaths</td><td>Nature</td><td>266</td><td>212</td><td>212</td><td>No</td><td>Yes</td><td>Johns Hopkins University, 2020</td></tr><tr><td>Temperature Rain</td><td>Nature</td><td>32072</td><td>725</td><td>725</td><td>No</td><td>Yes</td><td>Godahewa et al (2021j,k)</td></tr><tr><td>Sunspot</td><td>Nature</td><td>1</td><td>73931</td><td>73931</td><td>No</td><td>No</td><td>Sunspot, 2015</td></tr><tr><td>Saugeen River Flow</td><td>Nature</td><td>1</td><td>23741</td><td>23741</td><td>No</td><td>No</td><td>McLeod and Gweon, 2013</td></tr><tr><td>US Births</td><td>Nature</td><td>1</td><td>7305</td><td>7305</td><td>No</td><td>No</td><td>McLeod and Gweon (2013)</td></tr><tr><td>Solar Power</td><td>Energy</td><td>1</td><td>7397222</td><td>7397222</td><td>No</td><td>No</td><td>Godahewa et al (2021a)</td></tr><tr><td>Wind Power</td><td>Energy</td><td>1</td><td>7397147</td><td>7397147</td><td>No</td><td>No</td><td>Godahewa et al (2021d)</td></tr></table>

to use metrics that effectively capture changes in trends and seasonality. In such cases, cumulativeerror-based metrics or explained variance metrics can be useful.

Probabilistic model evaluation metrics, on the other hand, account for uncertainty, enabling theassessment of both the confidence and variability of predictions. These metrics help evaluate not onlythe predicted values but also the uncertainty surrounding them. Beyond basic error-based metrics,several advanced metrics are used. Interval Metrics assess the confidence intervals of the predictions,considering both the predicted values and their associated uncertainty (Khosravi et al, 2011). Thesemetrics are especially useful in fields such as financial market forecasting and risk management.Quantile Metrics evaluate the accuracy of specific quantiles of predicted values, making them par-ticularly useful when assessing how well various quantiles match the actual values in situations withhigh uncertainty (Koenker and Bassett Jr, 1978). Quantile Metrics are beneficial in areas such asweather forecasting and insurance, where forecasts are assessed across different risk levels. SharpnessMetrics, which evaluate the width of predicted confidence intervals, are used to assess how narrowand accurate the prediction intervals are (Zhao et al, 2020). This metric is valuable in logistics andsupply chain forecasting, where minimizing the width of prediction intervals is crucial.

Thus, various evaluation metrics are crucial in assessing model performance, and each should bechosen based on the specific characteristics of the data and the requirements of the problem. In real-world scenarios, the appropriate use of metrics allows for the selection of the most effective model,which can then be leveraged for efficient decision-making. In contrast, in the field of deep learningresearch, MSE and MAE remain widely used due to their simplicity, intuitiveness, and the factthat they have been extensively utilized in previous studies, making them suitable for performancecomparison (Chen et al, 2023).

# 3 Historical TSF Models

# 3.1 Conventional Methods (Before Deep Learning)

# 3.1.1 Statistical Models

Prior to machine learning and deep learning, traditional statistical models, which laid the founda-tion for analyzing sequential data, were commonly utilized for time series forecasting. Exponentialsmoothing was introduced by Brown (1959), Holt (1957), and Winters (1960) as a method to forecastfuture values using a weighted average of past data. This method operates by computing an exponen-tial weight decay of historical data, assigning greater weight to more recent data. The AutoregressiveIntegrated Moving Average (ARIMA) model was formalized through the work of George Box andGwilym Jenkins in their book “Time Series Analysis: Forecasting and Control” (Box et al, 1970).The ARIMA model predicts future values by leveraging the autocorrelation in data and is composedof three main components. AutoRegressive (AR) uses a linear combination of past values to predictthe current value. Moving Average (MA) employs a linear combination of past error terms to predictthe current value. Integrated (I) removes non-stationarity (the property where mean and variancechange over time) by differencing the data to achieve stationarity. While exponential smoothing mod-els are advantageous for real-time data analysis due to their simplicity, ARIMA models are bettersuited for capturing complex patterns, making them ideal for long-term forecasting. The SARIMAmodel extends ARIMA by incorporating seasonal differencing, along with seasonal autoregressive andmoving average components, allowing it to effectively model and predict data with regular cyclicalpatterns. These statistical models are based on specific assumptions and are simple, intuitive, anduseful for identifying basic patterns in data.

# 3.1.2 Machine Learning Models

While statistical methods struggle to capture complex patterns in time series data due to theirreliance on predefined linear relationships, machine learning models excel in learning nonlinear pat-terns directly from the data without relying on such assumptions. To address the limitations ofstatistical methods, traditional machine learning models have been increasingly applied to time seriesforecasting. Decision Trees (Quinlan, 1986) are machine learning models that use a tree structurefor classification or prediction. The term “classification and regression tree (CART) analysis” wasfirst introduced by Barlin et al (2013), and in time series forecasting, regression decision trees areused to split data into a tree structure to predict continuous values. While they are intuitive andeasy to interpret, they are prone to overfitting. Support Vector Machines (SVM), introduced by


Evaluation Metrics for Time Series Fo


<table><tr><td>Evaluation Type</td><td>Category</td><td>Metric</td><td>Formula</td><td>Explanation</td></tr><tr><td rowspan="15">Deterministic</td><td rowspan="3">Error-Based: Absolute</td><td>Mean Absolute Error (MAE)</td><td>\(\frac{1}{n}\sum_{t=1}^{n}|yt - \hat{y}_t|\)</td><td>Measures the average magnitude of the errors in a set of predictions, without considering their direction.</td></tr><tr><td>Mean Squared Error (MSE)</td><td>\(\frac{1}{n}\sum_{t=1}^{n}(yt - \hat{y}_t)^2\)</td><td>Measures the average of the squared errors.</td></tr><tr><td>Root Mean Squared Error (RMSE)</td><td>\(\sqrt{\frac{1}{n}\sum_{t=1}^{n}(yt - \hat{y}_t)^2}\)</td><td>Square root of the average of squared errors, giving higher weight to larger errors.</td></tr><tr><td rowspan="2">Error-Based: Relative</td><td>Mean Absolute Percentage Error (MAPE)</td><td>\(\frac{1}{n}\sum_{t=1}^{n}\left|\frac{yt - \hat{y}_t}{yt}\right| \times 100\)</td><td>Measures the size of the error in percentage terms.</td></tr><tr><td>Symmetric Mean Absolute Percentage Error (sMAPE)</td><td>\(\frac{1}{n}\sum_{t=1}^{n}\frac{|yt - \hat{y}_t|}{(|yt| + |yt|)/2} \times 100\)</td><td>Measures the accuracy based on relative error.</td></tr><tr><td rowspan="2">Error-Based: Cumulative</td><td>Mean Forecast Error (MFE)</td><td>\(\frac{1}{n}\sum_{t=1}^{n}(yt - \hat{y}_t)\)</td><td>Average of forecast errors, indicating bias.</td></tr><tr><td>Cumulative Forecast Error (CFE)</td><td>\(\sum_{t=1}^{n}(yt - \hat{y}_t)\)</td><td>Sum of all forecast errors, measures total bias over the forecast horizon.</td></tr><tr><td>Error-Based: Scaled</td><td>Mean Absolute Scaled Error (MASE)</td><td>\(\frac{1}{n}\sum_{t=1}^{n}|yt - \hat{y}_t| / \frac{1}{n-1}\sum_{t=2}^{n}|yt - \hat{y}_t - 1|\)</td><td>MAE scaled by the MAE of a naive forecast.</td></tr><tr><td rowspan="3">Explained Variance Metrics</td><td>Coefficient of Determination (R²)</td><td>1 - \(\frac{\sum_{t=1}^{n}(yt - \hat{y}_t)^2}{\sum_{t=1}^{n}(yt - \hat{y})^2}\)</td><td>Proportion of variance explained by the model.</td></tr><tr><td>Adjusted Coefficient of Determination (Adjusted R²)</td><td>1 - \(\frac{(1-R^2)(n-1)}{n-k-1}\)</td><td>R² adjusted for the number of predictors.</td></tr><tr><td>Explained Variance Score (EVS)</td><td>1 - \(\frac{\text{Var}(y_t - \hat{y}_t)}{\text{Var}(y_t)}\)</td><td>Measures the proportion of variance explained by the model.</td></tr><tr><td rowspan="4">Model Selection Metrics</td><td>Akaike Information Criterion (AIC)</td><td>2k - 2ln(L)</td><td>Trade-off between goodness of fit and model complexity.</td></tr><tr><td>Bayesian Information Criterion (BIC)</td><td>k ln(n) - 2ln(L)</td><td>Similar to AIC with a stronger penalty for models with more parameters.</td></tr><tr><td>Hannan-Quinn Criterion (HQC)</td><td>AICc = 2k - 2ln(L) + \(\frac{2k(k+1)}{n-k-1}\)</td><td>Alternative to AIC and BIC with different penalty terms.</td></tr><tr><td>Corrected Akaike Information Criterion (AICc)</td><td>AIC + \(\frac{2k(k+1)}{n-k-1}\)</td><td>AIC with correction for small sample sizes.</td></tr><tr><td rowspan="7">Probabilistic</td><td rowspan="2">Error-Based Metrics</td><td>Logarithmic Score (Log Score)</td><td>-\(\frac{1}{n}\sum_{t=1}^{n}\log(pt)\)</td><td>Evaluates the difference between predicted probabilities and actual outcomes using a logarithmic function.</td></tr><tr><td>Continuous Ranked Probability Score (CRPS)</td><td>\(\int_{-\infty}^{\infty}(\hat{F}(z) - 1(z \geq y_t))^2 dz\)</td><td>Evaluates the difference between the predicted probability distribution and the observed value using the cumulative distribution function.</td></tr><tr><td rowspan="2">Interval Metrics</td><td>Prediction Interval Coverage Probability (PICP)</td><td>\(\frac{1}{n}\sum_{t=1}^{n}I(y_t \in [\hat{y}_{lower}, t, \hat{y}_{upper}, t]\)</td><td>Measures the proportion of observed values that fall within the predicted intervals.</td></tr><tr><td>Prediction Interval Width (PIW)</td><td>\(\frac{1}{n}\sum_{t=1}^{n}(\hat{y}_{upper}, t - \hat{y}_{lower}, t)\)</td><td>Evaluates precision by measuring the width of prediction intervals.</td></tr><tr><td rowspan="2">Quantile Metrics</td><td rowspan="2">Quantile Loss (Pinball Loss)</td><td>\(\frac{1}{n}\sum_{t=1}^{n}\left\{\tau \cdot (yt - \hat{y}_t) \quad if \ y_t \geq \hat{y}_t \right\}\)</td><td>Penalizes over- and under-predictions based on quantile τ.</td></tr><tr><td>\(\left(1 - \tau\right) \cdot (\hat{y}_t - y_t) \quad if \ y_t &lt; \hat{y}_t\)</td><td></td></tr><tr><td>Sharpness Metrics</td><td>Sharpness</td><td>\(\frac{1}{n}\sum_{i=1}^{n}\operatorname{Var}(\hat{y}_i)\)</td><td>Evaluates the concentration by the width of intervals or variance.</td></tr></table>

Cortes and Vapnik (Cortes, 1995), are supervised learning models used for classification and regres-sion analysis, characterized by finding the maximum margin. They handle high-dimensional dataand non-linearities effectively, making them robust for classification and regression tasks. SupportVector Regression (SVR) applies SVM concepts to regression problems, predicting data points usingan optimal hyperplane (Vapnik et al, 1996). This hyperplane is trained to minimize errors betweendata points, ensuring errors do not exceed a certain threshold. Gradient Boosting Machines (GBM)were developed in 1999 with an explicit regression gradient boosting algorithm. This method usesensemble learning to combine multiple weak models into a single, strong predictive model (Friedman,2001). This method iteratively improves the model, offering high predictive performance. XGBoost,an extension of the Gradient Boosting algorithm (Chen and Guestrin, 2016), incorporates variousoptimization techniques to enhance efficiency and performance. It gained significant attention forits outstanding performance in machine learning competitions, particularly in time series forecast-ing challenges. These machine learning models outperform traditional statistical models in capturingdata structures and patterns, demonstrating high predictive power on large datasets (Kontopoulouet al, 2023). Their ability to automatically learn and optimize various data features and relationshipshas led to their widespread use in time series forecasting.

# 3.2 Fundamental Deep Learning Models

# 3.2.1 MLPs: The Emergence and Constraints of Early Artificial NeuralNetworks

The development of the Multi-layer Perceptron (MLP) (Rumelhart et al, 1986) and the back-propagation algorithm established the foundation for artificial neural networks. Early artificial neuralnetwork models utilizing these MLPs demonstrated strong performance in modeling nonlinear pat-terns, prompting numerous attempts to apply them to time series data processing. However, severalkey limitations, as outlined below, restricted the training of deep neural networks using MLPs.

• Limited in learning temporal dependencies: MLPs are well-suited for processing fixed-lengthinput vectors but struggle to adequately capture the temporal dependencies in time series data.

• Vanishing gradient issue: The vanishing gradient problem in deep networks made trainingdifficult.

• Lack of data and computing resources: The scarcity of large-scale datasets and high-performance computing resources made it challenging to effectively train complex neural networkmodels.

At that time, artificial neural network technology was still immature, and there was a lack ofdeep understanding and effective methodologies for dealing with time series data. Consequently,traditional statistical methods and machine learning models previously discussed continued to bewidely used for time series analysis.

# 3.2.2 RNNs: The first neural network capable of processing sequential dataand modeling temporal dependencies

# The Emergence and Early Applications of RNNs

Recurrent Neural Networks (RNNs) (Hopfield, 1982) emerged, opening up new possibilitiesfor processing time series data. RNNs are specialized models designed to process sequential data,such as time series data, and were utilized to overcome the limitations of MLPs. The structure ofan RNN consists of an input layer, a hidden layer, and an output layer, and it uses the output fromprevious time steps(hidden layer output) as the input for the current time step. This structural featureallows RNNs to naturally model the dependencies in data over time. The hidden state preservespast information by storing data from the previous step and passing it on to the next step, therebymaintaining historical context. Additionally, it is parametrically efficient because the same weightsare used for calculations across all time steps, regardless of the sequence length.

Due to these features, RNNs have been utilized in various fields where sequential data modelingis crucial, such as time series data analysis, natural language processing (NLP), and so on. However,early RNNs had the following issues.

• Vanishing gradient problem: When RNNs learn long sequences, the gradient gradually dimin-ishes during the backpropagation process, causing information from earlier time steps to fail toreach later time steps. Thus, this made it difficult to learn long-term dependencies.

• Exploding gradient problem: In contrast to the vanishing gradient problem, the gradient valuescould become excessively large, making the training process unstable.

• Difficulty in parallel processing: Due to the sequential nature of RNNs, where the computationat each time step depends on the previous one, parallel processing becomes challenging. This leadsto high computational costs and slow learning speeds.

Because of these critical drawbacks, RNNs were not widely used until the early 2000s.

# Overcoming the Limitations of RNNs

To address the aforementioned issues with vanilla RNNs, researchers began developing various mod-els. Notably, the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) modelwas developed to address the long-term dependency issues of vanilla RNNs. With the cell stateand gate mechanisms (input, output, and forget gates), LSTMs preserve crucial information overextended periods and discard unnecessary information. The Gated Recurrent Unit (GRU) (Choet al, 2014) emerged as a simpler alternative to LSTM, offering similar performance while using amore straightforward structure(update and reset gates) to manage information. These two modelseffectively addressed the vanishing gradient problem and sparked a boom in RNN-based models fortime series analysis. Many subsequent RNN-based models used these two as their backbone, makingthem powerful tools for handling time series data until the advent of Transformers.

# Notable RNN Variants

The Dilated RNN (Chang et al, 2017) proposed Dilated Recurrent Skip Connections as a solutionto various issues inherent in fundamental vanilla RNNs. This approach enhanced parameter effi-ciency in long sequences and enabled the network to effectively learn complex dependencies acrossdiverse temporal scales. The DA-RNN (Qin et al, 2017) attempted to effectively address long-term dependencies, which were challenging for existing methods, by employing a dual-stage attentionmechanism. Utilizing an encoder with Input Attention and a decoder with Temporal Attention, itadaptively selected salient information from input features and temporal steps, thereby enhancing theaccuracy and performance of time series predictions. Additionally, the MQ-RNN (Wen et al, 2017)combined the Sequence-to-Sequence neural network framework with Quantile Regression to provideprobabilistic forecasts, thereby reflecting uncertainties across various scenarios. This model simulta-neously generated multiple forecast horizons at each time step and employed the Forking-Sequencestraining method to enhance the stability and performance of the RNN.

# 3.2.3 CNNs: Extracting key patterns in time series data beyond just images

# The Emergence and Early Applications of CNNs

The Neocognitron (Fukushima, 1980) was designed for visual pattern recognition and served asan early form of Convolutional Neural Networks (CNNs), introducing the fundamental concepts ofCNNs. The CNN architecture, which became more widely known with the development of LeNet(LeCun et al, 1998), learned spatial patterns in images through a combination of convolutional andpooling layers. These features made CNNs well-suited for handling image data, leading early CNN-based models to primarily focus on image data without being directly applied to time series data.However, over time, their potential for handling time series data has also been recognized.

# Attempts to Apply CNNs to Time Series Data

1D CNNs use one-dimensional convolutional filters to learn local patterns in time series data, allow-ing them to extract features that consider the temporal structure of the data. A prominent exampleis WaveNet (Van Den Oord et al, 2016), which utilized 1D convolutions and dilated convolutions tomodel speech signals, effectively capturing long-term dependencies in audio signals. WaveNet demon-strated the utility of CNN-based models in speech synthesis and time series prediction, leading to thewidespread adoption of CNN-based models for time series analysis. The development of the Tempo-ral Convolutional Networks (TCNs) (Bai et al, 2018) model further highlighted the potentialof CNN-based models in the time series domain. TCN consists of multiple layers of 1D convolutionalnetworks, using dilated convolutions at each layer to achieve a wide receptive field, enabling the

construction of deeper networks. This allows TCNs to effectively learn from long sequences of data.TCNs have demonstrated excellent performance in various sequential domains, including time seriesprediction, signal processing, and natural language processing.

# CNN and RNN Hybrid Models

Building on the popularity of fundamental time series forecasting models like RNNs and the emerginginterest in CNN models, hybrid CNN-RNN models began to emerge, combining the strengths ofboth architectures. Models that combined CNNs and LSTMs were particularly advantageous forsimultaneously learning local patterns and long-term dependencies in time series data. The structureinvolved using CNNs to extract complex features from the time series data and LSTMs to learntemporal dependencies.

DCRNN (Li et al, 2018) modeled traffic flow as a diffusion process on a directed graph, cap-turing both temporal and spatial dependencies. The Diffusion Convolution leveraged bidirectionalrandom walks on the graph to capture spatial dependencies within the traffic network, while theGated Recurrent Units (GRU) modeled temporal dependencies. The introduction of the DiffusionConvolutional Gated Recurrent Unit (DCGRU) enabled the effective processing of temporal informa-tion. TPA-LSTM (Shih et al, 2019) combined CNN and LSTM with an added attention mechanism,introducing the Temporal Pattern Attention (TPA) methodology to learn important patterns intime series data. Local patterns were extracted using CNN, while the combination of LSTM and theattention mechanism captured significant temporal patterns.

# 3.2.4 GNNs: Structurally modeling relationships between variables

# The Emergence and Slow Growth of GNN-Based Models

Graph Neural Networks (GNNs) (Scarselli et al, 2008) were primarily developed to pro-cess graph-structured data because they can effectively model the complex structural relationshipsbetween nodes and edges in graph data. Over time, GNNs gradually began to be applied to time seriesdata analysis, primarily because multivariate time-series data often encapsulates intricate structuralrelationships.

During their initial development, GNNs were less popular compared to widely used RNN orCNN-based models. The prevailing belief was that GNNs were not suitable for time series analysiscompared to CNNs which are effective at extracting local patterns, and RNNs which are strong atlearning long-term dependencies. However, as the range and complexity of time-series data increased,understanding their structural characteristics became more important. Consequently, GNNs beganto gain prominence in fields such as traffic prediction and social networks. Research on applyingGNNs to time-series data has primarily focused on capturing the dynamic characteristics of graphdata, and the structural learning capabilities of GNNs have proven to be highly useful in capturingthe complex patterns within time-series data.

# Development and Expansion of GNN Applications

Until the development of Transformers, GNNs consistently garnered interest, showing consistent yetslow growth compared to RNNs and CNNs. The development of Graph Convolutional Networks(GCNs) (Kipf and Welling, 2017) marked another turning point for GNNs. GCNs learn node featuresthrough convolution operations on graph structures and are powerful tools for processing graph data.This advancement positioned GCNs as a versatile solution for addressing a wide range of graph-basedchallenges.

GCNs were more effective at learning the relationships between variables in time series andtemporal locality, and subsequently, GNNs began to be widely used for time-series data. Spatial-Temporal Graph Convolutional Networks (ST-GCN) (Yan et al, 2018) were developed as amodel combining GCNs and RNNs to learn complex patterns in spatio-temporal data. By leveragingGCNs to learn spatial patterns and RNNs to capture temporal patterns effectively, ST-GCN demon-strated the potential of GNNs in time-series problems, such as traffic prediction. Graph AttentionNetworks (GATs) (Veliˇckovi´c et al, 2018) were introduced, learning relationships between nodesusing an attention mechanism. This allowed GATs to capture the importance of nodes, making themadvantageous for identifying significant events in time-series data. Dynamic Graph Neural Net-works (DyGNNs) (Ma et al, 2020) were developed to learn graph structures that change overtime. Designed to reflect the dynamic characteristics of time-series data, DyGNNs could effectivelycapture complex temporal dependencies and structural relationships by allowing the graph structure

to evolve over time. Temporal Graph Networks (TGNs) (Rossi et al, 2020) were introduced asa model in which the nodes of a graph change over time, enabling the learning of temporal patternsin time-series data. By using a memory module to track the temporal state changes of nodes, TGNslearn the information of neighboring nodes whose importance varies over time, thus providing a basisfor predicting future states.

After the era of traditional statistical techniques and machine learning methodologies, funda-mental deep learning approaches such as RNNs, CNNs, and GNNs were widely used for time-seriesanalysis and prediction over a long period. However, the advent of Transformers brought a revo-lutionary change to the AI field. Upon their introduction, Transformers outperformed all existingmethods across various domains and continue to dominate as the leading foundation model. In thetime-series domain, a similar trend was observed. Beginning with the introduction of the LogsparseTransformer (Li et al, 2019), numerous transformer-based models emerged, achieving state-of-the-art performance. As a result, fundamental deep learning methods experienced a period akin to the IceAge. The remarkable historical TSF models mentioned above are organized chronologically in Fig. 8.

![](images/5e5240b5e40becfc5a2daf29586673669034582884e641fd1be01f3769433d78.jpg)



Fig. 8: Remarkable Historical TSF Models


# 3.3 The Prominence of Transformer-based Models

Transformer (Vaswani et al, 2017), is an innovative model designed to perform complex sequence-to-sequence tasks in natural language processing. This model features an encoder-decoder structureand utilizes a self-attention mechanism to capture the relationships between tokens in the inputsequence.

Modeling approaches in self-attention can be broadly categorized into unidirectional and bidi-rectional paradigms. Unidirectional modeling restricts the self-attention mechanism to consider onlythe tokens preceding the current token in the sequence. For a sequence $( x _ { 1 } , x _ { 2 } , \dots , x _ { L } )$ , the attentionfor token $x _ { t }$ is computed over the subsequence $( x _ { 1 } , x _ { 2 } , \ldots , x _ { t } )$ to ensure causality, where $L$ denotesthe length of the input sequence. The unidirectional modeling is predominantly employed in tasksrequiring sequential generation, such as language generation (Brown et al, 2020; Dai et al, 2019). Incontrast, bidirectional modeling allows the self-attention mechanism to utilize the entire sequence.For token $x _ { t }$ , attention is computed over $( x _ { 1 } , x _ { 2 } , \dots , x _ { L } )$ , thereby leveraging both preceding and suc-ceeding tokens. The bidirectional modeling is employed in tasks requiring holistic context, such asmasked token prediction (Devlin et al, 2019; Clark et al, 2020).

As a result, Transformers have gained significant attention for replacing fundamental RNN-basedmodels, offering parallel processing capabilities, and effectively addressing long-term dependencyissues. Transformers’ success with sequential data naturally led to their extension into time seriesapplications. The attention mechanism plays a significant role in TSF by considering the relation-ships across all time steps simultaneously, giving more weight to important moments, and effectivelylearning the trends and patterns in the data.

This section explores various Transformer variants that address the limitations of the originalTransformer for time series forecasting tasks. It discusses the background of these variants, how they

improve upon the original Transformer, and the reasons for their enhanced performance in time seriesprediction. Additionally, it addresses the remaining limitations of these variant models.

# 3.3.1 Transformer Variants

In recent years, substantial research has focused on transformer-based time series analysis, particu-larly long-term time series forecasting (LTSF), which has led to the development of various models(Zeng et al, 2023). The original Transformer model has several limitations when applied to long-termtime series forecasting (TSF). These limitations include quadratic time and memory complexity, aswell as error accumulation caused by the auto-regressive decoder design. Specifically, the time andmemory complexity of self-attention increases quadratically with the length of the input sequence,denoted as $O ( L ^ { 2 } )$ , where $L$ represents the length of the input sequence.

This high complexity can become a computational bottleneck, especially for time series forecastingtasks that rely on long-term historical information. To address these issues, Transformer variantsprimarily focus on reducing time and memory complexity. An attention pattern determines whichtokens a token attends to. Fig. 9 compares Full Self-Attention and Sparse Self-Attention patterns toillustrate the efficiency of Sparse Attention (Beltagy et al, 2020). In Fig. 9a, all tokens attend to eachother, sharing global information. In contrast, Fig. 9b shows that only a subset of tokens is attendedto, reducing computational cost while preserving essential information.

![](images/7b13bf7be524fe506925c6f70369fc177aaab8141b89673870afa99efd588ffa.jpg)


![](images/4674c38752bfb9813f691eebc37f67f706708dd83bb19441d0ded881d8a71deb.jpg)



Fig. 9: Comparison of Full and Sparse Self-Attention Patterns


LogTrans (Li et al, 2019) leverages the efficiency of Log Sparse attention mechanisms to reducetime and memory complexity to $O ( L l o g L )$ , making it well-suited for long sequence forecasting tasks.This model assumes recent data is more relevant than older data, utilizing positional sparsity to focusattention only on relevant data. Reformer (Kitaev et al, 2020) enhances efficiency through locality-sensitive hashing and reversible residual layers, reducing memory and computational requirementsto $O ( L l o g L )$ while maintaining performance on long sequences. Locality-sensitive hashing (LSH)approximates attention by clustering similar items and performing attention partially on these clus-ters. Informer (Zhou et al, 2021) improves computational and memory efficiency to $O ( L l o g L )$ byutilizing a ProbSparse self-attention mechanism and a generative-style decoder. It effectively handleslong sequences and captures dependencies across various time scales. Informer addresses the limi-tations of Reformer, which performs well only on extremely long sequences, and LogTrans, whichuses heuristic positional-based attention. ProbSparse self-attention features dynamic query selec-tion based on query sparsity measurement. Autoformer (Wu et al, 2021) integrates time seriesdecomposition into the internal blocks of the Transformer and utilizes an auto-correlation mechanismto aggregate similar sub-series. Autoformer improves upon previous methods by addressing point-wise dependency and aggregation issues. It employs a decomposition architecture to handle complextemporal patterns by separating seasonal and trend components. Pyraformer (Liu et al, 2021a)introduces a novel pyramid attention mechanism designed to efficiently model long-term dependen-cies in time series data. By using a hierarchical structure to progressively reduce sequence length,Pyraformer significantly lowers computational complexity compared to fundamental Transformer

models. Fedformer (Zhou et al, 2022) combines Fourier-enhanced decomposition blocks with fre-quency domain sparse representations, integrating seasonal-trend decomposition from Autoformerand offering a more effective representation of time series data. The sparse representation basis canbe Fourier-based or Wavelet-based, encompassing a subset of frequency components, including bothhigh and low frequencies. Additionally, attempts have been made to overcome the limitations of nor-malized Transformers, which often produce indistinguishable attention patterns and fail to capturesignificant temporal dependencies. Non-stationary Transformer (Liu et al, 2022) identifies thisissue as excessive normalization and addresses it with De-stationary Attention, which allows for theuse of non-normalized data during QKV computations.

# 3.3.2 Limitation of Transformer-based Models

However, even the models designed specifically for time series forecasting based on the originalTransformer still have the following limitations.

# Efficiency Challenges

Efforts to overcome the primary drawback of quadratic scaling in computational and memory com-plexity with window length have not fully resolved the issues. Various approaches to reduce thiscomplexity have been proposed, but the attention mechanism inherently incurs higher computationaland memory costs compared to MLP-based or convolutional models with $O ( L )$ complexity. ApplyingTransformers to the time domain does not easily mitigate this problem. Models such as Fedformer(Zhou et al, 2022), which uses Fourier or Wavelet transformations to operate in the frequency domain,have been made to address these issues. Although research into more efficient attention variants isongoing (Tay et al, 2020), these solutions often sacrifice some of the effective characteristics of Trans-formers. Sparse attention mechanisms, used to reduce Self-Attention complexity, may result in theomission of important information. Models like LogTrans and Pyraformer introduce explicit sparsitybiases in the attention mechanism but may suffer from significant performance degradation due tothe loss of crucial information. None of these variants have yet been proven effective across diversedomains (Gu and Dao, 2024).

# Context Window Limited to Current Input

In principle, Transformers can handle inputs of arbitrary length, provided there are sufficient compu-tational resources. However, transformers must process the entire context at once in a single input,causing the memory and computation requirements to grow substantially with longer context length.By contrast, RNNs and State Space Models (SSMs) maintain hidden states over time, allowing themto preserve context beyond the immediate input. We highlight this practical limitation, which oftenarises when handling very long contexts.

# Ineffectiveness of Expanding Input Window Length

Another significant issue is the minimal or no performance improvement observed when increasing theinput window length. Strong time series forecasting (TSF) models are generally expected to achievebetter results with larger look-back window sizes due to their robust temporal relationship extractioncapabilities. However, research (Zeng et al, 2023; Zhou et al, 2021; Wen et al, 2023) indicates that theperformance of Transformers either deteriorates or remains stable as the look-back window increases,in contrast to improvements seen with linear-based methods (Zeng et al, 2023). This suggests thattransformer-based models tend to overfit noise rather than extract long-term temporal informationwhen provided with longer sequences.

In conclusion, while transformer-based models have made significant advancements in time series fore-casting, they still face limitations. Therefore, future research will continue to address these limitationsand will also serve as a catalyst for re-exploring various alternative architectures.

# 3.4 Uprising of Non-Transformer-based Models

As previously discussed, transformer-based models demonstrate strong performance in processingtime series data. However, they have several limitations when dealing with long-term time series data.

• The point-wise operations of the self-attention mechanism have quadratic time complexity.Therefore, as sequence length increases, the computational load grows exponentially, making itchallenging to handle long-term historical information.

• Because storing the relationship information for all input token pairs requires substantial memoryusage, applying Transformers in environments with limited GPU memory becomes challenging.

• When the length of the look-back window exceeds the structural capacity, learning long-termdependencies becomes challenging.

• Due to the high complexity of the model, large-scale and high-quality datasets are required. In theabsence of sufficient data, overfitting can occur leading to a drop in model performance.

Contemporary TSF tasks increasingly require the prediction of diverse multivariates and longsequences. Unlike earlier tasks that involved relatively short sequences, the limitations of transformer-based models have become more pronounced when dealing with long sequences. Researchers adheringto the Transformer’s philosophy have begun focusing intensively on two aspects to address theseissues: reducing computational complexity and enhancing long-term dependency learning. Conversely,some researchers have reverted to non-transformer methodologies, renewing interest in fundamentaldeep learning models such as RNNs, CNNs, GNNs, and MLPs. Non-transformer-based models havethe potential to overcome the limitations of Transformers, and research has begun to leverage thesestrengths to move beyond the hegemony of the Transformer backbone.

RNN-based models sequentially process input data, retaining the previous state at each timestep to predict the next state. This enables the natural modeling of long-term dependencies andhelps overcome the limitations of Transformers by leveraging time-ordering learning. They requirerelatively less data and memory, making them less restrictive in their application. Additionally, theirability to handle real-time streaming data enhances their versatility across various applications.

Convolution operations in CNN-based models offer lower time complexity and reduced mem-ory requirements compared to the self-attention mechanism in Transformers. Since time series dataoften exhibit periodic trends and seasonality, the ability of convolution to extract local patternscomplements Transformers’ specialization in learning global patterns, addressing their shortcomings.

GNN-based models can effectively represent and learn the complex relationships and struc-tures of time series data through nodes and edges. They integrate localities efficiently through localoperations, resulting in lower memory usage. Additionally, their ability to model both spatial andtemporal relationships simultaneously makes them advantageous for handling multivariate datasets.

The primary advantage of MLP-based models lies in their simple structure, which facilitatesefficient training, fast inference, and straightforward handling. Current AI models require lightweightdesigns suitable for mobile applications, and MLP-based models are well-suited for environmentswith many constraints.

Especially, the counterattack of simple linear models based on MLPs is one of the notable trendshifts. The emergence of LTSF-Linear (Zeng et al, 2023) models which surpass transformer-basedmodels using only simple linear layers has triggered this change. One of the key characteristics oftime series data is that the time ordering of data points holds important meaning. Conversely, thepermutation-invariant nature of the Transformer’s attention mechanism, which emphasizes relation-ships between data points, is argued to be inherently unsuitable for time series data. Althoughpositional embedding attempts to preserve time order, the multi-head attention process inevitablyleads to information loss (Zhou et al, 2021).

LTSF-Linear models highlighted the critical limitations of Transformers in handling longsequences, demonstrating that simple linear models could better preserve time-ordering informationand extract features related to trends and seasonality.

This claim gained further traction through subsequent studies, leading to a more diversified inter-est in backbone models across various architectures. The emergence of this model sparked skepticismregarding the performance of Transformer-based models and served as a catalyst for the explorationof various alternative architectures. The era of Transformer prevalence has come to an end, usheringin a renaissance in time series research, characterized by a reevaluation of fundamental DNNs, theemergence of novel models like Mamba, and new approaches such as the utilization of LMMs.

# 4 New Exploration of TSF Models

# 4.1 Overcoming Limitations of Transformer

As previously mentioned in 3.4, the performance of simple linear models in LTSF-Linear (Zeng et al,2023) surpassing fundamental transformer-based models has raised doubts about the effectiveness ofTransformers. However, in the field of NLP, Transformers still demonstrate superior performance inhandling long-term dependencies in sequential data compared to other models (Patwardhan et al,2023). This observation suggests that while Transformers have great potential, researchers have notfully leveraged their capabilities in time series analysis.

Therefore, various methods have emerged in time series forecasting to overcome the limitationsof existing Transformer-based models. This section categorizes and explains in detail the specificlimitations of existing models and how these limitations are addressed. The structure begins withan introduction to the patching technique, followed by the use of cross-dimension and exogenousvariables, and then provides a detailed explanation of other approaches, concluding with a summaryof key points in Table 5.

# 4.1.1 Patching Technique

Transformers were originally developed for natural language processing (NLP), and applying them totime-series analysis requires appropriate adjustments to fit the domain of time series. Different fromthe rich semantic information carried by individual word tokens in NLP, individual data points in timeseries are often similar to their neighboring values, lacking substantial information (Nie et al, 2023).Therefore, the point-wise attention mechanism of fundamental models fails to consider the broadercontext or patterns that may span multiple consecutive time steps and only calculates attentionfor individual time steps, which makes it difficult to capture the characteristics of time series data.For this reason, considering the surrounding context along with a single time point provides moreinformation in time series data.

Patching refers to the technique of dividing input sequences into multiple patches, as illustratedin Fig. 10. This method preserves the information within each patch, thereby enhancing locality. Byprocessing patches instead of individual points, the model processes fewer tokens, thereby reducingthe computational complexity of the attention mechanism. This approach helps overcome the issueof prediction performance degradation, which can occur when sparse attention is used to makeself-attention more efficient, potentially missing critical information.

![](images/4dc7048a5a24f2132b93dddab522ec6e679f3bc7edc5c40b9503b7b64eb7f774.jpg)



Fig. 10: Patching Technique in Self-Attention for Time Series Forecasting


PatchTST (Nie et al, 2023), inspired by Vision Transformer’s (Dosovitskiy et al, 2021) divi-sion of images into $1 6 \times 1 6$ patches for capturing local semantic information, divides time seriesinto 64 patches. This approach allows for utilizing a longer look-back window by grouping datainto patches. By processing time-series data in patch units, PatchTST maximizes the advantagesof Transformer models for time-series applications, achieving better performance than LTSF-Linear.

PatchTST employs only the Transformer encoder, flattens the encoder output, and uses a linearlayer for the final predictions. Additionally, the study shows that channel independence (CI) yieldsbetter performance, highlighting the limitations in learning channel correlations despite the intuitiveconsideration of channel dependencies (CD) in multivariate scenarios. MTST (Zhang et al, 2024e)addresses the limitation of PatchTST in learning patterns across different scales present in time-series data. By adopting a multi-scale approach, MTST proposes an effective model utilizing bothshorter and longer patches for locality and long-term trend analysis. PETformer (Lin et al, 2024c)critiques the flattening of Transformer encoder output in PatchTST, which results in a significantincrease in parameters. PETformer introduces a placeholder-enhanced technique, modeling past andfuture data on the same time scale, thereby reducing the number of parameters by over 95% com-pared to PatchTST. This reduction enhances generalization performance while using less memoryand computational resources. The model also leverages rich context information by allowing directinteraction between past and future data, maintaining the continuity of time-series data.

By applying the patching technique, it was possible to refute the notion that traditional linearmodels are superior to Transformers. This led to the patching technique becoming a widely adoptedapproach.

# 4.1.2 Cross-Dimension

In multivariate time-series forecasting, understanding the relationships between variables is crucialfor improving prediction accuracy. Intuitively, higher temperatures lead to increased ice cream sales,indicating a relationship between variables. Despite this apparent correlation, surprising results haveshown that models treating channels independently, such as LTSF-Linear, PatchTST, and PET-former, often outperform those considering inter-channel correlations. This result implies that currentmodels fail to effectively capture the relationships between variables. Time series analysis differs sig-nificantly from natural language processing (NLP) and computer vision (CV) in terms of channelcorrelation. In NLP, there is no clear concept of channels. In CV, although channels exist, theirrelationships are tightly intertwined and well-defined, as seen in the RGB representation of images.

Conversely, in time series analysis, channel relationships can be either independent or interdepen-dent and often hidden, adding complexity to the task. Therefore, time series analysis requires modelscapable of capturing these intricate correlations. While earlier Transformer-based models primarilyfocused on temporal attention, recent models have increasingly focused on explicitly modeling thecorrelations between variables.

Fig. 11 illustrates the explicit modeling of relationships between variables using the attentionmechanism.

![](images/810e1c194ce07d7bc50ee80ae33ae143cacfffe700b8f33ce5d18fef910b21ce.jpg)



Fig. 11: Cross-Dimensional Self-Attention for Modeling Variable Relationships


The overall progression of advancements is as follows. It begins with modeling that explicitlyaccounts for correlations between variables. This progresses to directly modeling the relationshipsbetween both temporal and variable aspects. Additionally, the model incorporates the possibility oftime lags in the relationships between variables, allowing it to flexibly learn dependencies.

Crossformer (Zhang and Yan, 2023) breaks away from solely temporal attention by employ-ing a Two-Stage Attention mechanism that sequentially processes Cross-Time and Cross-Dimensionstages. It divides each dimension’s time series into patches, embeds them into 2D vectors, andperforms attention. Crossformer incorporates a router mechanism in the Cross-Dimension stage tohandle the increased complexity of dual attention. This hierarchical structure across multiple scalesenables Crossformer to effectively model both temporal dependencies and inter-dimensional correla-tions, significantly enhancing multivariate time series forecasting performance. DSformer (Yu et al,2023) criticizes Crossformer for emphasizing global interactions between variables in time series data,while overlooking the importance of local information, such as short-term variations and patterns.DSformer addresses this by using double sampling, obtaining global information via downsamplingand local information through piecewise sampling. These samples undergo parallel temporal variableattention, allowing the model to integrate global, local, and inter-variable correlations in multivari-ate time series data through dual sampling and time-variable attention mechanisms. CARD (Wanget al, 2024b) points out the structural complexity and high computational cost of Crossformer’s hier-archical attention mechanism. Simplifying the structure, CARD employs only the encoder and uses alightweight token blend module instead of explicitly generating token hierarchies. This module mergesadjacent tokens to represent larger ranges, efficiently leveraging multi-scale information and enhanc-ing prediction accuracy. A robust loss function is introduced to prevent overfitting, adjusting weightsto favor near-future data over far-future data, which shows less temporal correlation. iTransformer(Liu et al, 2024c) innovatively reverses the time and variable dimensions in the standard Transformermodel, learning inter-variable correlations. By embedding each variable into variate tokens via aTransformer encoder and performing temporal predictions through a feed-forward network, iTrans-former surprisingly outperforms Crossformer, highlighting the limitations of merely adding explicitchannel attention. Its advantage lies in not needing to alter the original Transformer structure.VCformer (Yang et al, 2024b) addresses the issue of neglecting time lags in variable correlations.Its variable correlation attention module calculates cross-correlation scores between queries and keysacross various lags using FFT (Fast Fourier Transform), efficiently exploring and adaptively inte-grating correlations for each lag. GridTST (Cheng et al, 2024c) transforms time series data into agrid format, applying horizontal and vertical attention mechanisms to model time and variable cor-relations effectively. Like PatchTST and iTransformer, GridTST leverages the original Transformer,distinguishing itself from the more complex Crossformer. It experiments with three configurations:time-first, channel-first, and cross-application, finding that prioritizing channel attention yields thebest performance, aligning with human intuitive analysis of inter-variable relationships. UniTST(Liu et al, 2024b) critiques the inability of cross-time and cross-dimension attention methods todirectly and explicitly learn complex inter-variable and intra-variable dependencies. It proposes aunified attention mechanism by flattening patch tokens. To mitigate the increased computationalcost, UniTST incorporates a dispatcher module to reduce memory complexity and enhance modelfeasibility. DeformTime (Shu and Lampos, 2024) introduces deformable attention, dynamicallyadjusting to recognize and adapt to important data features instead of fixed attention areas. Paralleldeformable attention mechanisms within the Transformer encoder capture correlations between vari-ables and time, adaptively extracting critical information across various time intervals and variablecombinations, significantly improving prediction accuracy and generalizability.

# 4.1.3 Exogenous Variable

Most existing research primarily utilizes endogenous variables for prediction. However, in real-worldscenarios, relying solely on endogenous variables can be insufficient due to the complexity of variousinfluencing factors. For instance, stock price predictions are significantly affected by external fac-tors such as economic indicators, political changes, and technological advancements. Ignoring theseexternal factors and relying only on past data of endogenous variables can lead to failures in accu-rately predicting market volatility. Therefore, incorporating exogenous variables as supplementaryinformation has emerged as a method to improve prediction performance.

TimeXer (Wang et al, 2024c) proposes a method to integrate exogenous variables into thecanonical Transformer model without structural changes. It operates by dividing the time series

data of endogenous variables into patches, learning the temporal dependencies of each patch throughself-attention. Then, it generates variate tokens summarizing the entire series of endogenous andexogenous variables and learns their interactions using a cross-attention mechanism. Through thisprocess, TimeXer simultaneously considers the temporal patterns of endogenous variables and theimpact of exogenous variables, enabling more precise and in-depth time-series predictions. However,TimeXer requires manual identification and input of appropriate exogenous variables. If unsuitabledata is provided, it can hinder the model’s predictive accuracy. TGTSF (Xu et al, 2024b) integratestext data from channel descriptions and news messages to enhance prediction accuracy. It embedschannel descriptions and news messages using a pre-trained text model, transforming them into asequence of vectors over time. The cross-attention layer then calculates the relevance of the textto each channel. By incorporating text data into the time series prediction model, TGTSF notonly improves prediction accuracy but also allows for direct comparison of the impact of textualinformation on predictive performance.

# 4.1.4 Additional Approaches

Beyond the topics discussed above, there are various other approaches to time series forecasting.

# Generalization

Research has been conducted to improve model generalization, avoid overfitting, and achieve con-sistent performance across diverse datasets. SAMformer (Ilbert et al, 2024) addresses the issue ofself-attention mechanisms converging to sharp local minima, causing entropy collapse during train-ing, and demonstrates that applying SAM (Sharpness-Aware Minimization) can significantly enhanceperformance. Minusformer (Liang et al, 2024b) highlights the redundancy and overfitting causedby a large number of parameters in Transformers. To combat this, it employs a boosting ensemblemethod, where each subsequent model predicts the residuals of the previous model’s outputs, thusreducing redundancy and improving generalization.

# Multi-scale

The multi-scale approach extracts more information from time series data across various scales,offering distinct advantages. Scaleformer (Shabani et al, 2023) proposes a general framework bystacking existing models across different scales, resulting in improved performance. Pathformer(Chen et al, 2024b), on the other hand, allows the model to learn adaptive scales independently,rather than relying on fixed scales.

# Decoder-only

Large-scale language models (LLMs) like LLaMA3 (Dubey et al, 2024) have been successfully imple-mented using only a decoder without the need for an encoder. The decoder-only architecture issimpler and involves less complex computations, resulting in faster training and inference. Addition-ally, the decoder-only structure helps avoid the temporal information loss often associated with theself-attention mechanism in encoders. This has led to a research proposal aimed at improving perfor-mance in time series forecasting using a decoder-only structure. CATS (Kim et al, 2024) addressesthe high time and memory complexity of self-attention in Transformer encoders and the loss of tem-poral order information. It proposes a structure using only cross-attention, focusing solely on therelationship between future and past data with a decoder-only architecture, which reduces parame-ter count and enhances efficiency. In contrast to the encoder-only structure of most models discussedCATS demonstrates the effectiveness of using only the decoder in achieving superior performance.

# Feature Enhancement

Fredformer (Piao et al, 2024) identifies the issue of frequency bias in time series data, where learningtends to focus disproportionately on either low or high frequencies. It addresses this by normalizingthe frequencies to eliminate bias. Basisformer (Ni et al, 2024) proposes a method to constructflexible relationships with each time series by leveraging biases learned through contrastive learning.

# 4.2 Growth of Fundamental Deep Learning Models

Since the advent of simple linear models, there has been a surge in research focused on non-transformer-based models. Attention has shifted to various architectures such as MLP, RNN, CNN,


omy and Methodologies of Transformer Models for Time Ser


<table><tr><td>Main Improvement</td><td>Model Name</td><td>Main Methodology</td><td>Channel Correlation</td><td>Enc/Dec Usage</td><td>Publication</td></tr><tr><td rowspan="3">Patching Technique</td><td>PatchTST</td><td>· Patching · Channel Independence</td><td>CI</td><td>Enc</td><td>2023</td></tr><tr><td>MTST</td><td>· Multiple Patch-based Tokenizations</td><td>CI</td><td>Enc</td><td>2024</td></tr><tr><td>PETformer</td><td>· Placeholder-enhanced Technique</td><td>CI</td><td>Enc</td><td>2022</td></tr><tr><td rowspan="8">Cross-Dimension</td><td>Crossformer</td><td>· Dual Attention: Cross-time &amp; Cross-dimension</td><td>CD</td><td>Enc &amp; Dec</td><td>2023</td></tr><tr><td>DSformer</td><td>· Dual Sampling &amp; Dual Attention</td><td>CD</td><td>Enc</td><td>2023</td></tr><tr><td>CARD</td><td>· Dual Attention · Token Blend Module for multi-scale</td><td>CD</td><td>Enc</td><td>2024</td></tr><tr><td>iTransformer</td><td>· Attention on Inverted Dimension</td><td>CD</td><td>Enc</td><td>2024</td></tr><tr><td>VCformer</td><td>· Variable Correlation Attention Considering Time Lag · Koopman Temporal Detector for Non-stationarity</td><td>CD</td><td>Enc</td><td>2024</td></tr><tr><td>GridTST</td><td>· Dual Attention with original Transformer</td><td>CD</td><td>Enc</td><td>2024</td></tr><tr><td>UniTST</td><td>· Unified Attention by Flattening</td><td>CD</td><td>Enc</td><td>2024</td></tr><tr><td>DeformTime</td><td>· Deformable Attention Blocks</td><td>CD</td><td>Enc</td><td>2024</td></tr><tr><td rowspan="2">Exogenous Variable</td><td>TimeXer</td><td>· Integration of Endogenous and Exogenous Information</td><td>CD</td><td>Enc</td><td>2024</td></tr><tr><td>TGTSF</td><td>· Exogenous Variable with Description, News</td><td>CD</td><td>Enc &amp; Dec</td><td>2024</td></tr><tr><td rowspan="2">Generalization</td><td>SAMformer</td><td>· SAM (sharpness-aware minimization)</td><td>CD</td><td>Enc</td><td>2024</td></tr><tr><td>Minusformer</td><td>· Dual-stream and Subtraction mechanism</td><td>CD</td><td>Enc</td><td>2024</td></tr><tr><td rowspan="2">Multi-scale</td><td>Scaleformer</td><td>· Multi-scale framework</td><td>Any</td><td>Enc &amp; Dec</td><td>2023</td></tr><tr><td>Pathformer</td><td>· Adaptive Multi-scale Blocks</td><td>CD</td><td>Enc &amp; Dec</td><td>2024</td></tr><tr><td>Decoder-only</td><td>CATS</td><td>· Cross-Attention-Only Transformer</td><td>CI</td><td>Dec</td><td>2024</td></tr><tr><td rowspan="2">Feature Enhancement</td><td>Fredformer</td><td>· Frequency Debias</td><td>CD</td><td>Enc</td><td>2024</td></tr><tr><td>BasisFormer</td><td>· Automatic Learning of a Self-adjusting Basis</td><td>CD</td><td>Dec</td><td>2023</td></tr></table>

and GNN, with many models surpassing Transformers and achieving remarkable performanceimprovements. Although transformer-based models exhibit excellent performance across numerousfields, they have structural limitations in learning temporal order information, which is crucial fortime series problems. While past tasks were simple and general enough to overlook these limitations,current real-world tasks involve many constraints and data-specific issues with diverse variables,necessitating approaches from various perspectives. Each architecture has its own strengths, andthese characteristics provide valuable solutions for addressing diverse contemporary time series fore-casting challenges. In this section, we will investigate the latest major models for each architectureand analyze their technical features. The key characteristics of each backbone architecture have beenbriefly summarized in comparison to Transformers in Table 6.

# 4.2.1 MLP-Based Models

MLP-based models have recently emerged as a key methodology for replacing Transformers in timeseries forecasting tasks. The simple structure of MLPs makes them easy to handle and interpret. Addi-tionally, they perform well even in constrained environments with limited computational resourcesand limited data. Their lightweight architectures enable fast training and inference, making themincreasingly important and widely used in contemporary industries. Previously, interest in MLPsdiminished due to their structural limitations, such as the lack of sequential dependency, challengeswith long-term dependency, difficulties in processing high-dimensional data, and limitations in cap-turing periodic patterns. However, recent advancements have enabled long-sequence learning andvarious technical enhancements, leading to remarkable performance improvements.

N-BEATS (Oreshkin et al, 2020), consisting of simple Fully Connected (FC) layers, demon-strated superior performance compared to traditional statistical models. This model utilizes ablock-based architecture with repeatedly stacked backcast and forecast paths and enhances general-ization performance through ensemble learning. By applying decomposition, it separates trend andseasonality, modeling them using Polynomial Basis and Fourier Basis, respectively, making it highlyinterpretable. Moreover, recent studies have improved the N-BEATS model to predict various quan-tiles, enabling probabilistic forecasting (Smyl et al, 2024). The improved N-HiTS (Challu et al,2023) model can handle multivariate data and overcomes the limitations of N-BEATS by employingtechniques such as multi-rate signal sampling, non-linear regression, and hierarchical interpolation.Koopa (Liu et al, 2024d) is a model designed to effectively handle non-stationary time series databy utilizing Koopman theory to predict the dynamic changes of components. It uses a Fourier filterto separate time-invariant and time-variant elements, which are then fed into respective Koopmanpredictors(KP). In KP, nonlinear time series data are linearly transformed, making them easier tomanage. To effectively capture the characteristics of each component, the time-invariant KP uses aglobally learned Koopman operator, while the time-variant KP computes a local operator for pre-dictions. TSMixer (Ekambaram et al, 2023) is a lightweight patch-based model that introduces theMLP-mixer from the computer vision field to the time-series domain. It efficiently learns long-termand short-term dependencies through inter-patch and intra-patch mixers respectively, and explicitlylearns channel relationships through an inter-channel mixer. To enhance the understanding of inter-channel correlations, it adjusts the original forecast results using reconciliation heads and employsgated attention to filter important features. By introducing these techniques, TSMixer upgrades thesimple MLP structure, resulting in improved performance that outperforms complex Transformermodels. FreTS (Yi et al, 2024) is a model that leverages two key characteristics of the frequencydomain—global view and energy compaction—to directly train an MLP model in the frequencydomain. While existing models primarily use frequency transformation to verify the periodicity ofthe model, this model distinguishes between channel and temporal dependencies in the frequencydomain as real and imaginary parts, respectively, and learns them directly to better extract hid-den features. TSP (Wang et al, 2024e) utilizes a PrecMLP block with a precurrent mechanism toeffectively model temporal locality and channel dependency. The precurrent mechanism combinesprevious information with current information to create hybrid features at each time point, serving asa computationally-free method to effectively recognize and process temporal locality. This lightweightMLP model comprises an encoder, consisting of tMLP operating in the time dimension and vMLPoperating in the variable dimension, and a decoder using a linear model. FITS (Xu et al, 2024c) isa very lightweight model with fewer than 10,000 parameters, yet it demonstrates competitive perfor-mance comparable to larger models. It performs interpolation through a complex linear layer thatlearns amplitude scaling and phase shifts in the frequency domain, thereby expanding the frequency


arison of Other Deep Learning Models with Transformers in T


<table><tr><td>Criteria</td><td>Transformer-based models</td><td>MLP-based models</td><td>CNN-based models</td><td>RNN-based models</td><td>GNN/GCN-based models</td></tr><tr><td>Structure</td><td>Complex self-attention mechanism</td><td>Simple layer, relatively easy to implement and interpret</td><td>Utilizes convolutional layers, effectively capturing specific local patterns</td><td>Specialized in sequential data processing, effectively handling temporal dependencies but may struggle with long sequences</td><td>Learns relationships between nodes using graph structures, effectively capturing complex relationships</td></tr><tr><td>Data Requirements</td><td>Requires large datasets</td><td>Can train on smaller datasets</td><td>Can train on smaller datasets</td><td>Can train on smaller datasets, suitable for quick training with limited data</td><td>Can achieve high performance with relatively small datasets</td></tr><tr><td>Training Time</td><td>Relatively slow due to global attention mechanisms</td><td>Relatively fast</td><td>Generally faster due to localized computations</td><td>Trains effectively on smaller datasets but can be slow for long sequences</td><td>Varies depending on graph complexity</td></tr><tr><td>Model Size</td><td>Comparatively larger and more parameter-intensive</td><td>Comparatively small, efficient use of resources</td><td>Typically smaller and more parameter-efficient, making it resource-efficient and scalable</td><td>Comparatively small</td><td>Depends on graph size and complexity, achieving high performance with fewer parameters in specific problems</td></tr><tr><td>Interpretability</td><td>Difficult to interpret</td><td>Relatively high interpretability</td><td>More interpretable through visualizations of filters and feature maps</td><td>Moderately interpretable, easier to understand and explain model behavior</td><td>Moderately interpretable depending on graph structure and model complexity</td></tr><tr><td>Performance</td><td>Suitable for learning long-term dependencies</td><td>Suitable for short-term predictions with sufficient performance in many cases</td><td>Excels at capturing local temporal dependencies, superior for problems with strong local patterns</td><td>Suitable for short-term and sequential dependencies, providing sufficient performance in specific time series problems</td><td>Excels at learning complex dependencies between nodes, offering high performance in learning specific relationship patterns</td></tr><tr><td>Flexibility</td><td>Requires complex adjustments</td><td>Easily adjustable for specific problems</td><td>Easily customizable with various types of convolutions</td><td>Extensible with various RNN architectures</td><td>Can handle various graph structures and formats, adaptable to different data types and structures</td></tr><tr><td>Application</td><td>Suitable for complex time series problems or NLP-related tasks</td><td>Versatile for various general forecasting problems</td><td>Well-suited for applications requiring spatial and temporal locality, effective for a wide range of time series problems</td><td>Effective for sequential data and time series forecasting, but struggles with long-term dependencies without modifications</td><td>Suitable for complex graph structures in tasks like social networks, recommendation systems, and time series graphs</td></tr><tr><td>Hardware Requirements</td><td>High due to their complex structure and computationally intensive self-attention mechanisms</td><td>Lower due to their simpler structure and fewer computational demands</td><td>Lower computational and memory requirements</td><td>Low but inefficient on parallel hardware</td><td>Generally low but depends on graph size</td></tr><tr><td>Memory Usage</td><td>Higher memory usage due to full sequence attention</td><td>Lower memory usage due to their simple structure and fewer parameters</td><td>Lower memory usage due to localized operations</td><td>Low but can increase with sequence length</td><td>Generally low but depends on graph size</td></tr><tr><td>Parallel Processing</td><td>Highly parallelizable but requires synchronization due to attention mechanisms</td><td>Highly parallelizable due to independent computations</td><td>Highly parallelizable due to independent convolution operations</td><td>Difficult due to sequence dependencies</td><td>Difficult due to graph structure dependencies</td></tr></table>

representation of the input time series. By using a low-pass filter to remove high-frequency compo-nents, which are often noise, the model efficiently represents the data and reduces the number oflearnable parameters. U-Mixer (Ma et al, 2024) utilizes a hierarchical structure of U-Net’s encoder-decoder composed of MLPs to extract and combine low-dimensional and high-dimensional features.Each MLP block processes temporal dependencies and channel interactions separately, enabling sta-ble feature extraction. Additionally, the model employs a stationary correction method to limit thedifferences in data distribution before and after processing, thereby restoring the non-stationaryinformation of the data. By calculating the transformation matrix and mean difference between theinput and output data, the model adjusts the output to maintain temporal dependency, therebyenhancing its prediction performance. TTMs (Ekambaram et al, 2024) is a time series foundationmodel that uses TSMixer as its backbone and relies solely on time series data for rapid pre-training.This model is trained on the extensive collection of time series datasets with diverse channels andresolutions from the Monash archive (Table 3). Despite the data-specific characteristics of the timeseries datasets, TTMs demonstrate efficient transfer learning and high generalization performance.TimeMixer (Wang et al, 2024a) leverages the observation that time series data exhibit distinct pat-terns at different sampling scales. To extract important information from past data, multi-scale timeseries are generated through downsampling. In the Past-Decomposable-Mixing (PDM) block, theseasonal and trend components are decomposed and mixed separately. The Future-Multipredictor-Mixing (FMM) block then integrates predictions by ensembling multiple predictors, which utilizethe past information extracted at various scales. CATS (Lu et al, 2024) enhances the performanceof multivariate time series forecasting by generating Auxiliary Time Series (ATS) from the OriginalTime Series (OTS) and integrating them into the prediction process. The model proposes varioustypes of ATS constructors, each with different roles and objectives, combining them to maximize pre-dictive performance. To effectively generate and utilize ATS, the model structure and loss functionincorporate three key principles: continuity, sparsity, and variability. Despite using a simple two-layerMLP predictor as the foundation, CATS effectively predicts multivariate information through theuse of ATS. HDMixer (Huang et al, 2024a) is a model that applies a Length-Extendable Patcherto overcome the limitations of fixed-length patching. It measures the point-wise similarity with fixedpatches and compares the patch entropy loss, updating parameters to increase the complexity ofthe extendable patches. Through the Hierarchical Dependency Explorer mixer, it learns long-term,short-term dependencies and cross-variable dependencies. SOFTS (Han et al, 2024) proposes theSTar Aggregate-Redistribute (STAR) Module to effectively learn the relationships between channels.It converts the data from each channel into high-dimensional vectors and then aggregates these vec-tors to extract comprehensive information, referred to as the core. The generated core representationis then combined with the time series representation of each channel and converted back to the timeseries dimension, reducing complexity and enhancing robustness. SparseTSF (Lin et al, 2024b) isan extremely lightweight model that uses fewer than 1,000 parameters while demonstrating excellentgeneralization performance. It downsamples the data into multiple periods, performs predictions oneach generated sub-sequence, and then upsamples to create the overall prediction sequence. While itshows limited performance only on data with clear periodic patterns, it efficiently handles complextemporal dependencies with minimal computational resources. TEFN (Zhan et al, 2024) proposesa novel backbone model from the perspective of information fusion to extract latent distributionsfrom simple data structures. Based on evidence theory (Shafer, 1976), the Basic Probability Assign-ment Module maps each time and channel dimension of the time series data to a mass function inthe event space. This mass function assigns probabilities to multiple possibilities for each data point,allowing for explicit modeling of uncertainty. The generated mass functions are fused using theirexpected values to derive the final predictions. This model achieves high predictive performance withfew parameters and fast training times. PDMLP (Tang and Zhang, 2024) surpasses the performanceof complex transformer-based models using only simple linear layers and patching. It divides thedata into patches of various scales and embeds each patch with a single linear layer. The embeddingvectors are then decomposed into smooth components and residual components containing noise,allowing the model to process the sequences in two ways: intra-variable and inter-variable. AMD(Hu et al, 2024b) introduces the AMS block, which assesses the importance of various temporal pat-terns and generates appropriate weights for each pattern. By simultaneously modeling the temporaland channel dependencies of the input information decomposed into multiple scales, it effectivelycaptures interactions at different scales. Instead of merely integrating the results from each scale, itimproves prediction performance by reflecting the importance of each pattern through the weights.

# 4.2.2 CNN-Based Models

CNNs were primarily developed for image recognition and processing tasks because they are highlyeffective at identifying 2D spatial patterns. Some studies have begun utilizing Temporal Convo-lutional Networks (TCNs), which are 1D CNNs that move along the time axis to recognize localpatterns, effectively extracting key features of time series data. Additionally, CNNs can use filtersof various sizes to capture patterns at multiple scales, allowing them to effectively learn both short-term and long-term patterns in time series data. CNN’s deep network technology and parallelism,which have been developed over a long period of time, have provided high performance and relia-bility for time series forecasting. However, the fixed filter size of CNNs lacks flexibility in learningcomplex patterns of long sequences and poses challenges in capturing long-term dependencies. Fur-thermore, as the network depth increases, there are limitations such as information loss, increasedcomputational cost, and higher memory usage. For these reasons, the relatively flexible Transformer,which is advantageous for learning long-term dependencies, has gained more attention in time seriesdata processing. However, due to their superior ability to capture diverse local patterns within longsequences, CNNs are once again receiving attention as a backbone architecture.

Recently, 3D convolution methodologies have been actively studied for handling complex spatio-temporal time series data (Chen et al, 2021c; Feng et al, 2024a). 3D convolution processes input datain three dimensions (2D spatial axes $^ +$ temporal axis) using convolution operations. This approachenables the extraction of richer patterns by processing not only the temporal axis but also spatial ormulti-dimensional aspects of the data. Such methods have demonstrated high performance in region-based prediction tasks, and future research is expected to focus on developing more sophisticateddata representations and learning structures.

TimesNet (Wu et al, 2023) introduces the TimesBlock, which embeds 1D time series into 2D ten-sors for analysis to effectively capture multi-periodicity. Extracting key frequencies via Fast FourierTransformation (FFT) expands the data into a 2D tensor, representing intra-period variation incolumns and inter-period variation in rows. It uses a parameter-efficient Inception module with 2Dkernels to capture temporal variations across various scales. After converting back to a 1D tensor, itadaptively aggregates the data based on the importance of frequency amplitude values. PatchMixer(Gong et al, 2023) introduces a patch-mixing design to efficiently replace the permutation invariantproperty of the Transformer’s attention mechanism. It learns intra-patch correlations using single-scale depth-wise separable convolutions and inter-patch correlations using point-wise convolutions onthe patched input sequence. ModernTCN (Luo and Wang, 2024) enhances the fundamental TCNsto make them more suitable for time series analysis. Using larger kernels than conventional TCNsexpands the effective receptive field, reducing the number of layers and effectively capturing long-term dependencies. It separates depth-wise convolution and point-wise convolution to independentlylearn temporal dependencies and feature dependencies, thereby enhancing computational efficiency.Additionally, it explicitly handles cross-variable dependency in multivariate time series data, result-ing in improved performance. ConvTimeNet (Cheng et al, 2024b) follows the framework of aTransformer encoder, replacing the role of self-attention with depthwise convolution and the role ofmulti-head attention with convolutions of various kernel sizes to increase computational efficiency. Ituses Deformable Patch Embedding, which adaptively adjusts each patch’s size and position based onthe data. Depthwise convolutions capture temporal features for each channel, while point-wise con-volutions model channel dependencies. ACNet (Zhang and Wang, 2024) aims to effectively modeltemporal dependencies and nonlinear features. It starts by removing unnecessary noise from the datausing Wavelet Denoising. Then, it extracts temporal features through Multi-Resolution Dilated Con-volution and Global Adaptive Average Pooling. To more accurately capture nonlinear features, GatedDeformable Convolution is employed, which adaptively adjusts the sampling positions. Additionally,ACNet employs dynamic prediction, retraining the model with new data if prediction performancedeclines. FTMixer (Li et al, 2024d) leverages the observation that the time domain effectively cap-tures local dependencies while the frequency domain excels at learning global dependencies. It appliesconvolution in the frequency domain to capture global dependencies. Meanwhile, it converts multi-scale patches using Discrete Cosine Transformation (DCT), which is computationally simpler thanDiscrete Fourier Transformation (DFT), and then applies convolution to capture local dependencies.

# 4.2.3 RNN-Based Models

RNNs were fundamentally developed to process sequential data. Their recurrent structure can effec-tively model the temporal dependencies of time series data. Gated RNN models, such as LSTM and

GRU, have partially alleviated the long-term dependency issues of vanilla RNNs. However, RNNs aredifficult to parallelize because they process data sequentially. This slow speed is particularly detri-mental when handling long sequences. RNNs also have inherent limitations in learning long-termdependencies and global information. Transformers have effectively addressed these issues throughself-attention mechanisms. However, unlike Transformers, which rely on large datasets, RNN-basedmodels can learn effectively with smaller amounts of data. They also use memory efficiently and arewell-suited for real-time data processing due to their sequential nature, making them advantageousfor online streaming applications. Additionally, because time ordering is naturally learned, RNNsstill have inherent advantages over Transformers for time series processing.

PA-RNN (Zhang et al, 2024c) aims to address prediction Uncertainty Quantification and ModelCompression issues for dependent time series data by proposing an optimization methodology forSparse Deep Learning. It applies a Mixture of Gaussian Prior to introducing sparsity to the parame-ters and uses a Prior Annealing technique to gradually increase the strength of the prior distributionduring the training process. Properties such as Posterior Consistency, Structure Selection Consis-tency, and Asymptotic Normality ensure the model’s theoretical validity. WITRAN (Jia et al, 2024)proposes a new paradigm by implementing information transmission in bi-granular flows to capturelong-term and short-term repetitive patterns. It models global and local correlations through theHorizontal Vertical Gated Selective Unit (HVGSU), which uses bidirectional Gated Selective Cells(GSC). To enhance the efficiency of information transmission through parallel processing, it appliesthe Recurrent Acceleration Network (RAN). This approach results in excellent prediction perfor-mance while reducing time complexity and maintaining memory complexity. SutraNets (Bergsmaet al, 2023) transforms long sequences into multiple lower-frequency sub-series in order to effectivelypredict long sequence time series data. It performs autoregressive predictions among the sub-series,with each sub-series being conditionally predicted based on the values of other sub-series. Thisapproach reduces the signal path length and allows for the generation of long sequences in fewersteps. Consequently, it mitigates the common RNN problem of error accumulation and models long-distance dependencies better. CrossWaveNet (Huang et al, 2024b) uses a dual-channel network toseparate and process the seasonality and trend-cyclic components of data. The input data are ini-tially decomposed into individual elements through series decomposition, and features are extractedusing enriched LSTM and GRU structures. These extracted features undergo a cross-decompositionprocess to further refine the elements, which are then sent to each channel for integration. Seriesdecomposition is performed at each RNN step, progressively filtering and reintegrating the seasonal-ity and trend components to learn precise information. DAN (Li et al, 2024c) proposes a new modelthat utilizes Polar Representation learning to predict long-term time series data with high volatility.The Distance-weighted Auto-regularized Neural network (DAN) distinguishes and learns the polarrepresentations of distant and nearby data, combining them to enhance prediction performance. Itaddresses the imbalance problem of extreme data, which occur infrequently but are critical for accu-rate predictions, using Kruskal-Wallis Sampling and employs a Multi-Loss Regularization Method toeffectively learn both extreme and normal values simultaneously. RWKV-TS (Hou and Yu, 2024)proposes an efficient Recurrent Weighted Key-Value (RWKV) backbone characterized by linear timecomplexity and memory usage. It uses a Multi-head Weighted Key-Value (WKV) Operator, which issimilar to the self-attention mechanism but maintains linear time and space complexity. Each headcaptures different temporal patterns, increasing the representational power of the information andenabling parallel computation, thus providing high computational efficiency. This allows RWKV-TS to capture longer sequence information more effectively than fundamental RNNs. CONTIME(Jhin et al, 2024) introduces Continuous GRU to minimize the prediction delay issue in time seriesforecasting. This approach applies Neural ODE to the existing GRU, modeling data changes con-tinuously over time. The bi-directional structure integrates forward and backward through the timeseries data, capturing more accurate temporal dependencies. By employing Time-Derivative Regu-larization, the model is guided to learn the rate of change in predictions over time, enhancing boththe accuracy and speed of predictions.

# 4.2.4 GNN-Based Models

GNNs or GCNs are specialized in processing graph-structured data, being suitable for modeling com-plex interactions and learning local patterns in time series data. Multivariate time series data ofteninvolve interactions between variables, which can be effectively represented as relationships betweennodes and edges. By assigning various attributes to nodes and edges, richer representations can be

learned, and the structural characteristics of specific domains can be better reflected. Traditionally,various approaches such as GCN, ST-GCN, GAT, and TGN have been proposed, contributing to theadvancement of the time series forecasting field.

However, these models primarily focus on learning local information from adjacent nodes, leadingto a lack of capability in learning global information and difficulty in capturing long-term depen-dencies with distant nodes. The sequential calculation of nodes and edges makes parallel processingchallenging, and the models’ specificity to particular graph structures reduces their generalizationability. These drawbacks have diminished interest in GNN-based models. However, with the recentadvancements in social networks, much of current data naturally follows a graph structure. Vari-ous optimization techniques and hardware acceleration technologies have been developed, making itpossible to process large-scale graph data. Additionally, the increased practical relevance due to theflexibility in dynamically handling time-varying data has once again drawn researchers’ attention tothis architecture.

MSGNet (Cai et al, 2024a) effectively learns the complex correlations of MTS data by combin-ing frequency domain analysis with adaptive graph convolution. It uses Fast Fourier Transformationto identify key periods and transform time series data into various time scales. Then, it employs Mix-hop graph convolution to learn inter-channel correlations and uses Multi-head Attention and ScaleAggregation to capture intra-series correlations. TMP-Nets (Coskunuzer et al, 2024) is a model pro-posed to learn features extracted from Temporal MultiPersistence (TMP) vectorization and capturekey topological patterns. It combines two advanced topological data analysis techniques, Multiper-sistence and Zigzag Persistence, to effectively capture topological shape changes in the data. SpatialGraph Convolution is used to model dependencies between nodes in the graph, and the topologicalfeatures extracted with TMP vectors are learned using CNN. HD-TTS (Marisca et al, 2024) is amodel that demonstrates high performance even with missing data through masking and hierarchicaldownsampling. It incrementally reduces data in the temporal and spatial processing modules, learn-ing various temporal and spatial dependencies. The model uses an attention mechanism to learn thepatterns of missing values and combines them with weights to generate the final prediction. Fore-castGrapher (Cai et al, 2024b) is based on the finding that the attention mechanism is more suitablefor modeling inter-channel correlations than temporal correlations. Utilizing the similarity betweenGNN’s neighborhood aggregation and attention mechanisms, it linearly embeds each channel as anode and learns the relationships between channels using GNN. Each GNN layer employs a self-learnable adjacency matrix to independently learn the interactions between nodes. It uses learnablescalers to divide node features into multiple groups and applies 1D convolution to each group.

# 4.2.5 Model-Agnostic Frameworks

Some studies propose model-agnostic methodologies that can be universally applied without beinglimited to a specific model backbone. These studies focus on the intrinsic characteristics of timeseries data or specific problems, improving model robustness by addressing these issues. Model-agnostic architectures, not being tied to a particular model, allow for flexible selection of the optimalmodel through various comparisons. They are also highly reusable and make it easier to interpretand understand the model’s predictions, making them applicable to a wide range of domains andscenarios.

These models primarily address the distribution shift problem, which is one of the core issues intime series forecasting, and propose various plug-and-play alleviation methods. The details relatedto this will be discussed in Section 5, while here, we will focus on other methodologies, excludingdistribution shift alleviation methods.

RobustTSF (Cheng et al, 2024a) introduces a selective sampling algorithm to train a resilientprediction model in Time Series Forecasting with Anomalies (TSFA). It calculates anomaly scoresbased on the difference between the trend and the actual data, selecting samples with scores belowa certain threshold to train the model, thereby minimizing the impact of anomalies. By analyzingthree types of anomalies (Constant, Missing, Gaussian), it identifies the most robust loss functionas MAE. This approach addresses the discontinuity issues of the traditional detection-imputation-retraining pipeline, enabling the model to deliver more stable and consistent performance. PDLS(Hounie et al, 2024) proposes a constrained learning approach by setting an upper bound on the lossat each time step. It controls the loss by setting a constant upper bound (ϵ) for all time steps andintroduces a variable (ζ) that relaxes the constraints, allowing automatic adjustment during train-ing. This model overcomes the problem of existing methodologies that focus on averaging prediction

performance, which often leads to uneven error distributions. Leddam (Yu et al, 2024) proposes twoindependently usable modules, Learnable Decomposition and Dual Attention, to effectively capturethe complex patterns of inter-series dependencies and intra-series variations in MTS. The LearnableDecomposition module replaces the traditional moving average kernel with a learnable 1D convolu-tion kernel initialized with a Gaussian distribution for decomposition, making it more sensitive tocritical information. The Dual Attention module simultaneously models inter-data dependencies andintra-series variability using channel-wise self-attention and auto-regressive self-attention. InfoTime(Qi et al, 2024) proposes a method to effectively utilize inter-channel and temporal correlations. Forchannel mixing, it employs Cross-variable Decorrelation Aware feature Modeling (CDAM) to mini-mize redundant information between channels and enhance useful mutual information. Additionally,it uses TAM(Temporal correlation Aware Modeling) to maximize the temporal correlations betweenthe predicted future sequence and the target sequence. CCM (Chen et al, 2024a) combines thestrengths of the traditional channel-independent and channel-dependent strategies to overcome theirlimitations. It dynamically clusters channels based on their inherent similarities and learns represen-tative patterns for each cluster to generate prototype embeddings. Separate FFNs are used for eachcluster to perform predictions for the channels within the cluster, and the prototype embeddingsenable zero-shot predictions. HCAN (Sun et al, 2024b) addresses the issue of excessive flatten-ing caused by using the MSE loss function and proposes reconfiguring the prediction problem as aclassification problem to better learn high-entropy features. It extracts important features from thebackbone model, divides them into multiple layers and categories, and uses an Uncertainty-AwareClassifier at each layer to reduce uncertainty. It learns relative prediction values within each classi-fied category and integrates the prediction results from multiple layers through the Hierarchy-AwareAttention module. This approach effectively captures the complex patterns and variability in timeseries data. TDT Loss (Xiong et al, 2024) points out the error accumulation issue in the traditionalauto-regressive approach which recursively models Temporal Dependencies among Targets (TDT).To effectively capture changes between consecutive time points, it represents TDT using first-orderdifferencing. The final TDT loss combines the target prediction loss, TDT values prediction loss, andan adaptive weight, allowing the model to dynamically balance between target prediction and TDTlearning. Thus, TDT loss replaces the optimization objective of non-autoregressive models.

The different deep learning models and their methodologies discussed so far are summarized inTable 7.

# 4.3 Emergence of Foundation Models

In recent years, foundation models have demonstrated remarkable performance and strong zero-shot capabilities across various tasks and domains, leading to increased focus (Touvron et al, 2023;Liu et al, 2024a; Li et al, 2023; Achiam et al, 2023; Gallifant et al, 2024; Radford et al, 2021).While significant progress has been made in domains like vision and language, developing foundationmodels for time series data has faced several challenges. Firstly, time series data exhibit distinctcharacteristics depending on the dataset. For instance, electrocardiogram (ECG), weather, and sensordata from industrial processes have unique properties in terms of variables, frequency, periodicity, andnoise, often requiring domain-specific knowledge for effective modeling. Additionally, unlike the visionand language domains, where large-scale pre-training corpora can be relatively easily constructedfrom publicly available sources like the web, collecting time series data is more difficult due to highacquisition costs and security concerns. Despite these obstacles, research on time series foundationmodels is essential for improving model scalability and generality. This necessity has led to activeexploration in several key directions, particularly in the field of time series forecasting.

# 4.3.1 Sequence Modeling with LLMs

One prominent approach leverages the sequence modeling capabilities of LLMs. LLMs have rev-olutionized deep learning with their groundbreaking generalizable sequence modeling. Given thesequential nature of both text and time series data, extending the sequential capabilities of languagemodels to time series is a natural progression. Early research includes GPT4TS (Zhou et al, 2023),which demonstrates that by freezing most of the language model’s backbone and fine-tuning only thelayer normalization parameters and positional embeddings on time series data, GPT4TS can serve asa general time series task solver for forecasting, anomaly detection, and classification. Additionally,PromptCast (Xue and Salim, 2023) introduces a framework where numerical time series sequencesare input as text prompts to a large language model. Then the model outputs the forecasting results


Table 7: Taxonomy and Methodologies of Fundamental Deep Learning Architectures forTime Series Forecasting


<table><tr><td>Model Name</td><td>Base</td><td>Main Methodology</td><td>Channel Correlation</td><td>Publication</td></tr><tr><td>N-BEATS</td><td rowspan="17">MLP</td><td>· Neural Basis Expansion (Polynomial Basis &amp; Fourier Basis)</td><td>CI</td><td>2020</td></tr><tr><td>N-HiTS</td><td>· Neural Hierarchical Interpolation
· Multi-Rate Data Sampling</td><td>CI</td><td>2023</td></tr><tr><td>Koopa</td><td>· Koopa Block with Koopman Predictor(KP)</td><td>CD</td><td>2023</td></tr><tr><td>TSMixer</td><td>· MLP Mixer layer architecture
· Gated attention (GA) block
· Online hierarchical patch reconciliation head</td><td>CI/CD</td><td>2023</td></tr><tr><td>FreTS</td><td>· Frequency-domain MLPs</td><td>CD</td><td>2023</td></tr><tr><td>TSP</td><td>· PrecMLP block with precurrent mechanism</td><td>CD</td><td>2024</td></tr><tr><td>FITS</td><td>· Complex Frequency Linear Interpolation
· Low Pass Filter(LPF)</td><td>CI</td><td>2024</td></tr><tr><td>U-Mixer</td><td>· Unet Encoder-decoder with MLPs
· Stationarity Correction</td><td>CD</td><td>2024</td></tr><tr><td>TTMs</td><td>· Multi-Resolution Pre-training via TTM Backbone
(TSMixer blocks)
· Exogenous mixer</td><td>CD</td><td>2024</td></tr><tr><td>TimeMixer</td><td>· Past-Decomposable-Mixing (PDM) block
· Future-Multipredictor-Mixing (FMM) block</td><td>CD</td><td>2024</td></tr><tr><td>CATS</td><td>· Auxiliary Time Series(ATS) Constructor</td><td>CD</td><td>2024</td></tr><tr><td>HDMixer</td><td>· Length-Extendable Patcher
· Patch Entropy Loss
· Hierarchical Dependency Explorer</td><td>CD</td><td>2024</td></tr><tr><td>SOFTS</td><td>· STar Aggregate-Redistribute Module</td><td>CD</td><td>2024</td></tr><tr><td>SparseTSF</td><td>· Cross-Period Sparse Forecasting</td><td>CI</td><td>2024</td></tr><tr><td>TEFN</td><td>· Basic Probability Assignment(BPA) Module</td><td>CD</td><td>2024</td></tr><tr><td>PDMLP</td><td>· Multi-Scale Patch Embedding &amp; Feature Decomposition
· Intra-, Inter-Variable MLP</td><td>CD</td><td>2024</td></tr><tr><td>AMD</td><td>· Multi-Scale Decomposable Mixing(MDM) Block
· Dual Dependency Interaction(DDI) Block
· Adaptive Multi-predictor Synthesis(AMS) Block</td><td>CD</td><td>2024</td></tr><tr><td>TimesNet</td><td rowspan="6">CNN</td><td>· Transform 1D-variations into 2D-variations
· Timesblock</td><td>CD</td><td>2023</td></tr><tr><td>PatchMixer</td><td>· Patch Embedding
· PatchMixer layer with Patch-mixing Design</td><td>CI</td><td>2023</td></tr><tr><td>ModernTCN</td><td>· ModernTCN block with DWConv &amp; ConvFFN</td><td>CD</td><td>2024</td></tr><tr><td>ConvTimeNet</td><td>· Deformable Patch Embedding
· Fully Convolutional Blocks</td><td>CD</td><td>2024</td></tr><tr><td>ACNet</td><td>· Temporal Feature Extraction Module
· Nonlinear Feature Adaptive Extraction Module</td><td>CD</td><td>2024</td></tr><tr><td>FTMixer</td><td>· Frequency Channel Convolution
· Windowing Frequency Convolution</td><td>CD</td><td>2024</td></tr><tr><td>PA-RNN</td><td rowspan="7">RNN</td><td>· Mixture Gaussian Prior
· Prior Annealing Algorithm</td><td>CI</td><td>2023</td></tr><tr><td>WITRAN</td><td>· Horizontal Vertical Gated Selective Unit
· Recurrent Acceleration Network</td><td>CI</td><td>2023</td></tr><tr><td>SutraNets</td><td>· Sub-series autoregressive networks</td><td>CI</td><td>2023</td></tr><tr><td>CrossWaveNet</td><td>· Deep cross-decomposition
· Dual-channel network</td><td>CD</td><td>2024</td></tr><tr><td>DAN</td><td>· RepGen &amp; RepMerg with Polar Representation Learning
· Distance-weighted Multi-loss Mechanism
· Kruskal-Wallis Sampling</td><td>CI</td><td>2024</td></tr><tr><td>RWKV-TS</td><td>· RWKV Blocks with Multi-head WKV Operator</td><td>CD</td><td>2024</td></tr><tr><td>CONTIME</td><td>· Bi-directional Continuous GRU with Neural ODE</td><td>CI</td><td>2024</td></tr><tr><td>MSGNet</td><td rowspan="4">GNN/GCN</td><td>· ScaleGraph block with Scale Identification
· Multi-scale Adaptive Graph Convolution.
· Multi-head Attention and Scale Aggregation</td><td>CD</td><td>2024</td></tr><tr><td>TMP-Nets</td><td>· Temporal MultiPersistence(TMP) Vectorization</td><td>CD</td><td>2024</td></tr><tr><td>HD-TTS</td><td>· Temporal processing module with Temporal message passing
· Spatial processing module with Spatial message passing</td><td>CD</td><td>2024</td></tr><tr><td>ForecastGrapher</td><td>· Group Feature Convolution GNN (GFC-GNN)</td><td>CD</td><td>2024</td></tr><tr><td>RobustTSF</td><td rowspan="7">Model-agnostic</td><td>· RobustTSF Algorithm</td><td>-</td><td>2024</td></tr><tr><td>PDLS</td><td>· Loss Shaping Constraints
· Empirical Dual Resilient and Constrained Learning</td><td>-</td><td>2024</td></tr><tr><td>Leddam</td><td>· Learnable Decomposition Module
· Dual Attention Module</td><td>CD</td><td>2024</td></tr><tr><td>InfoTime</td><td>· Cross-Variable Decorrelation Aware Feature Modeling
(DRAM)
· Temporal Aware Modeling (TAM)</td><td>CD</td><td>2024</td></tr><tr><td>CCM</td><td>· Channel Clustering &amp; Cluster Loss
· Cluster-aware Feed Forward</td><td>CD</td><td>2024</td></tr><tr><td>HCAN</td><td>· Uncertainty-Aware Classifier(UAC)
· Hierarchical Consistency Loss(HCL)
· Hierarchy-Aware Attention(HAA)</td><td>-</td><td>2024</td></tr><tr><td>TDT Loss</td><td>· Temporal Dependencies among Targets(TDT) Loss</td><td>-</td><td>2024</td></tr></table>


- : Indicates that the feature is model-agnostic and depends on which backbone model is applied.


in a question-answering format. This approach integrates domain-specific knowledge through textand provides forecasting results in a human-friendly format.

Both GPT4TS and PromptCast rely on fine-tuning to achieve their results. In contrast, with-out fine-tuning, LLMTime (Gruver et al, 2023) has demonstrated impressive zero-shot forecastingcapabilities. The main idea of LLMTime is encoding time series data as a string of numerical digits,which emphasizes the importance of preprocessing and its inherent dependencies. To align the timeseries modality with the language modality, Time-LLM (Jin et al, 2024) introduces the concept ofpatch reprogramming. This concept aims to mitigate the challenges of a large reprogramming spaceand attempts to connect time series local characteristics with language semantics, such as “shortup”. While patch reprogramming offers more flexibility by reducing the reliance on large-scale timeseries corpora, it also presents the challenge of adapting time series data to fit the characteristics oflarge language models.

# 4.3.2 Pre-training

Another approach that focuses on building large-scale time series corpora to pre-train time seriesfoundation models from scratch has emerged. An example is Lag-LLaMA (Rasul et al, 2024), whichuses a decoder-only Transformer to generate forecasting results, enabling probabilistic forecasting.Lag-LLaMA also consolidates 27 publicly available forecasting datasets from various domains tocreate a comprehensive pre-training corpus. On the other hand, TimesFM (Das et al, 2024) extendsbeyond publicly available forecasting datasets by incorporating additional pre-training corpora basedon Google Trends and Wiki Pageview statistics. It also utilizes synthetic datasets generated frompiece-wise linear trends, autoregressive processes, and sine and cosine functions to capture universalcharacteristics. The entire pre-training corpus spans hundreds of billions of time steps. While mostof the time series foundation models rely on temporal embedding, CHRONOS (Ansari et al, 2024)takes a different idea by learning a patch-based tokenizer, similar to conventional language models,to capture the intrinsic “language” of time series data.

Conventional foundation models often overlook the relationships between variables in multivariatetime series forecasting, typically extending to multivariate forecasting by independently combiningunivariate forecasts based on channel independence. Uni2TS (Woo et al, 2024) addresses this lim-itation by explicitly considering the expansion to arbitrary multivariate time series using variateIDs. Additionally, it leverages a large-scale time series dataset called LOTSA, which accounts formulti-domain and multi-frequency characteristics.

# 4.4 Advance of Diffusion Models

Diffusion models are renowned generative models that have gained prominence for their ability toproduce high-quality images, as demonstrated by DALL-E2 (Ramesh et al, 2022), Stable Diffu-sion (Rombach et al, 2022), and Imagen (Saharia et al, 2022). In addition to their success in imagegeneration, diffusion models have shown excellent performance in various fields, such as audio gener-ation, natural language processing, and video generation (Cao et al, 2024). Consequently, there hasbeen a growing number of research papers exploring their application in time series forecasting.

Diffusion models, theoretically proposed by Sohl-Dickstein et al (2015), are inspired by non-equilibrium statistical physics. They learn by progressively adding noise to the data and thengradually reversing the process to recover the original data. However, this initial research lacked con-crete methods for training, making practical implementation challenging. This issue was addressed byDenoising Diffusion Probabilistic Models (DDPM) (Ho et al, 2020) and Noise Conditional Score Net-works (NCSN) (Song and Ermon, 2019). DDPM models the forward and reverse processes explicitlybased on a probabilistic approach, enabling efficient learning. NCSN, on the other hand, employs scorematching techniques to refine the noise removal process, resulting in the generation of higher-qualitysamples.

A diffusion model operates through two primary processes: the forward process and the reverseprocess. In the forward process, noise is gradually added to the data until it transforms into completenoise. Conversely, the reverse process reconstructs meaningful data from random noise. By traininga denoising network, the model generates data by injecting random values and processing themthrough the reverse process.

Diffusion models have a significant advantage in modeling uncertainty through the forward andreverse processes. By providing multiple possible prediction outcomes instead of a single prediction,it can reflect the uncertainty of the real world, making it highly beneficial for TSF. Therefore, it


nomy and Methodologies of Foundation Models for Time Ser


<table><tr><td>Approach</td><td>Model Name</td><td>Main Improvement &amp; Methodology</td><td>Publication</td></tr><tr><td rowspan="4">Sequential modeling with LLM</td><td>GPT4TS</td><td>·Demonstrate the effectiveness of LLM for time series modeling
·Fine-tune the layer normalization and positional embedding parameters</td><td>2023</td></tr><tr><td>PromptCast</td><td>·Enable text-level domain-specific knowledge for TSF
·Cast TSF problem into question and answering format</td><td>2023</td></tr><tr><td>LLMTime</td><td>·Zero-shot TSF with pre-trained LLMs
·Covert time series input into a string of digits</td><td>2023</td></tr><tr><td>Time-LLM</td><td>·Align time series modality into text modality
·Introduce patch reprogramming to mitigate a large reprogramming space</td><td>2024</td></tr><tr><td rowspan="4">Pre-training</td><td>Lag-Llama</td><td>·First pre-training based time series foundation model
·Pre-train a decoder-only model with autoregressive loss</td><td>2024</td></tr><tr><td>TimesFM</td><td>·Pre-trained with hundreds of billions time steps
·Autoregressive decoding with arbitrary forecasting length</td><td>2024</td></tr><tr><td>CHRONOS</td><td>·Learning the language of time series
·Utilize tokenizer to capture the intrinsic language of time series</td><td>2024</td></tr><tr><td>UniTS</td><td>·Explicit consideration of multivariate TSF
·Provide variate IDs to directly consider multiple variables</td><td>2024</td></tr></table>

can offer prediction confidence intervals or enable probabilistic forecasting, thereby contributing todecision-making.

For forecasting tasks, it is crucial to employ conditional generation, where conditions are incorpo-rated into the model to produce data that aligns with the given conditions. In this context, historicaldata is used as a condition and injected into the model to enable it to learn and predict future values.Fig. 12 illustrates the diffusion model process, visualizing the transition from data to noise and backto data. Here, the addition of conditions explains the concept of the conditional diffusion model.

Diffusion-based models for time series forecasting have achieved significant performance improve-ments through the introduction of effective conditional embedding, the integration of time seriesfeature extraction, and advancements in the diffusion models themselves. The following sectionsexplain the key characteristics of diffusion-based models according to these classification criteria, andTable 9 provides a clear summary of this information.

![](images/0eff01b0d1c02f680b3e1535943916100dfee4db4f23c96e491807aceaa48725.jpg)



Fig. 12: Conditional Diffusion Process for Time Series Data


# 4.4.1 Effective Conditional Embedding

Early diffusion-based models for time series forecasting focused on effective conditional embeddingto guide the reverse process (Li et al, 2024b). Typically, for forecasting tasks, conditional diffusionmodels use past data as a condition to predict the future. Therefore, the meticulous constructionof the condition is paramount for the denoising model to effectively learn the data and enhanceprediction performance. This highlights the importance of data preparation in the forecasting process.This preparation allows the model to effectively utilize past information, thereby enhancing theaccuracy of future time-series predictions and improving the model’s ability to learn from historicaldata.

TimeGrad (Rasul et al, 2021) is the first notable diffusion model that operates using an autore-gressive Denoising Diffusion Probabilistic Model (DDPM) (Ho et al, 2020) approach. It encodes pastdata with an RNN, using the hidden state in a conditional diffusion model for forecasting. Thehidden state encapsulates information from past sequential data, capturing temporal dependencieseffectively. The denoising network employs dilated ConvNets with residual connections, adopted fromWaveNet (Van Den Oord et al, 2016) and DiffWave (Kong et al, 2021), which are designed for audiogeneration. Unlike the commonly used U-net for image synthesis in diffusion models, TimeGrad uti-lizes a broader receptive field suitable for time series data, similar to audio. CSDI (Tashiro et al,2021) captures temporal and feature dependencies of time series using a two-dimensional attentionmechanism. Designed for both forecasting and imputation tasks, CSDI replaces RNNs with Trans-formers for imputation since RNNs are not suitable. The attention mechanism learns the relationshipsacross all positions in the input sequence, capturing long-term dependencies effectively. Therefore,for long-term time series forecasting, using attention is more advantageous than RNNs and dilatedconvolution, as seen in TimeGrad. SSSD (Alcaraz and Strodthoff, 2023) demonstrates that replac-ing the Transformer used as the denoising network in CSDI with the S4 model (Gu et al, 2022)yields superior performance. This is because S4 is more efficient and better at capturing long-termdependencies compared to the high computational complexity required for Transformer attention.TimeDiff (Shen and Kwok, 2023) enhances prediction accuracy by using future mixups duringtraining, mixing past data with actual future values to generate conditional signals. Including some

actual future values helps the model create effective conditional signals for more accurate predic-tions. Additionally, it addresses the boundary disharmony issue in non-autoregressive models likeCSDI and SSSD. TimeDiff uses a simple statistical model, the Linear Autoregressive model, to pro-vide an initial guess, alleviating the boundary disharmony problem. TMDM (Li et al, 2024b) offersan extreme form of effective conditional embedding. It states that the best condition is the predic-tion itself, using prediction values from transformer-based models like Informer (Zhou et al, 2021) orAutoformer (Wu et al, 2021), which have shown good performance in time series forecasting tasks, asthe condition. This allows the Transformer to handle the estimated mean while the diffusion modelfocuses on uncertainty estimation. Unlike previous researches that utilize conditional embeddingsonly in the reverse process, TMDM uses conditional information as prior knowledge for both theforward and reverse processes.

# 4.4.2 Time Series Feature Extraction

Time series data hides various features, and there are techniques to extract these unique characteris-tics effectively. Thus, many works have emerged that combine time series feature extraction methodswith diffusion models to understand the complex patterns in time series data and improve predictionperformance.

# Decomposition

Decomposition techniques involve breaking down time series data into components such as trend,seasonality, and irregularity, analyzing the unique patterns of each component. Diffusion-TS (Yuanand Qiao, 2024) points out that conventional methods cannot properly learn each component becausethe forward process causes the components to collapse. Therefore, it models the decomposed timeseries data individually during the diffusion process, learning each component, such as trend, sea-sonality, and residuals, independently and then recombining them to restore the entire time seriesdata.

# Frequency Domain

Fourier analysis, a type of decomposition technique, converts time series data into the frequencydomain to analyze periodic components. This method helps identify periodic patterns and removenoise. Crabb´e et al (2024) explore the idea that the representation of time series data in the frequencydomain can provide useful inductive biases for score-based diffusion models. This paper demonstratesthat the components of time series data are more clearly represented in the frequency domain, anddiffusion models in the frequency domain outperform those in the time domain.

# Multi-Scale

Multi-scale techniques analyze time series data at various time scales, effectively extracting long-termtrends and diverse features. This approach plays a crucial role in understanding the complex patternsof time series data to improve prediction performance. MG-TSD (Fan et al, 2024b) observes that theforward process of diffusion models aligns with gradually smoothing fine data into coarser representa-tions. It suggests that coarse-granularity data can serve as effective guides in the intermediate stagesof diffusion models. In other words, the initial stages of the reverse process involve coarse-granularitydata, guiding the process to intermediate-stage targets. This multi-granularity level approach helpslearn various levels of information, enhancing prediction stability and accuracy. mr-Diff (Shen et al,2024) constructs time series data at multiple resolutions, performing sequential predictions at eachresolution. In the initial stages, it predicts coarse data, and in subsequent stages, it uses these predic-tions as conditions to generate finer data gradually. This structure incrementally adds finer patternsand noise at each stage, ultimately reconstructing high-resolution time series data. This allows foreffective prediction of both long-term trends and short-term fluctuations in time series data.

# 4.4.3 Additional Approaches

Beyond the two main approaches mentioned earlier, there are numerous examples where techniquesevolving from the diffusion framework itself are applied to models for time series forecasting.

# Score-Based Generative Modeling through SDEs

Score-Based Generative Modeling through Stochastic Differential Equations (SDEs)(Song et al, 2021) serves as continuous energy-based generative models, handling data in a con-tinuous time domain, thus reflecting natural and precise changes and noise in the real world. Thepreviously discussed methods are based on DDPM (Ho et al, 2020), which add and remove noiseincrementally using fixed-time steps. However, the DDPM approach faces challenges such as specificfunctional constraints and sensitivity to hyperparameters. Some research applying SDEs overcomesthese limitations and explores diverse approaches. ScoreGrad (Yan et al, 2021) is the first workto apply SDE, overcoming the constraints of DDPM-based models and offering a more flexible andpowerful generative model. $\mathbf { D ^ { 3 } M }$ (Yan et al, 2024) addresses the limitations of conventional SDEs,such as the complexity of determining drift and diffusion coefficients and slow generation speed,by utilizing a decomposable noise-removal diffusion model based on explicit solutions. This methodreduces computational complexity through clear SDE formulations and separates the signal decayand noise injection processes in model design. As a result, it enhances model efficiency and accu-racy while accelerating generation speed. Crabb´e et al (2024) improve denoising score matching inthe frequency domain by using mirrored Brownian motions instead of standard Brownian motion,emphasizing the symmetry between components when applying SDEs.

# Latent Diffusion Model

Latent Diffusion Model (Rombach et al, 2022) is a generative model that operates not on theoriginal data directly but within a latent space where the diffusion process takes place. Transformingdata into the latent space reduces complexity and stabilizes the training process, resulting in high-quality outputs. LDT (Feng et al, 2024b) applies the concept of the latent diffusion model to timeseries data, addressing the non-stationarity issues that often arise during the compression into thelatent space. Dynamically updating statistical properties during the autoencoder training processeffectively overcomes these challenges and enhances model performance.

# Guidance

Some works use guidance instead of explicitly feeding conditions into the denoising network for fore-casting. Diffusion-TS (Yuan and Qiao, 2024) employs classifier guidance, using a separate classifierto guide the sampling process through the gradients of the classifier. This method maintains the basicunconditional diffusion model while performing conditional generation tasks through various classi-fiers. It generates samples that are better aligned with specific conditions, resulting in higher-qualityoutputs. However, classifier guidance requires a classifier for each time step, which necessitates train-ing a new classifier instead of using pre-trained ones. LDT (Feng et al, 2024b) uses classifier-freeguidance, learning both conditional and unconditional models within a single model to perform con-ditional sampling. This work eliminates the need for an additional classifier and implicitly obtainsa classifier, making implementation simpler and more efficient. TSDiff (Kollovieh et al, 2024) pro-poses observation self-guidance, allowing the use of an unconditional diffusion model for conditionalgeneration without separate networks or modifications to the training process.

# 4.5 Debut of the Mamba

# 4.5.1 History of the State Space Models (SSMs)

One of the notable recent developments is the emergence of a new architecture called Mamba(Gu and Dao, 2024). In an atmosphere previously influenced by Transformers, Mamba has garneredsignificant interest from researchers and is rapidly establishing its own ecosystem.

RNNs, which were once the mainstream in sequence modeling, lost their dominance after theadvent of Transformers. This was due to the limitations in information encapsulation of a “context”(single vector) in RNN-based encoder-decoder models and the slow training speed inherent in theirrecurrent nature. In contrast, the parallelism of the attention mechanism and its ability to focus onall individual pieces of information overcame the limitations of RNNs and demonstrated superiorperformance. However, new challenges have emerged with Transformers, such as the quadratic com-putational complexity, which limits the window length, and the increased memory requirements forprocessing long sequences. Subsequently, many efforts have been made to overcome the limitationsof both approaches while preserving their advantages. In this context, some research that continuesthe philosophy of RNNs has turned its attention to state space models (SSMs) (Kalman, 1960).


nomy and Methodologies of Diffusion Models for Time Seri


<table><tr><td>Main Improvement</td><td>Model Name</td><td>Main Methodology</td><td>Diffusion Type</td><td>Conditional Type</td><td>Publication</td></tr><tr><td rowspan="5">Effective Conditional Embedding</td><td>TimeGrad</td><td>· Autoregressive DDPM using RNN &amp; Dilated Convolution</td><td>DDPM</td><td>Explicit</td><td>2021</td></tr><tr><td>CSDI</td><td>· 2D Attention for Temporal &amp; Feature Dependency
· Self-supervised Training for Imputation</td><td>DDPM</td><td>Explicit</td><td>2021</td></tr><tr><td>SSSD</td><td>· Combination of S4 model</td><td>DDPM</td><td>Explicit</td><td>2023</td></tr><tr><td>TimeDiff</td><td>· Future Mixup
· Autoregressive Initialization</td><td>DDPM</td><td>Explicit</td><td>2023</td></tr><tr><td>TMDM</td><td>· Integration of Diffusion and Transformer-based Models</td><td>DDPM</td><td>Explicit</td><td>2024</td></tr><tr><td rowspan="4">Time-series Feature Extraction</td><td>Diffusion-TS</td><td>· Decomposition techniques
· Instance-aware Guidance Strategy</td><td>DDPM</td><td>Guidance</td><td>2024</td></tr><tr><td>Diffusion in Frequency</td><td>· Diffusing in the Frequency Domain</td><td>SDE</td><td>Explicit</td><td>2024</td></tr><tr><td>MG-TSD</td><td>· Multi-granularity Data Generator
· Temporal Process Module
· Guided Diffusion Process Module</td><td>DDPM</td><td>Explicit</td><td>2024</td></tr><tr><td>mr-Diff</td><td>· Integration of Decomposition and Multiple Temporal Resolutions</td><td>DDPM</td><td>Explicit</td><td>2024</td></tr><tr><td rowspan="2">SDE</td><td>ScoreGrad</td><td>· Continuous Energy-based Generative Model</td><td>SDE</td><td>Explicit</td><td>2021</td></tr><tr><td>D^3 M</td><td>· Decomposable Denoising Diffusion Model based on Explicit Solutions</td><td>SDE</td><td>Explicit</td><td>2024</td></tr><tr><td>Latent Diffusion Model</td><td>LDT</td><td>· Symmetric Time Series Compression
· Latent Diffusion Transformer</td><td>DDPM</td><td>Guidance</td><td>2024</td></tr><tr><td>Guidance</td><td>TSDiff</td><td>· Observation Self-guidance</td><td>DDPM</td><td>Guidance</td><td>2023</td></tr></table>

SSM is a mathematical framework used to model the dynamic state of a system that changes overtime, compressing only the essential information for effective sequential modeling. SSM describesthe relationship between the internal state of the system and external observations and is used invarious fields such as control theory, signal processing, and time series analysis. It comprises a ‘Stateequation’ that explains how the internal state changes over time and an ‘Observation equation’ thatexplains how the internal state is transformed into external observations. Although it is a continuousmodel that performs linear transformations on the current hidden state and input, it can handlediscrete sequences like time series through discretization (Fig. 13). It closely resembles RNNs in thatit combines observation data and hidden state data.

![](images/0041cd30cfe74dd91e590e13c70fca98a4ef33a721c9b69f5a40729aa526510b.jpg)



Fig. 13: Diagram of Discretized State Space Model


However, SSMs have different characteristics from RNNs in that they are linear and time-invariant (LTI). This means that the operations performed on each token do not vary, allowing forthe pre-computation of global kernels. In other words, the parameter matrices of the system do notchange over time and operate consistently across time. Therefore, the ability to precompute globallyapplicable kernels enables parallel processing, which can overcome the limitations of RNNs.

Early applications of SSMs, such as the S4 (Gu et al, 2021) model, utilized diagonalization of thetransition matrix to effectively model long-term dependencies in long and complex sequence data.These models were able to achieve high performance when combined with deep learning architectureslike Transformers, RNNs, and CNNs. Subsequently, based on S4, advanced blocks like H3 (Fu et al,2023) were developed, which hierarchically structured convolution, gating, and MLP to provide moreefficient and powerful sequence modeling (Fig. 14).

# 4.5.2 Introduction of the Mamba

SSMs have fixed state transition and observation equations, which limits their ability to flexiblyhandle input data. Furthermore, due to their inherent continuous system origins, they are weaker ininformation-dense and discrete domains such as text.

Mamba addresses these limitations of SSMs by introducing an advanced deep structured SSMmodel with selective SSM. It is designed so that the parameters of the SSM dynamically changedepending on the input, allowing for selective memory or disregard of specific parts of the sequence,enabling efficient data processing. However, this approach, while enhancing the system’s flexibility tobetter learn complex patterns, sacrifices the parallel processing advantages of SSMs. To compensate,traditional techniques such as kernel fusion, parallel scan, and recomputation are applied to effi-ciently implement selective SSMs on GPUs. Mamba adopts a simplified architecture centered aroundselective SSM, replacing the first multiplicative gate in the traditional H3 block with an activationfunction and incorporating an SSM into the gated MLP block. Due to the absence of attention mech-anisms or separate MLP blocks, computational complexity is reduced, resulting in efficient training,fast inference, and high scalability and versatility.

![](images/9de7be433d5eb44e69b0a4b7471362988093faa3ad8d646c573d52fd6a4cd1d3.jpg)



Fig. 14: Structure of the Mamba Block


# 4.5.3 Applications of the Mamba

In the field of time-series forecasting, there is a growing trend to apply Mamba. A lot of variantsbased on Mamba have been proposed to address TSF tasks, and these models are reported to exceedthe performance of Transformer SOTA models.

Most of these models typically share the following common features:

• They aim to ensure stability in large networks. Mamba, based on SSM, heavily relies on theeigenvalues of the system’s dynamic matrix. Since the general solution of the system is expressedas an exponential function of the eigenvalues, if the real part of the eigenvalues is positive, thesystem cannot converge and becomes unstable. Therefore, a key feature is the ideation aimed atimproving this instability.

• They explicitly incorporate the learning of channel correlations. Recent research has demonstratedthe superiority of channel dependence (CD) learning strategies, and the adoption of the CD strategyby the iTransformer, which shows SOTA performance, has further heightened this trend. This isa common feature observed in other foundational models as well.

• They incorporate various techniques from transformer-based models. Mamba-based models, whichintegrate various techniques from Transformers such as patching, frequency domain applica-tion, bidirectional processing, and FFN incorporation, are demonstrating significant performanceimprovements. The diverse techniques developed over many years of research on Transformermodels provide valuable ideation for Mamba variants.

The following are representative examples of Mamba-based models for handling TSF tasks andtheir key technical features.

# Embedding and Multi-Scale Learning

In this section, we introduce methodologies for embedding and learning from data at various scales.These approaches primarily focus on richly extracting information to capture long-term dependenciesand context.

TimeMachine (Ahamed and Cheng, 2024) aims to capture long-term dependencies and contextat different scales through two-stage embeddings. It is divided into internal and external embeddingsbased on embedding dimensions, and each section consists of two parallel Mamba modules thatexplore temporal dependencies and channel dependencies, respectively. The internal Mamba moduleoperates at a low resolution to capture both global and local contexts, while the external Mambamodule operates at a high resolution to capture the global context.

# Channel Correlation Learning

Next, we examine models that focus explicitly on learning the inter-channel correlations in time seriesdata. These models are centered around techniques that effectively integrate information from eachchannel and model their interdependencies.

S-Mamba (Wang et al, 2024d) features bi-directional Mamba blocks to consider both past andfuture information, enabling the learning of inter-channel correlations. The role of learning tempo-ral dependencies is assigned to the Feed Forward Network (FFN). By clearly separating the roles ofMamba blocks and FFN, computational efficiency is improved, ensuring the stability of the Mambaarchitecture in large networks. In SiMBA (Patro and Agneeswaran, 2024b), channel modeling isachieved through Einstein FFT (EinFFT), while sequence modeling is handled by Mamba modules.After applying the Fast Fourier Transform (FFT), the real and imaginary parts are separated, andtheir respective weights are learned. Channel mixing is performed through Einstein Matrix Multi-plication (EMM), creating new data that reflects the relational information between channels, thusinternalizing channel relationships. Additionally, to ensure system stability, eigenvalues of the statematrix are adjusted to be negative through Fourier transformation and nonlinear layers. MambaTS(Cai et al, 2024c) reconstructs sequences by integrating past information of all variables to learnchannel correlations. Since variable information is integrated in advance, unnecessary convolution inMamba blocks is removed to enhance computational efficiency, and dropout is applied to TemporalMamba Blocks (TMB) to reduce overfitting. Additionally, Variable Permutation Training (VPT) isintroduced to dynamically determine the optimal order of integrated variable information, enablingpredictions based on the optimal sequence of variables. C-Mamba (Zeng et al, 2024) generates newvirtual sample data by linearly combining different channels (channel mixup), which is expected toenhance generalization performance. It uses a main block that combines patched Mamba moduleswith attention modules for learning channel relationships.

# Sequence Information and Dependency Learning

This category emphasizes methods for learning the sequential information and dependencies in timeseries data. It proposes various techniques for modeling both long-term and short-term dependencieswithin the series.

Mambaformer (Xu et al, 2024a) is a hybrid model that combines Mamba with a Transformerdecoder framework. Since the Mamba block naturally internalizes the sequence order information,positional embedding is unnecessary. Long-term dependencies are learned through the Mamba block,while short-term dependencies are captured through the self-attention layer, effectively capturingthe overall dependencies. This approach overcomes the computational efficiency limitations of atten-tion mechanisms. Bi-Mamba+ (Liang et al, 2024a) integrates patching techniques to finely learninter-dependencies in the data. To preserve long-term information, it introduces Mamba+ blocks,which add a forget gate to the Mamba block. It also employs a Bi-Mamba+ encoder to processinput sequences bidirectionally. Using the Spearman correlation coefficient, the Series-Relation-Aware (SRA) decider is designed to automatically select channel tokenization strategies (CI or CD).DTMamba (Wu et al, 2024b) is composed of Dual Twin Mamba blocks, effectively learning long-term dependencies in time series data channel independently. Each Twin Mamba block consists oftwo parallel Mamba structures that process the same input data to capture different patterns effec-tively. One Mamba structure learns detailed patterns and short-term variations, while the otherlearns overall patterns and long-term trends.

# Theoretical Frameworks and Efficient Modeling

Lastly, we explore models that introduce new theoretical frameworks or propose efficient modelingtechniques. These models focus on effectively capturing the dynamic characteristics of time seriesdata and employ theoretical approaches and methodologies that enhance computational efficiency.

Time-SSM (Hu et al, 2024a) proposes a theoretical framework called the Dynamic SpectralOperator, which extends the Generalized Orthogonal Basis Projection (GOBP) theory for efficientuse of SSM. The Dynamic Spectral Operator explores the changing spectral space over time toeffectively capture the dynamic characteristics of time series data. To implement this, a novel variantof the SSM basis called Hippo-LegP is proposed, enabling more precise modeling of time series dataand achieving optimal performance through S4D-real initialization. This allows it to demonstrateexcellent performance with only about one-seventh of the parameters required by Mamba models.Chimera (Behrouz et al, 2024) features the use of 2D SSM to capture dependencies independentlyalong the time and variable axes. By updating states in parallel along both axes, it achieves efficientcomputation.

The emergence of a new deep learning architecture, Mamba, is causing a shift in the long-standinghegemony of deep learning architectures. In current TSF tasks, which increasingly deal with long-term sequences, Mamba’s strengths—efficient sequence processing, selective information retention,

simplified architecture, and hardware optimization—prove to be highly valuable. These featuresallow Mamba-based models to overcome the limitations of Transformers, showing rapid performanceimprovements in a short period and suggesting a new direction for deep learning modeling. Thegrowth trajectory of Mamba raises attention to whether it will become the new dominance in thisfield.

# 5 TSF Latest Open Challenges & Handling Methods

# 5.1 Channel Dependency Comprehension

# Spread of Channel Independent Strategy

Multi-variate time series (MTS) forecasting primarily hinges on how well it can learn the short-term and long-term temporal dependencies. However, many recent real-world datasets predominantlydeal with multivariate data, where the relationships between variables carry significant semanticinformation. As the relationships between variables become more complex, models can provide moreinformation and thus improve predictive performance by leveraging this complexity.

Traditionally, it has been vaguely assumed that understanding the relationships between variablesin time series forecasting problems would make better performance. Despite being obtained fromdifferent instruments, the data observe the same phenomenon, which intuitively suggests that theyoffer rich interpretations from various perspectives. However, with the PatchTST (Nie et al, 2023)model adopting a channel-independent (CI) strategy and achieving state-of-the-art (SOTA) perfor-mance, research has begun to question the previously assumed channel-dependent (CD) strategy.These studies have shown that high performance can be attained without learning the interactionsbetween variables.

The CI strategy simplifies the model by excluding inter-variable modeling, allowing it to focussolely on learning the temporal dependencies of each channel. This approach reduces model com-plexity and enables faster inference, while also mitigating the risk of overfitting due to noise frominter-variable interactions. Additionally, since the addition of new channels does not affect the model,it can adapt flexibly to changes in data. These advantages have led many studies to adopt the CIstrategy, resulting in improved performance. However, the CI strategy did not consistently showsuperior performance across all studies. The CD strategy still demonstrated high performance inmany studies, such as iTransformer (Liu et al, 2024c). Additionally, both strategies showed inconsis-tent performance depending on the datasets used. Consequently, without clear justification, both CIand CD strategies were employed according to the researchers’ preferences for a period of time.

# Importance of Learning Channel Correlations

Learning the correlations between variables remains important. In multivariate time series data,each variable does not change independently but is interdependently related to others. Even if theserelationships sometimes introduce noise or fail to provide critical information, the relationships them-selves are not devoid of meaning. The information from multivariate variables often intertwines tocreate complex patterns that cannot be captured by a single variable. Understanding the correlationsbetween multiple variables helps interpret these complex patterns.

Modeling the correlations between variables is also crucial for improving prediction accuracy.Especially when dealing with long sequence patterns, it is essential to understand the numerous localpatterns within them. During this process, important causal relationships are often derived fromother variables or exogenous factors. In the case of noisy data, learning the correlations betweenvariables can help extract key information effectively. By comprehensively observing the informationfrom multiple variables, it is possible to emphasize important features and complement missinginformation.

# What Makes CI Look Better?

If the CD strategy is important and has the potential for significant performance improvements, whydo many studies show that the CI strategy performs better? To answer this question, some researchhas been conducted, and the following summary can be provided based on the “The capacity androbustness trade-off: Revisiting the channel independent strategy for multivariate timeseries forecasting” (Lu Han, 2024) paper.

According to the study, an examination of the Auto-correlation Function (ACF) values betweenthe training set and test set of the benchmark data, which have been used in various experiments,


onomy and Methodologies of Mamba Models for Time Seri


<table><tr><td>Main Improvement</td><td>Model Name</td><td>Main Methodology</td><td>Channel Correlation</td><td>Base</td><td>Publication</td></tr><tr><td>Embedding and Multi-Scale Learning</td><td>TimeMachine</td><td>· Integrated Quadruple Mambas</td><td>CD</td><td>Mamba</td><td>2024</td></tr><tr><td rowspan="4">Channel Correlation Learning</td><td>S-Mamba</td><td>· Channel Mixing: Mamba VC Encoding Layer
· Sequence Modeling: FFN TD Encoding Layer</td><td>CD</td><td>Mamba MLP</td><td>2024</td></tr><tr><td>SiMBA</td><td>· Channel Mixing: Einstein FFT (EinFFT)
· Sequence Modeling: Mamba</td><td>CD</td><td>Mamba</td><td>2024</td></tr><tr><td>MambaTS</td><td>· Temporal Mamba Block (TMB)
· Variable Permutation Training (VPT)</td><td>CD</td><td>Mamba</td><td>2024</td></tr><tr><td>C-Mamba</td><td>· Channel Mixup
· C-Mamba Block (PatchMamba + Channel Attention)</td><td>CD</td><td>Mamba</td><td>2024</td></tr><tr><td rowspan="3">Sequence Information and Dependency Learning</td><td>Mambaformer</td><td>· Mambaformer (Attention + Mamba) Layer</td><td>CI</td><td>Mamba Transformer</td><td>2024</td></tr><tr><td>Bi-Mamba+</td><td>· Series-Relation-Aware (SRA) Decider
· Mamba+ Block
· Bidirectional Mamba+ Encoder</td><td>CI/CD</td><td>Mamba</td><td>2024</td></tr><tr><td>DTMamba</td><td>· Dual Twin Mamba Blocks</td><td>CI</td><td>Mamba</td><td>2024</td></tr><tr><td rowspan="2">Theoretical Frameworks and Efficient Modeling</td><td>Time-SSM</td><td>· Dynamic Spectral Operator with Hippo-LegP</td><td>CD</td><td>Mamba</td><td>2024</td></tr><tr><td>Chimera</td><td>· 2-Dimensional State Space Model</td><td>CD</td><td>Mamba</td><td>2024</td></tr></table>

revealed a distribution shift. The ACF can identify the correlation between data values at specifictime lags in time series data.

A distribution shift refers to the difference in statistical properties between the training data andtest data extracted from the same dataset. Distribution shifts can occur for various reasons, oftendue to anomalies in the training series or variations in the trend, but sometimes the exact causecannot be clearly defined. An important finding from the study is that the CI strategy demonstratesrelatively greater robustness to distribution shifts. Since the CI strategy relies on the average ACFof all channels, it is less sensitive to distribution shifts compared to the CD strategy, which dependson the ACF variations of individual channels. Additionally, by excluding inter-variable relationships,the CI strategy simplifies the model, reducing the likelihood of overfitting and enhancing robustness.Therefore, the CI strategy has been able to demonstrate good performance on benchmark datasetswhere distribution shifts are present.

However, this does not imply that the CI strategy is superior to the CD strategy. It merelysuggests that, in some datasets, the positive effects resulting from the robustness of the CI strategyoutweigh the advantages of learning inter-variable relationships. Conversely, if distribution shifts canbe effectively alleviated, the CD strategy can provide much more useful information than the CIstrategy. To this end, even the application of simple regularization methods, such as Predict Residualwith Regularization (PRReg), adapting low-rank layers, and using the MAE loss function, can reversethe performance gap. Furthermore, various distribution shift alleviation methodologies have beenresearched, and these can be applied in a model-agnostic manner. The details of these methodologieswill be discussed in the next section, and the previous content has been illustrated in Fig. 15

![](images/59d2dd65511d163bbbe0fe613d733ede76531c679ffb295a1e94922040195564.jpg)



Fig. 15: Comparison of CI and CD Strategies in Channel Correlations


# Recent Approaches

As the effectiveness of learning channel correlation has been validated, recent multivariate time seriesforecasting problems have predominantly adopted the CD strategy. In contrast, the CI strategy isincreasingly applied in a limited way, primarily in univariate models focused on temporal dependency.

Approaches to addressing recent MTS problems can be broadly categorized into three types. Thefirst approach involves explicitly integrating modules for channel mixing into the backbone model.

Most models adopt this strategy, aiming to maximize effectiveness by explicitly incorporating channelcorrelation. However, as discussed earlier, distribution shifts in the dataset can potentially degrade theperformance of the CD strategy, necessitating the introduction of appropriate alleviation methods.By employing this combination, the strengths of learning channel correlations can be applied morereliably, leading to enhanced model performance. The distribution shift alleviation methods will bediscussed in detail in Section 5.2.

The second approach implicitly incorporates channel correlations into the input. Instead of explic-itly integrating channel learning modules into the backbone model, this method takes a preprocessingapproach by mixing channel information in advance to create a new input, which is then fed intothe prediction model. This new input combines the mixed channel information with the originalindividual channel data before being processed by the prediction model. By inherently reflecting therelationships between channels, this method allows even a simple CI backbone to achieve the effec-tiveness of a more complex CD model. Models like SOFTS (Han et al, 2024) and C-LoRA (Nieet al, 2024) fall into this category.

The third approach adaptively selects between the CI and CD strategies. The model is designedto operate in both directions, allowing it to choose the most effective strategy based on the charac-teristics of the dataset being used. In some cases, like TSMixer (Ekambaram et al, 2023), the usercan manually select the appropriate strategy, while in others, such as Bi-Mamba+ (Liang et al,2024a), the model automatically determines the optimal strategy. The second and third approachesboth utilize the CI strategy alongside the CD strategy, aiming to integrate the advantages of bothinto the model. A summary of the criteria for selecting these strategies is presented in Fig. 16

In the past, the criteria for selecting channel strategies were ambiguous, but recent research hasprovided clear guidelines. Based on these advancements, users can now choose the appropriate channelstrategy according to the characteristics of their dataset, effectively improving model performancethrough various approaches.

![](images/9ce5786d01068b661cc8104045ab33e2a0237b121fa941d668cd1aeefc294a4d.jpg)



Fig. 16: Recent Approaches to Channel Strategies


# 5.2 Alleviation of Distribution Shift

Many real-world time series data exhibit non-stationarity, where their distribution gradually changes.For instance, trends such as increasing electricity consumption and shifts in consumer preferences canlead to changes in data distribution. This non-stationarity introduces challenges for the generalizationof time series forecasting (TSF) models by creating distribution discrepancies within the trainingdata and between the training and test data. To mitigate the issue of distribution shift caused bynon-stationarity in time series forecasting, various research efforts have been proposed.

Representative approaches for addressing distribution shifts include Domain Adaptation, Trans-fer Learning, and Robustness Techniques. Domain Adaptation aims to reduce the distributiondifferences between the source and target domains to improve generalization performance acrossdomains. This approach is divided into supervised domain adaptation, which uses some labeled datafrom the target domain, and unsupervised domain adaptation which aligns feature distributionsusing unlabeled datasets. Unsupervised domain adaptation began with the introduction of Maxi-mum Mean Discrepancy (MMD) (Wang et al, 2023a) -based methods and advanced with theemergence of Generative Adversarial Networks (GANs) (Sankaranarayanan et al, 2018), whichled to the development of adversarial domain adaptation techniques. Transfer Learning (Pan andYang, 2009) refers to applying a model trained in one domain or task to a new dataset. This includesFeature Transfer, which reuses or retrains specific layers of a pre-trained network for a new task,and Fine-Tuning which adjusts the entire pre-trained model to fit the new dataset. Transfer learn-ing originated from pre-trained embeddings in NLP in the late 1990s, expanded through CNNs, andeventually evolved with LLMs. Robustness Techniques enhance a model’s ability to maintainperformance despite data uncertainties, such as noise, outliers, and data scarcity. Methods such asDropout (Srivastava et al, 2014) and Data Augmentation (Wong et al, 2016) help models handlenoise, while robust loss functions like the Huber loss (Huber, 1992) reduce sensitivity to outliers.These techniques have been widely adopted in time series forecasting models since 2020.

This survey focuses on one prominent approach that introduces normalization and denormaliza-tion modules within a normalization-TSF model-denormalization framework. This framework hasevolved as a specialized approach for mitigating non-stationarity within the same domain of timeseries data. Here, normalization is applied to the look-back window before feeding it into the TSFmodel, aiming to remove input statistics. As a result, the TSF model processes inputs from a lesstime-variant distribution. Denormalization is then applied to the forecasting window obtained fromthe TSF model to restore the original statistics. The denormalized forecasting window is ultimatelyused as the final forecast. However, calculating the appropriate statistics for normalization and denor-malization in non-stationary scenarios is a non-trivial task, and numerous studies have been proposedwithin this framework to address this challenge.

DAIN (Passalis et al, 2019) introduces a Deep Adaptive Input Normalization layer that learnshow much to shift and scale each observation end-to-end. Considering the changing distributioncharacteristics of time series data, this approach outperforms widely used normalization methods likebatch normalization (Ioffe and Szegedy, 2015) and instance normalization (Ulyanov et al, 2016) acrossvarious domains, highlighting the importance of normalization techniques in TSF. However, DAINomits a denormalization step, meaning it does not account for restoring statistics in the forecastingresults. In contrast, RevIN (Kim et al, 2021), which extends instance normalization to be reversible,adopts a normalization-denormalization framework. RevIN performs instance normalization on eachlook-back window, followed by variable-wise multiplication of a learnable scale factor and additionof a bias factor. During de-normalization, the same parameters are applied in reverse. NST (Liuet al, 2022) normalizes the look-back window in a non-parametric manner without a learnable affinetransformation and then de-normalizes the prediction window using the mean and standard deviationof the look-back window. Dish-TS (Fan et al, 2023) learns a module that predicts the statistics of theprediction window, considering the distribution shift between the look-back window and subsequentprediction windows, and performs normalization and de-normalization accordingly. SAN (Liu et al,2024e) considers the distribution shift within both the look-back window and prediction windows,performing statistical prediction on smaller slices.

# 5.3 Enhancing Causality

# Why Causal Analysis is Essential for Accurate Time Series Forecasting

Causal analysis is crucial in achieving accurate time series forecasting by better understanding theunderlying factors driving data patterns. Time series data often exhibit correlations that can be mis-leading without a proper understanding of causality. For instance, two variables may show a strongcorrelation simply due to coincidence or because they are both influenced by a third, unobserved vari-able. Without distinguishing between correlation and causation, forecasting models risk attributingchanges in the data to irrelevant or spurious factors, leading to inaccurate predictions. Causal anal-ysis helps overcome this by identifying the true cause-and-effect relationships, ensuring the model isgrounded in reality rather than coincidental patterns.


Table 11: Normalization-Denormalization-based Approaches to Alleviate Distribution Shifts in TimeSeries Forecasting


<table><tr><td>Model Name</td><td>Main Improvement &amp; Methodology</td><td>Publication</td></tr><tr><td>DAIN</td><td>·Introduce adaptive normalization
·Adaptive scaling, shift, and gating layers to normalize look-back window</td><td>2019</td></tr><tr><td>RevIN</td><td>·Introduce normalization-denormalization framework
·Denormalize prediction with reversible affine transformation parameters</td><td>2022</td></tr><tr><td>NST</td><td>·Non-parametric normalization-denormalization
·Normalize and denormalize using mean and std of look-back window without learnable parameters</td><td>2022</td></tr><tr><td>Dish-TS</td><td>·Divides distribution shifts:
  1) within look-back window and 2) between look-back window and forecasting window
·Introduce Dual-CONET modules for statistics prediction</td><td>2023</td></tr><tr><td>SAN</td><td>·Predict statistics in slice-level
·Slice forecasting windows and predict mean and std for each slice</td><td>2023</td></tr></table>

Moreover, causal analysis enhances the interpretability and practical application of forecastingmodels. By explaining how different variables influence the outcome, causal models uncover theunderlying mechanisms driving the forecast, which is crucial for decision-making. Businesses andpolicymakers can use these insights to predict the impact of specific actions, such as policy changes ormarketing strategies, and make informed decisions based on the likely effects. This ability to simulateinterventions and conduct counterfactual scenarios makes causal analysis an indispensable tool foraccurate and actionable time series forecasting.

# Research on TSF with Causality

As mentioned earlier, utilizing causality in time series forecasting not only improves prediction accu-racy but also enhances model interpretability. For these reasons, research applying causal discoveryinformation to time series forecasting models is actively ongoing. In particular, various methodologieshave been proposed in fields such as healthcare, environmental science, and social sciences, wherecausal discovery is actively researched.

In TSF, various causal inference methods are employed to identify cause-and-effect relationshipswithin the data. The Granger Causality Test (Granger, 1969) examines whether the historicalinformation of one variable helps predict the future of another variable. This method detects thedirection of causal influence based on regression analysis. However, it has limitations in fully exclud-ing indirect correlations caused by the presence of a third variable. Structural Causal Models(SCM) (Pearl et al, 2000) utilize causal graphs and structural equations to model the relation-ships between variables. These models enable intervention simulations and counterfactual analyses,providing a visual representation of causal relationships to aid interpretation and integrate interac-tions across multiple variables. Do-Calculus (Pearl, 1995) is an intervention analysis technique forquantitatively analyzing the effects of interventions by computing the impact that changes in spe-cific variables have on others, thereby supporting causal predictions. Propensity Score Matching(PSM) (Rosenbaum and Rubin, 1983) is a method for performing causal inference by matchinggroups with similar characteristics. This approach minimizes the influence of confounders and moreaccurately evaluates the effects of interventions. Directed Acyclic Graphs (DAGs) (Pearl, 1998)are directional, non-cyclic graphs that visually represent the relationships between causes and effects.DAGs help clearly identify causal structures and understand complex interactions among variables.Various attempts are being made to enhance the causality of TSF by utilizing these diverse causalinference methods (Runge et al, 2019; Sch¨olkopf et al, 2021).

Qian et al (2023) introduce a model to predict Kuroshio Volume Transport (KVT). It employsmultivariate causal analysis to discover causal relationships and selects only the variables with causalrelationships to make predictions using an LSTM model. This approach captures meaningful infor-mation from causally related variables and prevents the model from being confused by unrelatedvariables. Mu et al (2023) propose a model for predicting the North Atlantic Oscillation. It usesinformation obtained through causal discovery not only for variable selection but also by directlyapplying it to a GCN (Graph Convolutional Network). The overall structure utilizes a symmetricalencoder and decoder of ConvLSTM, with the GCN acting as a coupler in between. Sharma et al(2022) focuses on a model for energy consumption. It applies the Granger causality test to determine

the causal relationship between weather conditions and energy consumption and then integrates thiscausal information into a Bi-LSTM to improve energy consumption prediction accuracy. Causal-GNN (Wang et al, 2022b) is a model aimed at epidemic forecasting. It extracts causal features usingthe SIRD model and incorporates them into an Attention-Based Dynamic GNN module to learnspatio-temporal patterns. Caformer (Zhang et al, 2024b) criticizes existing methods for failing tolearn causal relationships effectively due to false correlations caused by environmental factors. Toaddress this, it explicitly models environmental influences and removes them to obtain reliable causalrelationships.

# 5.4 Time Series Feature Extraction

Time Series Feature Extraction is the process of extracting useful information from time series datato enhance model performance. In other words, it involves identifying and quantifying time seriesdata into key patterns, trends, seasonality, outliers, periodicity, and statistical characteristics totransform it into a form that is suitable for model training. This process clarifies the unique patternsor characteristics of time series data, allowing models to learn them more effectively. This enhancesthe understanding of data, improves the predictive performance of models, and increases efficiencythrough data compression. The reasons why Time Series Feature Extraction is necessary are asfollows:

# • Understanding the characteristics of data

Time series data, which is ordered sequentially over time, possesses unique characteristics suchas various patterns, seasonality, and periodicity. However, these data are complex and high-dimensional, making these features not easily apparent. The continuous values in time series datatend to be similar to adjacent values, which makes it challenging to accurately distinguish the con-text and meaning of each data point, often resulting in a lack of semantic information. Therefore,the process of transforming time series data into an analyzable format and extracting key featuresis crucial for understanding the inherent behaviors and interactions within the data.

# • Explainability of data

Unlike traditional machine learning methods, deep learning models automatically learn usefulfeatures from data without requiring manual feature extraction. However, due to the nonlinearstructure of deep learning models, it is often difficult for humans to interpret the learned features.The extracted features can better explain the structural characteristics of the data, helping tointerpret and understand its meaning. This process provides critical insights for making data-drivendecisions and contributes to improving explainability in complex model architectures.

# • Enhancing Model Performance

By summarizing complex time series data and removing noise, it emphasizes the essential signalsof the data, reducing the computational burden on models and improving predictive performance.Focusing on critical information allows the model to avoid overfitting to the training data anddevelop strong generalization capabilities.

Decomposing or transforming time series data into different forms before feeding it into themodel is a widely used and researched approach for feature extraction. These approaches help themodel focus on learning the critical information in the data, thereby reducing sensitivity to noiseand overfitting. These methods are not limited to specific architectures and can be applied acrossvarious models. Fig. 17 presents the key techniques for time series feature extraction, followed by anexplanation of related application models for these approaches.

![](images/e48a446efe8bc652bd72d59f5b72a2fc8a2159e591e6b9dfe2c869cc0f0a7488.jpg)



Fig. 17: Key Techniques in Time Series Feature Extraction


# 5.4.1 Decomposition

Time series decomposition has long been used as a fundamental time series feature extractiontechnique, which involves separating time series data into components such as trend, seasonality,periodicity, and residual. The advantage of decomposition is that it simplifies complex time seriesdata by breaking it down into understandable, independent components, enabling more accuratemodel predictions and easier analysis and interpretation.

# Moving Average Kernel

Many models apply a Moving Average kernel to the input sequence to separate trend and seasonalitycomponents. In this process, high-frequency noise or short-term fluctuations are removed, allowinglong-term trends to be more clearly captured and overall patterns of increase and decrease to beemphasized. The method of using a Moving Average Kernel is widely adopted due to its compu-tational simplicity and efficiency. However, it has limitations when dealing with complex nonlinearpatterns.

Autoformer (Wu et al, 2021) goes beyond using decomposition techniques merely as prepro-cessing for forecasting tasks by progressively decomposing time series data throughout the predictionprocess within the model itself. Similarly, CrossWaveNet (Huang et al, 2024b) employs a dual-channel network to perform gradual deep cross-decomposition, enabling it to capture complextemporal patterns effectively. Likewise, models such as FEDformer (Zhou et al, 2022), LTSF-Linear (Zeng et al, 2023), and PDMLP (Tang and Zhang, 2024) utilize a moving average kernelto individually model each component of the time series data, which are then recombined to makeeffective predictions. However, the Moving Average method lacks robustness because it is not learn-able by the model. Additionally, since it assigns equal weights to each data point within the slidingwindow, it has limited ability to identify specific patterns. To address these limitations, Leddam(Yu et al, 2024) use learnable 1D convolutional kernels, which can better capture nonlinear struc-tures and dynamic variations. Meanwhile, diffusion-based models face challenges as components liketrend and seasonality can easily collapse during the diffusion process. Diffusion-TS (Yuan and Qiao,2024) overcomes this by applying the diffusion process after decomposition, thereby preserving the

characteristics of each component more effectively. Through this approach, the traditional limita-tions of time series decomposition are overcome, allowing for a more effective separation of trend andseasonal components.

# Downsampling

The technique of using downsampling for decomposition is also frequently employed. Originatingfrom signal processing, downsampling involves reducing the number of samples from the originalsignal, effectively decreasing the data by a specific ratio. Time series forecasting is typically achievedthrough pooling methods, where representative values are extracted from specific segments of thetime series data, thereby suppressing high-frequency components and emphasizing low-frequencycomponents. By adjusting the pooling size, various downsampling levels can be achieved, allowingfor the exploration of multiple patterns within the data.

SparseTSF (Lin et al, 2024b) and SutraNets (Bergsma et al, 2023) use downsampling toseparate data into trend and periodic components, predicting each subseries independently. Thisallows the model to learn each component separately and better understand the influence of eachon the complex time series data, enhancing prediction accuracy. By dividing the complex time seriesdata into various sub-series, it can be decomposed into a simpler form, thereby reducing the overallcomplexity of the model. Therefore, this approach enhances the model’s generalization ability andhelps prevent overfitting.

# Non-linear Methods

While Moving Average Kernel and Downsampling are linear methods, Empirical Mode Decom-position (EMD) (Huang et al, 1998) is a non-linear and non-stationary signal analysis techniquethat decomposes time series data into a number of Intrinsic Mode Functions (IMFs). EMD involvesfinding local extrema in the signal and using linear interpolation for the upper and lower envelopes,and by subtracting their average, it extracts high-frequency components as IMFs iteratively. Thefinal residual signal, also known as the residue, reveals the long-term trend. EMD is advantageous forprocessing non-linear and non-stationary time series data due to its adaptive nature, effectively sep-arating and analyzing complex patterns and changes in the time series. EMD-BI-LSTM (Mouniret al, 2023) proposes a method that combines EMD with a conventional Bi-LSTM model for fore-casting electric load time series data. By applying EMD, the model effectively captures complexnon-linear relationships, thereby enhancing forecasting performance.

However, EMD has a mode mixing problem, where signals of different frequencies are mixed withinthe same IMF (Xu et al, 2019). In the signal decomposition process of EMD, IMF is generated basedon local extrema, which causes difficulties in independently separating components for signals withsudden changes, such as intermittent signals. This leads to a failure in frequency separation, whichnot only reduces prediction performance but also impairs interpretability. Ensemble EmpiricalMode Decomposition (EEMD) (Wu and Huang, 2009) addresses this mode mixing problem byadding noise to the original signal and repeating the decomposition process, averaging the results.Through this process, signals with different scales are properly aligned into the correct IMFs. Throughsuccessive iterations, the influence of the noise is reduced, and the components of the original signalare emphasized, providing more physically meaningful results and enabling more accurate and stabletime series decomposition. Yang and Yang (2020) proposes models applying EEMD to LSTM, linearregression, and Bayesian ridge regression for forecasting.

Variational Mode Decomposition (VMD) (Dragomiretskiy and Zosso, 2013) is a methodthat uses variational optimization to separate signals into modes with specific frequency bands. Eachmode’s frequency center and bandwidth are determined through variational optimization. As thefrequency ranges do not overlap, the separation is clearer and more stable. However, compared toEMD, which requires relatively simple calculations, VMD is computationally more complex due to itsuse of variational optimization. Peng et al (2023) proposes a new model for crude oil price forecastingthat applies VMD. By decomposing the time series data with VMD, transforming it into images,and extracting features with CNN, this method improves forecasting performance for oil prices. Iteffectively utilizes the advantages of VMD by making predictions with Bidirectional Gated RecurrentUnit (BGRU).

Various decomposition methods are widely used in time series forecasting. However, decomposi-tion methods also have some drawbacks. When the data is divided into separate components based ondifferent rules, the model predicts each component independently. As a result, important interactionsor relationships may not be considered, which can lead to underutilization of necessary information

for prediction and degrade performance. Nevertheless, considering the characteristics of time series,such as periodicity, trends, and noise, decomposition remains a powerful technique. Moreover, sincetime series data is more difficult to intuitively interpret compared to other data types, decompositionprovides interpretability, which helps increase the model’s trustworthiness.

# 5.4.2 Multi-scale

The multi-scale approach analyzes data at various time scales, capturing patterns and trends atmultiple levels simultaneously. This method is beneficial not only for time series data but also infields like computer vision and natural language processing (Fan et al, 2021; Nawrot et al, 2021). Byintegrating information from longer time frames, the multi-scale approach plays a significant role inimproving performance in time series forecasting.

MTST (Zhang et al, 2024e), PDMLP (Tang and Zhang, 2024), and FTMixer (Li et al, 2024d)extend the patching application to a multi-scale framework, where shorter patches effectively learnhigh-frequency patterns and longer patches capture long-term trends. TimeMixer (Wang et al,2024a) and AMD (Hu et al, 2024b) leverage multi-scale decomposition to capitalize on the strengthsof each scale, enabling the model to make more accurate predictions. HD-TTS (Marisca et al,2024) uses spatiotemporal downsampling to decompose data into various scales across both time andspace. Scaleformer (Shabani et al, 2023) and Pathformer (Chen et al, 2024b) apply the multi-scale concept to the overall model architecture, while HD-TTS hierarchically implements temporaland spatial downsampling. MG-TSD (Fan et al, 2024b) and mr-Diff (Shen et al, 2024) successfullyintegrate multi-scale by incorporating inductive biases that prioritize generating coarse data in theearly stages of the diffusion reverse process.

# 5.4.3 Domain transformation

A common method for enhancing the expression of latent periodic features in time series data is totransform the data into the frequency domain. Techniques such as Fourier transformation, Wavelettransformation, and Cosine transformation are commonly used for this purpose. Generally, time seriesdata exhibits periodicity, which refers to patterns that repeat at regular intervals. Frequency trans-formations effectively explore this periodicity. In time series analysis, frequency transformations areused with two main approaches: extracting periodic components and direct learning in the frequencydomain.

# Periodicity Extraction

Periodicity in time series data serves as critical information for predictive models, where features thatmay be hidden in the time domain can be more easily uncovered in the frequency domain. Frequencytransformations are advantageous because they can remove high-frequency noise while retainingimportant low-frequency components, thereby improving data quality. By selectively extracting keyfrequency components and feeding them into the model, the process helps in learning essentialpatterns, enhances computational efficiency, and reduces model complexity. Some models leveragethese characteristics to identify and extract various periodic patterns in the data. These adaptivelyextracted patterns are used either as inputs for the model or integrated into the learning process,thereby enhancing the model’s predictive performance.

Autoformer (Wu et al, 2021) utilizes periodicity extracted through auto-correlation within theAttention mechanism. TimesNet (Wu et al, 2023) transforms periodic components into a 2D formatto train the time series with CNNs. MSGNet (Cai et al, 2024a) uses periodic components to allowthe model to determine appropriate scale levels for multi-scale analysis autonomously. However, sincethe periodicity extracted in this manner is based on selective sampling, it does not encompass allthe latent information, and if critical information is not selected, performance degradation becomesinevitable.

# Training in the Frequency Domain

The approach of directly training models in the frequency domain is widely studied as it overcomesthese limitations by evenly learning all latent frequency components, ensuring that critical infor-mation is not missed. While time series data can exhibit complex and varied patterns in the timedomain, these patterns can often be succinctly represented by a few dominant frequency compo-nents when transformed into the frequency domain (Zhou et al, 2022). This simplification makes the

learning process more straightforward, as most of the information can be captured with a minimalset of frequency components. This method allows for an easy transition to the frequency domainwhile preserving the original information, thanks to various transformation and inverse transforma-tion techniques. When time series data exhibit nonlinear characteristics, learning in the frequencydomain can better capture these complex patterns than in the time domain. The frequency domainapproach doesn’t require consideration of time-axis variations, enabling stable performance even withnon-stationary data. Furthermore, this method allows for efficient learning by compressing the datawithout losing important characteristics.

In the frequency domain, each frequency component is represented as a complex number, con-sisting of a real part and an imaginary part, each conveying different information. The real part isrelated to the magnitude (amplitude) of the time series data, indicating how prominent the periodiccomponents are. The imaginary part, on the other hand, relates to the phase information of the data,determining the temporal positioning of the frequency components. By simultaneously understand-ing the amplitude and phase information in the frequency domain, models can accurately captureand predict complex periodic patterns in the data. The research initially focused on simply chang-ing the domain, but more recently, it has been moving towards expanding the expressiveness of thefrequency domain.

FreTS (Yi et al, 2024) and Crabb´e et al (2024) transform data into the frequency domain beforefeeding it into the model and then convert the predicted values back into the time domain. Simplyconverting data to the frequency domain has limitations in achieving good model performance, lead-ing to the development of models that overcome these challenges. Research continues to expand therepresentation in the frequency domain to fully utilize its rich features. FEDformer (Zhou et al,2022) applies season-trend decomposition and then performs attention in the frequency domain, facil-itating independent modeling of key information components. Fredformer (Piao et al, 2024) notonly transforms data into the frequency domain but also investigates frequency bias through exper-iments, addressing issues in the frequency domain using frequency normalization techniques. FITS(Xu et al, 2024c) utilizes distinct complex-valued linear layers in the frequency domain to learn ampli-tude scaling and phase changes, thereby enhancing the frequency representation of input time seriesthrough interpolation learning. DERITS (Fan et al, 2024a) effectively handles the non-stationarityof time series data by differentiating frequency components and representing them in a static formthat is easier to predict. SiMBA (Patro and Agneeswaran, 2024b) transforms time series data intothe frequency domain using Fourier transforms, learning real and imaginary components separately,thus allowing more precise analysis of data in the frequency domain and improving model predic-tion performance. While frequency bands capture global dependencies well, they often struggle withlocal dependency capture. To overcome this, WaveForM (Yang et al, 2023a) uses discrete wavelettransforms (DWT) to decompose time series data into various frequency bands while preserving thetime information of each band. This approach captures frequency changes within specific time inter-vals, simultaneously capturing features from both time and frequency domains. FTMixer (Li et al,2024d) proposes a method that directly utilizes data from both domains to leverage the strengths ofglobal dependency extraction in the frequency domain and local dependency extraction in the timedomain.

# 5.4.4 Additional Approach

In addition to the methods previously described, various other approaches have been explored. Anotable example is the use of High-Dimensional Embedding, which generates high-dimensional rep-resentations to better capture the essential information of the data. This approach extracts complex,multidimensional information from the original time series data and integrates it to enhance themodel’s predictive capabilities. For instance, CATS (Lu et al, 2024) creates a new Auxiliary TimeSeries (ATS) by combining variables from the input data and then utilizes these for prediction.SOFTS (Han et al, 2024) focuses on extracting common features (Core) from the variables andincorporating them into the learning process. Furthermore, BSA (Kang et al, 2024) enhances featurerepresentation by leveraging temporal correlations between continuously streaming input samples,rather than depending solely on a single current input. The method accumulates multiple scales offeature transformations over time via an exponential moving average (EMA), then applies Spec-tral Attention to these multi-scale momentums to capture long-range dependencies and enrich theresulting features.

Efforts to effectively represent the complex features of time series data for better model learningare being explored in various forms. These efforts are essential research topics for improving per-formance in the challenging field of time series forecasting. Additionally, integrating these featureextraction techniques appropriately within the basic structure of existing deep learning models cancreate synergy, enhancing the model’s predictive accuracy.

# 5.4.5 Automated Feature Engineering and Self-Supervised Learning

In TSF, extracting meaningful features has traditionally required domain expertise and extensivemanual work. However, such approaches are often time-consuming and costly, and they may misscritical patterns when dealing with complex data. To address these challenges, automated featureengineering has become increasingly important for automatically identifying key patterns in the data.One of the core techniques for automated feature engineering is self-supervised learning (Zhang et al,2024a).

Self-supervised learning is a pretraining approach that enables models to automatically learnimportant features from unlabeled datasets by identifying patterns directly from the data itself.This allows the model to capture structural patterns without relying on manual feature extraction,reducing the dependency on labeled data. Key approaches in self-supervised learning for TSF are asfollows. Transformation-based approach learns key features by comparing patterns before andafter data transformations, such as time shifts and scaling. For example, TS-TCC (Eldele et al,2021) enhances features by applying various transformations and learning the pattern similaritiesbetween the transformed data. Time Correlation Learning extracts temporal dependencies intime series data by learning the relationships between past and future time points. ContrastivePredictive Coding (CPC) (Oord et al, 2018) divides the data into multiple time segments andlearns the correlations between these segments. Mask-based Learning involves masking specifictime points in the time series data and predicting these masked points based on the remaining datato learn key patterns (Wang et al, 2022a). The application of such self-supervised learning approachescan enhance the performance of TSF (Xu et al, 2022; Wu et al, 2023).

Self-supervised learning enables models to automatically recognize and extract key patterns andnonlinear relationships from data without labeled examples. It can effectively learn from smalldatasets, addressing the issue of label scarcity and contributing to model lightweight and algo-rithm optimization. In particular, multimodal self-supervised learning techniques, which integratetime series data with other data types such as images and text, are expected to advance, enablingthe model to learn richer patterns. Such multimodal learning can significantly contribute to solvingcomplex decision-making problems, such as healthcare diagnostics and disaster prediction. Addition-ally, research on converting the learned features into interpretable forms will become increasinglyimportant. This will enhance the interpretability and reliability of the model and improve trust indata-driven decision-making processes.

In conclusion, self-supervised learning-based automated feature engineering is a pivotal researcharea for effectively understanding complex patterns in time series data and enhancing predictive per-formance. Research combining this approach with data efficiency improvements, multimodal learning,and explainable AI (XAI) is expected to further increase the practicality and reliability of TSF.

# 5.5 Model Combination Techniques

The instability of single models in time series forecasting has been a persistent challenge from thepast to the present. Due to the complexity, volatility, and distinguishing characteristics of time seriesdata, the use of a single model can lead to overfitting and unstable forecasting performance. There-fore, methods for combining models offer a simple yet effective approach to overcome these issues.Model combination techniques reduce forecasting uncertainty and enable more robust predictions.Makridakis et al (2020) experimentally demonstrates that model combination, as applied in the M4Competition task, improves forecast accuracy over single models or model selection. However, Modelcombination techniques can face issues such as model redundancy, increased computational costs,and reduced interpretability due to the complexity of the combined models. To maximize the com-bination effect, it is necessary to properly leverage model diversity or use sophisticated combinationmethods.

# 5.5.1 Ensemble Models

Ensemble models combine the independent predictions of several models to complement the weak-nesses of individual models and generate a final forecast. Ganaie et al (2022) summarizes thetheoretical aspects of ensemble learning’s success based on existing research, arguing that combin-ing predictions from multiple models is an effective way to improve model performance. This sectionexplores key ensemble methods, including bagging, boosting, and stacking, as illustrated in Fig. 18.

Bootstrap Aggregating (Bagging) involves randomly sampling the data to train several inde-pendent models and then combining their results through averaging or voting (Breiman, 1996).Petropoulos et al (2018) explains why bagging is effective for time series forecasting by addressingdata uncertainty, model uncertainty, and parameter uncertainty, improving forecasting performance.Kim and Baek (2022) combines Wavelet transforms with bagging techniques in MLP to improveunivariate time series forecasting performance. Using the Maximum Overlap Discrete Wavelet Trans-form (MODWT) (Nason et al, 2000), the signal is decomposed, and bagging is applied only to thedetail part, excluding trends. This creates data diversity by preserving the main trend while alteringthe detailed fluctuations.

Boosting overcomes the issue of model diversity by sequential training and combining weaklearners (Freund and Schapire, 1997). This approach addresses the diversity problem that can arisein bagging when each model is trained in a very similar manner. Each model places more weight onthe data that was incorrectly predicted by the previous models, aiming to reduce bias. This creates astrong learner by combining weak learners and ensures added diversity. Gradient boosting is an algo-rithm that optimizes models using the gradient of the loss function within the boosting framework(Friedman, 2001). XGBoost (eXtreme Gradient Boosting) executes the gradient boosting algorithmin parallel, making it faster and more effective in handling sparse data while improving learning speedand predictive performance (Chen and Guestrin, 2016). In the 2015 Kaggle (https://www.kaggle.com)machine learning competition, 17 out of 29 winning solutions used XGBoost (Chen and Guestrin,2016). As deep learning advanced, various works combining DL with XGBoost emerged. LightGBM(Ke et al, 2017) is a boosting-based algorithm with a leaf-wise tree splitting method, and Sui et al(2024) uses an ensemble approach that combines traditional and advanced methods for stock predic-tion. Specifically, the integration of LSTM, GRU, LR, and LightGBM models enhances forecastingperformance. However, boosting is generally not applied to deep learning models, as boosting typi-cally uses simple weak learners that are repeatedly trained to improve performance. Deep learningmodels like MLP already learn complex patterns effectively with large amounts of data, so the needfor iterative error correction is minimal. Thus, simpler machine learning methods are typically usedto construct weak learners instead of deep neural networks. Truchan et al (2024) proposes predictingtrends using linear models and then applying boosting to the residuals with tree-based models forforecasting. This model combines the simple trend estimation capability of linear models with thenonlinear pattern learning ability of tree-based models, achieving more precise and effective predic-tions in LTSF. However, there is also a case where boosting principles are applied to deep learningmodels, such as Liang et al (2024b), which integrates boosting’s progressive residual learning approachinto deep learning to enhance time series forecasting.

Stacking combines the predictions of several models to generate a final forecast (Wolpert, 1992).Stacked Generalization involves training several base learners, and then feeding their predictionresults into a meta-model to generate the final prediction. Unlike bagging and boosting, stacking doesnot directly combine the predictions of the models; instead, it inputs the prediction results into anew model to make the final forecast. Stacking is distinctive in that it incorporates model weightingsand analysis, enabling more refined and comprehensive predictions (Sharma et al, 2021). Massaoudiet al (2021) suggests stacking LightGBM and eXtreme Gradient Boosting (XGB) to tackle stochas-tic variations in short-term load forecasting, where the meta data generated by both models is inputto an MLP for the final prediction. Furthermore, the use of MLP as a meta-model instead of tradi-tional ML models like linear regression enables the learning of more complex, nonlinear relationships,improving prediction performance. The results demonstrate the model’s generalization ability andsuperior performance across various datasets. Stacking can be applied not only to deterministic fore-casting but also to probabilistic forecasting, and some studies have shown that stacking is effective inhandling uncertainty (Pavlyshenko, 2020; Dudek, 2024). Hasson et al (2023) explores the theoreticalaspects of stacked generalization. Through cross-validation, it provides theoretical guarantees thatthe performance of a stacked generalization model selected through cross-validation does not fallsignificantly behind that of an oracle model, the best performing individual model. The paper also

explains that extending the basic models used in stacking to learnable models, rather than constantmodels, provides better theoretical guarantees.

Ensemble models require significant training resources since each model must be trained indepen-dently. Nevertheless, ensemble techniques combine diverse, high-performance models to complementeach other and can be implemented in a relatively simple manner. Due to these characteristics, ensem-ble learning can enhance prediction stability and generalization capabilities in application fields,penetrating nearly all industries, from grain product distribution, semiconductor manufacturing, andsensor design to commercial software development and testing services (Lin and Huang, 2022).

![](images/9366d05785f34abc2dbead53048cff8e75e96cff513a8bc374976826211838fa.jpg)


![](images/6328484b720038d115e7249332b98e2bd109f7effa0ab69d4dfdad4c8ac03f2f.jpg)


![](images/9d3ad66e52addcb649509e7c6108f09c91550aa9683b868a43224f475187e5f8.jpg)



Fig. 18: Key Ensemble Techniques


# 5.5.2 Hybrid Models

Hybrid models combine different types of models or techniques to leverage the strengths of individualmodels while compensating for their weaknesses. These models employ a more sophisticated struc-tural integration approach compared to ensembles, encompassing various strategies such as combiningstatistical models, machine learning models, and deep learning models.

ESRNN (Smyl, 2020), a hybrid model that combines statistical models with deep learningarchitectures, integrates exponential smoothing (ES) and LSTM to leverage the interpretability ofstatistical methods alongside the powerful nonlinear learning capabilities of deep learning. ThroughES, the model effectively captures key components of individual time series, such as level, trend, andseasonality, while LSTM learns the correlations and nonlinear patterns across multiple time series.Additionally, the use of dynamic computation graphs and a hierarchical design provides strong per-formance and interpretability in real-world applications. This model highlights the potential of hybridmodels for time series forecasting with its superior performance. Moreover, AQ-ESRNN (Smyl et al,2024) model, an extension beyond deterministic forecasting to probabilistic forecasting, enables dis-tributional forecasting by quantifying prediction uncertainty. Subsequently, hybrid models combiningdeep learning architectures have advanced, and this section focuses on explaining such combinationsof deep learning architectures.

Particularly, time series data often involve not only temporal dependencies but also impor-tant interactions between variables. Therefore, many models explicitly learn the interdependencies

between variables in addition to temporal dependencies. However, incorporating all this informationinto a single architecture can lead to overfitting, which may limit the learning process. Recent mod-els often combine two or more architectures to efficiently learn by distinguishing the roles based onthe nature of the information.

WaveForM (Yang et al, 2023a) improves prediction performance by transforming time seriesdata into the wavelet domain and combining it with GNN. The Discrete Wavelet Transformation(DWT) uses High-Pass and Low-Pass Filters to decompose the time series data into small wavelets.This process removes noise and allows CNN to efficiently capture temporal and frequency charac-teristics. The GNN is then used to model the interactions between these components. TSLANet(Eldele et al, 2024) is a model that replaces the computationally expensive attention mechanism withCNN in the Transformer framework. It incorporates frequency analysis and a learnable thresholdto selectively attenuate high-frequency noise, effectively learning long-term and short-term interac-tions. Additionally, it uses parallel convolution layers with different kernel sizes to capture both localpatterns and long-term dependencies. This approach effectively compensates for the typical CNN’slimitations in modeling long-term dependencies. DERITS (Fan et al, 2024a) enhances the predictionperformance of non-stationary time series data by combining convolution operations in the frequencydomain with MLP in the time domain. The Frequency Derivative Transformation (FDT) convertstime series signals into the frequency domain and then differentiates the frequency components torepresent them in a static form that is easier to predict. The Order-adaptive Fourier ConvolutionNetwork (OFCN) is responsible for frequency filtering and learning dependencies in the frequencydomain. To improve prediction performance, it processes and fuses derivative information of variousorders in parallel, utilizing a Parallel-stacked Architecture. BiTGraph (Chen et al, 2024c) focuseson improving performance in situations with missing data by capturing both temporal dependen-cies and spatial structures. The Multi-Scale Instance Partial TCN module learns short-term andlong-term dependencies through kernels of various sizes and can also compensate for missing valuesthrough partial TCN. The Biased GCN module for inter-channel relationship learning represents therelationships between data points as a graph structure and adjusts the strength of information prop-agation between nodes, considering the missing patterns. Zhang et al (2021a) proposes a hybridmodel for wind speed forecasting. By performing decomposition using Singular Spectrum Analysis(SSA) and Multivariate Empirical Mode Decomposition (MEMD), the model removes noise and sep-arates the trend and various frequency components of the time series data, enabling more accurateforecasting. Then, by combining CNN with attention mechanism and Bidirectional Long Short-TermMemory, the model extracts spatial correlations and learns temporal dependencies, optimizing thewind speed forecasting performance. Zhang et al (2021b) also uses both ensemble and hybrid mod-els for TSF. The ensemble deep learning model, which combines various deep learning models suchas CNN, MLP, and LSTM, demonstrates excellent performance in forecasting real-world time seriesdata, including PM2.5 concentration, wind speed, and electricity prices. The model utilizes ExtendedAdaBoost (Freund and Schapire, 1997) to generate diverse base predictors, followed by applying astacking method to use Kernel Ridge Regression as a meta-predictor to produce the final forecast.

In hybrid models, optimization processes such as hyperparameter tuning, regularization tech-niques, and cross-validation can help reduce the risk of overfitting caused by the interdependence andcomplexity between multiple models. Additionally, evolutionary algorithms can be used to exploreand optimize model structures (Wei et al, 2022). Hybrid models may increase training time andresource consumption, and they can have limitations in interpretability and maintenance. Therefore,techniques like model compression or feature selection can be applied, and model interpretabilityissues can be addressed using interpretation methods.

# 5.6 Interpretability & Explainability

Although AI models exhibit outstanding performance, their black-box nature often undermines trustin decision-making processes (Xua and Yang, 2024). In mission-critical domains, particularly thosedealing with human life and safety, understanding the reasons behind model decisions is crucial,which has led to restrictions on the use of deep learning models (Papapetrou and Lee, 2024). TheEuropean Union, through the General Data Protection Regulation (GDPR), mandates the explana-tion of automated decisions, ensuring that individuals can understand the rationale behind decisionsmade by automated systems like AI models (Hamon et al, 2022). As data-driven decision-makingbecomes more widespread, the need to understand how a model arrives at specific predictions andwhy such outcomes occur is growing (Lim and Zohren, 2021).

Interpretability and explainability aim to ensure model understanding and transparency, makingit possible to know how and why a model produces certain results. These two concepts can be appliedin various ways. For example, consumers might demand explanations when they notice anomaliesin model outcomes, while AI researchers could use these concepts to improve model performance.Ultimately, interpretability and explainability promote not only research on AI models but also theirexpansion into various fields, enhancing performance and usability, which could drive fundamentalchanges in industries and societal paradigms (Han et al, 2023a; Jung et al, 2020; Kim et al, 2020).

These concepts are particularly important in time series data, where interpretability helps inunderstanding how a model recognizes and predicts periodic and repetitive patterns. Furthermore,explainability is necessary for anomaly detection models to clarify how anomalies are detected andwhich features are abnormal, thereby aiding decision-making.

The machine learning community has noted that there is no universally agreed-upon definitionfor the terms interpretability and explainability (Lipton, 2018). Consequently, in various studies,these terms are often used interchangeably, even though they carry different meanings and are useddifferently across domains (Graziani et al, 2023). To avoid confusion for our readers, we define andexplain these terms as follows. Interpretability focuses on understanding the internal workingsof a model, providing transparency about how the model makes predictions (S¸AHiN et al, 2024).Interpretability requires an intuitive understanding of the model’s structure, feature selection, andprediction process. Explainability, on the other hand, is concerned with providing reasons for amodel’s outputs in a way that humans can comprehend. It is a technical approach that helps makethe decision-making process of the model more understandable. In summary, interpretability is aboutunderstanding how the model works, while explainability is about providing understandable reasonsfor the model’s predictions.

# 5.6.1 Interpretability

Interpretability can be achieved through the following representative models. Temporal FusionTransformer (TFT) (Lim et al, 2021) is a deep learning model for multi-horizon forecasting pro-posed by the Google Cloud AI team, which utilizes the attention mechanism to effectively learn theinteractions between static and time-dependent input data. This model incorporates a static featureencoder, a variable selection gating mechanism, and interpretable weights, allowing the model tobetter capture the characteristics of time series data and significantly improve interpretability com-pared to a standard Transformer. The gating mechanism suppresses unnecessary inputs, enhancingmodel efficiency, while the static feature encoder converts static data into contextual vectors thatare incorporated into predictions. The following two models enhance the interpretability of the TFTmodel for wind speed prediction by applying a two-stage decomposition approach. Given the highvolatility and multi-resolution modes of wind speed data, the decomposition technique described inSection 5.4.1 allows for the independent analysis of the impact of each component on the predic-tion results, thus improving interpretability. EMD-EEMD-JADE-TFT decomposes the data usingEmpirical Mode Decomposition (EMD) and Ensemble Empirical Mode Decomposition (EEMD) andoptimizes the TFT’s hyperparameters using Adaptive Differential Evolution with optional externalarchive (JADE) (Wu and Wang, 2024). IEEMD-EWT-JADE-TFT model, on the other hand,uses Improved Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (IEEMD)and Empirical Wavelet Transform (EWT) for decomposition, and similarly optimizes TFT hyperpa-rameters with JADE (Wu et al, 2024a). Additionally, meteorological feature engineering combinesstatistical features to enhance both performance and interpretability.

# 5.6.2 Explainability

Different methodologies exist for models focused on explainability. Simple machine learning models,such as Linear Regression or Decision Trees, are transparent and provide an intuitive understandingof predictions by themselves. These models have intrinsic explainability, which falls under ante-hoc explainability, meaning their predictions can be easily explained from the design stage. On theother hand, deep learning models are highly complex and learn high-dimensional features, makingit difficult to achieve intrinsic explainability through simple aggregation methods. Therefore, post-hoc explainability techniques are primarily used for deep learning models. Post-hoc explainabilityrefers to approaches that interpret and explain the prediction or decision-making process after themodel has already been trained. Post-hoc interpretability can be divided into several subcategories.

Surrogate models are used to explain the predictions of complex models by training a simpler,interpretable model to explain the predictions. Local Interpretable Model-Agnostic Explanations(LIME) is a representative surrogate model that provides local explanations for individual predictions(Ribeiro et al, 2016). LIME works by generating perturbated sample data around a data point andobtaining predictions from the black-box model. Based on this, a simple linear model is trainedto identify the important features in that prediction. Yang et al (2023b) proposes an improvedLIME algorithm to address the interpretability issues of black-box models in wind power forecastingand applies it to a wind power prediction model. This model introduces a trust index to quantifyhow reliable the features used by the prediction model are and analyzes how the black-box modelhandles important features in the prediction. Additionally, this model proposes global trust modelingand interpretable feature selection methods, demonstrating that these approaches can enhance themodel’s reliability and reduce prediction errors.

Feature attribution is a method that calculates how much each input feature contributed to themodel’s prediction and evaluates the contribution of input features for a specific prediction. SHapleyAdditive exPlanations (SHAP) is a representative feature attribution technique based on the Shapleyvalue concept from game theory (Lundberg and Lee, 2017). SHAP fairly measures the contribution ofeach feature to model predictions. SHAP visualizes the importance of features from both global andlocal perspectives and helps identify interactions between model predictions and features. The keyidea behind SHAP is to compute the average contribution of each feature across all possible featurecombinations, which allows the decomposition of predictions into feature contributions, providingintuitive and interpretable results. Garc´ıa and Aznarte (2020) uses an LSTM model to predict NO $^ 2$concentrations and presents a method for interpreting the prediction results based on SHAP. WhileSHAP values are typically applied to linear models, this study extends SHAP by applying DeepSHAP to accommodate the nonlinear characteristics of deep learning models, making it easier tounderstand the complex decision-making process of deep learning models (Chen et al, 2022).

Counterfactual explanations describe what inputs need to be altered to change a specific pre-diction (Wachter et al, 2017). Counterfactual explanations generate counterfactual inputs that wouldalter the prediction. In time series research, counterfactual explanations have been widely used inclassification tasks, but their application to forecasting models has been relatively underexplored(Wang et al, 2023b). In time series forecasting, exogenous variables make predictions more complex,which highlights the importance of setting range constraints (Papapetrou and Lee, 2024). ForecastCFis a model that generates counterfactual explanations to explain the predictions of time series fore-casting models (Wang et al, 2023b). This model modifies the time series data through gradient-basedperturbation to ensure the model’s predictions fall within a specified range, thus improving themodel’s explainability. Specifically, users define the desired prediction range, and the model generatespredictions that satisfy these constraints, making the predictions more intuitive and understandable.

# 5.7 Spatio-temporal Time Series Forecasting

As the structure of time series data becomes increasingly complex, spatial information is now fre-quently incorporated into the datasets. Spatio-temporal time series data refers to datasets thatsimultaneously capture temporal variations and spatial distributions. This type of data consists ofvalues that change over time at specific locations, possessing both spatial interactions and temporaldependencies. Due to these characteristics, it provides critical information not only from patternsextracted along the time axis but also from spatial dependencies caused by interactions betweenlocations. For example, region-based traffic flow, air pollution levels, climate change, and electric-ity consumption are representative cases. The rapid advancements in technologies such as satellitedata and GPS have led to an explosive increase in location-based data. Therefore, spatio-temporaldata modeling has become increasingly important, as many real-world problems cannot be ade-quately explained by temporal variations alone (Sun et al, 2024a). Conventional multivariate timeseries models often fail to adequately address spatial correlations or network structures, necessitat-ing their extension to spatio-temporal models. To address this, numerous approaches utilizing GNNsand Transformers have been actively explored (Jin et al, 2023; Yang et al, 2024a; Chen et al, 2021a;Park et al, 2020).

Several representative approaches for handling spatio-temporal data are as follows. One is theGraph-Based Method for spatio-temporal modeling. Data is represented as a graph, where spatialrelationships between locations (nodes) are expressed as edges. GCNs or graph-based RNNs are used

to simultaneously learn spatial and temporal dependencies. This approach effectively models non-euclidean structured data but has drawbacks, such as complex preprocessing and high computationalcosts. These approaches are well-suited for applications such as traffic flow prediction by model-ing road networks or electricity demand forecasting by modeling connections between substations.DCRNN (Li et al, 2018) models traffic networks as directed graphs using Diffusion Convolution,effectively capturing spatial dependencies, including upstream and downstream node relationships.It also leverages GRU to capture temporal patterns and non-linearities, making it a representativegraph-based model for handling complex time series data. ST-GCN (Yan et al, 2018) models skele-ton dataset as graphs and extends GCN along the temporal axis to simultaneously model spatial andtemporal dependencies. DB-STGCN (Li et al, 2024a) predicts train delays by integrating dynamicadjacency matrices generated through Dynamic Bayesian Networks (DBN) into ST-GCN to cap-ture spatio-temporal dependencies. It improves prediction accuracy and interpretability through theintegration of attention mechanisms and the combination of GCN and GRU.

Another approach is the raster-based method. Data is converted into a 2D grid (raster) or videoformat to learn spatial locations and temporal patterns. Grid-based data representation: A specificspace is divided into grids, and data over time is collected for each grid cell. Video-based datarepresentation: Data is transformed into a 3D tensor, including the time axis. This format allows easyapplication of image processing techniques, such as CNNs, and 3D CNNs are particularly effective fordirectly learning spatio-temporal patterns. However, there are limitations in learning overly complexspatial relationships. They are suitable for modeling climate data from satellite imagery or recognizingactions in video data through spatiotemporal dynamics. ConvLSTM (Shi et al, 2015) replaces thefully connected state transitions of conventional LSTM with convolutions to learn spatial correlations,making it a representative early raster-based method. It represents all inputs, hidden states, andcell states as 3D tensors to maintain the grid-like spatial structure. ST-ResNet (Zhang et al, 2017)models citywide crowd flows based on residual learning. It partitions the city into a grid structureand represents the inflow and outflow of each grid cell as 2-channel image data. Residual networksare employed to effectively learn interactions between both nearby and distant regions. SLCNN(Zhang et al, 2020) utilizes Structure Learning Convolution (SLC) to define graphs as learnableparameters for the effective modeling of both global static structures and dynamic structures withindatasets. It replaces conventional CNN convolutions with SLC tailored to graph structures, generatinggraph-specific representations at each layer.

Recently, there have also been efforts to combine graph-based and raster-based methodologies tomore effectively model both spatial structures and temporal dependencies. They are ideal for repre-senting traffic flow as graphs and enabling detailed analysis of each node in smart city data. GMAN(Zheng et al, 2020) effectively learns the dynamic spatio-temporal relationships in traffic networksthrough Spatio-Temporal Embedding (STE) and ST-Attention Block. It utilizes spatial attention tocapture dynamic relationships between sensors, temporal attention to learn nonlinear temporal pat-terns, and gated fusion to integrate these features seamlessly. STSGCN (Song et al, 2020) employsthe Spatial-Temporal Synchronous Graph Convolutional Module (STSGCM) to synchronize andlearn spatial and temporal dependencies. It constructs a localized spatial-temporal graph to modelrelationships between neighboring nodes within the same time frame and nodes from past or futuretime frames simultaneously. GCN-SBULSTM (Chen et al, 2021b) proposes a model that com-bines GCN with Stacked Bidirectional Unidirectional LSTM (SBULSTM). The Bidirectional LSTMcaptures temporal patterns, while GCN learns spatial dependencies, enabling integrated modeling ofspatio-temporal interactions. Additionally, sequence-based methods process spatio-temporal data insequence form, focusing primarily on temporal dynamics.

The primary objective of spatio-temporal time series prediction models is to address the challeng-ing task of integrating complex data structures while simultaneously learning temporal dependenciesand spatial interactions. This enables accurate future state prediction and helps solve variousreal-world problems, facilitating precise decision-making.

# 6 Conclusion

This survey has been prepared to broaden the expertise of existing researchers and to assist beginnersin gaining a fundamental understanding amid the rapid growth of time series forecasting (TSF)research. By integrating key concepts of time series and the latest techniques, we aim to provideresearchers with clear direction and insights, fostering the continued advancement of this field. This

survey paper comprehensively reviews recent advancements in TSF, focusing on deep learning models.It also incorporates key papers from major AI and ML conferences, as well as several notable papersfrom the arXiv preprint repository (https://arxiv.org).

TSF is a highly regarded topic, gradually expanding from the prominence of Transformer modelsto various architecture-based models. Fundamental deep learning models like MLPs, CNNs, RNNs,and GNNs are being reassessed, and Transformers are also advancing by overcoming their previouslimitations. Additionally, innovative models like Mamba are emerging, and models such as Diffusionare also entering the field, experiencing rapid growth. Furthermore, as the demand for foundationmodels in the field of TSF intensifies, pre-trained models based on large language models (LLMs) areemerging. To create a powerful predictive model, researchers need to be aware of these trends andunderstand both the characteristics of time series data and the strengths and weaknesses of differentarchitectures.

Taking it a step further, we aim to address common challenges in TSF and provide deepinsights into future development directions. In particular, we reviewed various studies that focus onapproaches to channel correlation and address the challenges of distribution shift, which frequentlyoccur in real-world scenarios. Additionally, we emphasized the need for research that leverages causal-ity to eliminate spurious correlations, thereby enabling a deeper understanding of the underlyingessence. We also discussed the importance of studies focused on effectively extracting features fromcomplex time series data. In addition, we explored model combination techniques aimed at enhancingmodel performance, as well as spatio-temporal TSF problems to address complex real-world issues.Furthermore, we provided insights into improving model reliability by addressing interpretability andexplainability. Through this, we aim to present readers with potential directions for future research.

# Limitations and Future Work

This survey acknowledges several limitations that could be addressed in future research.

• We skipped the detailed theoretical backgrounds of the models. This survey aims to provide a com-prehensive overview, facilitating comparison and analysis, thereby allowing researchers to exploreareas of interest more effectively. Although comprehensive information is primarily provided,readers can access additional details through reference links if needed.

• We left the specific differences in characteristics across various time series datasets for future work.Time series data is often domain-specific, requiring expert knowledge, which can be integrated intomodels to enhance performance. This underscores the importance of interdisciplinary collaboration,and future research should continue to develop in this direction. By integrating insights from a widerange of academic fields into TSF, it can contribute to the development of robust and generalizedmodels.

• The aspect of expanding Artificial General Intelligence (AGI) and Adaptive Modeling could befurther explored. Research on techniques such as meta-learning, reinforcement learning, and neuro-symbolic AI can enhance the generalization and adaptability of models. In the current TSF field,time series-specific models and deep learning-based standard methodologies are dominant. How-ever, as TSF problems become increasingly complex, research on generalization and adaptabilityholds great potential.

Acknowledgements. We would like to express our heartfelt gratitude to Jisoo Mok, HyeongrokHan, Bonggyun Kang, Junyong Ahn, Jiin Kim, Youngwoo Kimh, Hyungyu Lee, Juhyeon Shin, Jai-hyun Lew, and Jieun Byeon from our research lab for their invaluable assistance in reviewing andproviding feedback on this paper. Their insights and suggestions have significantly enhanced thequality of our work.

# References



Abu-Mostafa YS, Atiya AF (1996) Introduction to financial forecasting. Applied intelligence 6:205–213





Achiam J, Adler S, Agarwal S, et al (2023) Gpt-4 technical report. arXiv preprint arXiv:230308774





Ahamed MA, Cheng Q (2024) TimeMachine: A Time Series is Worth 4 Mambas for Long-Term Forecasting, IOS Press. https://doi.org/10.3233/faia240677, URL http://dx.doi.org/10.3233/faia240677





Ahmed DM, Hassan MM, Mstafa RJ (2022) A review on deep sequential models for forecasting timeseries data. Applied Computational Intelligence and Soft Computing 2022(1):6596397





Alcaraz JL, Strodthoff N (2023) Diffusion-based time series imputation and forecasting with struc-tured state space models. Transactions on Machine Learning Research URL https://openreview.net/forum?id=hHiIbk7ApW





Alghamdi T, Elgazzar K, Bayoumi M, et al (2019) Forecasting traffic congestion using arimamodeling. In: 2019 15th international wireless communications & mobile computing conference(IWCMC), IEEE, pp 1227–1232





Ansari AF, Stella L, Turkmen AC, et al (2024) Chronos: Learning the language of time series. Trans-actions on Machine Learning Research URL https://openreview.net/forum?id=gerNCVqqtR,expert Certification





Athanasopoulos G, Hyndman RJ, Song H, et al (2011) The tourism forecasting competition.International Journal of Forecasting 27(3):822–844





Bai S, Kolter JZ, Koltun V (2018) An empirical evaluation of generic convolutional and recurrentnetworks for sequence modeling. arXiv preprint arXiv:180301271





Barlin JN, Zhou Q, Clair CMS, et al (2013) Classification and regression tree (cart) analysis ofendometrial carcinoma: seeing the forest for the trees. Gynecologic oncology 130(3):452–456





Behrouz A, Santacatterina M, Zabih R (2024) Chimera: Effectively modeling multivariate timeseries with 2-dimensional state space models. In: The Thirty-eighth Annual Conference on NeuralInformation Processing Systems, URL https://openreview.net/forum?id=ncYGjx2vnE





Beltagy I, Peters ME, Cohan A (2020) Longformer: The long-document transformer. arXiv:200405150





Benidis K, Rangapuram SS, Flunkert V, et al (2022) Deep learning for time series forecasting: Tutorialand literature survey. ACM Computing Surveys 55(6):1–36





Bergsma S, Zeyl T, Guo L (2023) Sutranets: sub-series autoregressive networks for long-sequence,probabilistic forecasting. Advances in Neural Information Processing Systems 36:30518–30533





Box GE, Jenkins GM, Reinsel G, et al (1970) Forecasting and control. Time Series Analysis 3(75):1970





Breiman L (1996) Bagging predictors. Machine learning 24:123–140





Brown RG (1959) Statistical forecasting for inventory control. (No Title)





Brown T, Mann B, Ryder N, et al (2020) Language models are few-shot learners. Advances in neuralinformation processing systems 33:1877–1901





Cai W, Liang Y, Liu X, et al (2024a) Msgnet: Learning multi-scale inter-series correlations for mul-tivariate time series forecasting. In: Proceedings of the AAAI Conference on Artificial Intelligence,pp 11141–11149





Cai W, Wang K, Wu H, et al (2024b) Forecastgrapher: Redefining multivariate time series forecastingwith graph neural networks. arXiv preprint arXiv:240518036





Cai X, Zhu Y, Wang X, et al (2024c) Mambats: Improved selective state space models for long-termtime series forecasting. arXiv preprint arXiv:240516440





Cao H, Tan C, Gao Z, et al (2024) A survey on generative diffusion models. IEEE Transactions onKnowledge and Data Engineering





Center JMK (2020) Dominick’s dataset





Challu C, Olivares KG, Oreshkin BN, et al (2023) Nhits: Neural hierarchical interpolation for timeseries forecasting. In: Proceedings of the AAAI conference on artificial intelligence, pp 6989–6997





Chang S, Zhang Y, Han W, et al (2017) Dilated recurrent neural networks. Advances in neuralinformation processing systems 30





Chen H, Lundberg SM, Lee SI (2022) Explaining a series of models by propagating shapley values.Nature communications 13(1):4512





Chen J, Lenssen JE, Feng A, et al (2024a) From similarity to superiority: Channel clustering fortime series forecasting. In: The Thirty-eighth Annual Conference on Neural Information ProcessingSystems, URL https://openreview.net/forum?id=MDgn9aazo0





Chen K, Chen G, Xu D, et al (2021a) Nast: Non-autoregressive spatial-temporal transformer for timeseries forecasting. arXiv preprint arXiv:210205624





Chen P, Fu X, Wang X (2021b) A graph convolutional stacked bidirectional unidirectional-lstm neuralnetwork for metro ridership prediction. IEEE Transactions on Intelligent Transportation Systems23(7):6950–6962





Chen P, ZHANG Y, Cheng Y, et al (2024b) Pathformer: Multi-scale transformers with adap-tive pathways for time series forecasting. In: The Twelfth International Conference on LearningRepresentations, URL https://openreview.net/forum?id=lJkOCMP2aW





Chen T, Guestrin C (2016) Xgboost: A scalable tree boosting system. In: Proceedings of the 22ndacm sigkdd international conference on knowledge discovery and data mining, pp 785–794





Chen X, Li X, Liu B, et al (2024c) Biased temporal convolution graph network for time series fore-casting with missing values. In: The Twelft International Conference on Learning Representations





Chen Y, Zou X, Li K, et al (2021c) Multiple local 3d cnns for region-based prediction in smart cities.Information Sciences 542:476–491





Chen Y, Ren K, Wang Y, et al (2024d) Contiformer: Continuous-time transformer for irregular timeseries modeling. Advances in Neural Information Processing Systems 36





Chen Z, Ma M, Li T, et al (2023) Long sequence time-series forecasting with deep learning: A survey.Information Fusion 97:101819





Cheng H, Wen Q, Liu Y, et al (2024a) Robusttsf: Towards theory and design of robust time seriesforecasting with anomalies. ICRL





Cheng M, Yang J, Pan T, et al (2024b) Convtimenet: A deep hierarchical fully convolutional modelfor multivariate time series analysis. arXiv preprint arXiv:240301493





Cheng X, Chen X, Li S, et al (2024c) Leveraging 2d information for long-term time series forecastingwith vanilla transformers. arXiv preprint arXiv:240513810





Chicco D, Warrens MJ, Jurman G (2021) The coefficient of determination r-squared is more infor-mative than smape, mae, mape, mse and rmse in regression analysis evaluation. Peerj computer





science 7:e623





Cho K, van Merri¨enboer B, Gulcehre C, et al (2014) Learning phrase representations using RNNencoder–decoder for statistical machine translation. In: Moschitti A, Pang B, Daelemans W(eds) Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process-ing (EMNLP). Association for Computational Linguistics, Doha, Qatar, pp 1724–1734, https://doi.org/10.3115/v1/D14-1179, URL https://aclanthology.org/D14-1179/





Clark K, Luong MT, Le QV, et al (2020) Electra: Pre-training text encoders as discriminatorsrather than generators. In: International Conference on Learning Representations, URL https://openreview.net/forum?id=r1xMH1BtvB





Cortes C (1995) Support-vector networks. Machine Learning





Coskunuzer B, Segovia-Dominguez I, Chen Y, et al (2024) Time-aware knowledge representations ofdynamic objects with multidimensional persistence. In: Proceedings of the AAAI Conference onArtificial Intelligence, pp 11678–11686





Crabb´e J, Huynh N, Stanczuk JP, et al (2024) Time series diffusion in the frequency domain. In:Forty-first International Conference on Machine Learning, URL https://openreview.net/forum?id=W9GaJUVLCT





Cryer JD (1986) Time series analysis, vol 286. Duxbury Press Boston





Dai Z, Yang Z, Yang Y, et al (2019) Transformer-XL: Attentive language models beyond a fixed-lengthcontext. In: Korhonen A, Traum D, M`arquez L (eds) Proceedings of the 57th Annual Meetingof the Association for Computational Linguistics. Association for Computational Linguistics, Flo-rence, Italy, pp 2978–2988, https://doi.org/10.18653/v1/P19-1285, URL https://aclanthology.org/P19-1285





Danese P, Kalchschmidt M (2011) The role of the forecasting process in improving forecast accu-racy and operational performance. International Journal of Production Economics 131(1):204–214.https://doi.org/https://doi.org/10.1016/j.ijpe.2010.09.006, URL https://www.sciencedirect.com/science/article/pii/S0925527310003282, innsbruck 2008





Das A, Kong W, Sen R, et al (2024) A decoder-only foundation model for time-series forecasting. In:Forty-first International Conference on Machine Learning, URL https://openreview.net/forum?id=jn2iTJas6h





Devlin J, Chang MW, Lee K, et al (2019) BERT: Pre-training of deep bidirectional transformersfor language understanding. In: Burstein J, Doran C, Solorio T (eds) Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long and Short Papers). Association for ComputationalLinguistics, Minneapolis, Minnesota, pp 4171–4186, https://doi.org/10.18653/v1/N19-1423, URLhttps://aclanthology.org/N19-1423





Dimri T, Ahmad S, Sharif M (2020) Time series analysis of climate variables using seasonal arimaapproach. Journal of Earth System Science 129:1–16





Dosovitskiy A, Beyer L, Kolesnikov A, et al (2021) An image is worth 16x16 words: Transformersfor image recognition at scale. In: International Conference on Learning Representations, URLhttps://openreview.net/forum?id=YicbFdNTTy





Dragomiretskiy K, Zosso D (2013) Variational mode decomposition. IEEE transactions on signalprocessing 62(3):531–544





Dubey A, Jauhri A, Pandey A, et al (2024) The llama 3 herd of models. arXiv preprintarXiv:240721783





Dudek G (2024) Stacking for probabilistic short-term load forecasting. In: International Conferenceon Computational Science, Springer, pp 3–18





Ekambaram V, Jati A, Nguyen N, et al (2023) Tsmixer: Lightweight mlp-mixer model for multivariatetime series forecasting. In: Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pp 459–469





Ekambaram V, Jati A, Dayama P, et al (2024) Tiny time mixers (ttms): Fast pre-trained models forenhanced zero/few-shot forecasting of multivariate time series. CoRR





Eldele E, Ragab M, Chen Z, et al (2021) Time-series representation learning via temporal andcontextual contrasting. Proceedings of the Thirtieth International Joint Conference on ArtificialIntelligence (IJCAI-21)





Eldele E, Ragab M, Chen Z, et al (2024) Tslanet: Rethinking transformers for time seriesrepresentation learning. International Conference on Machine Learning





Fan H, Xiong B, Mangalam K, et al (2021) Multiscale vision transformers. In: Proceedings of theIEEE/CVF international conference on computer vision, pp 6824–6835





Fan W, Wang P, Wang D, et al (2023) Dish-ts: a general paradigm for alleviating distribution shiftin time series forecasting. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp7522–7529





Fan W, Yi K, Ye H, et al (2024a) Deep frequency derivative learning for non-stationary time seriesforecasting. IJCAI





Fan X, Wu Y, Xu C, et al (2024b) MG-TSD: Multi-granularity time series diffusion models withguided learning process. In: The Twelfth International Conference on Learning Representations,URL https://openreview.net/forum?id=CZiY6OLktd





Feng R, Chen M, Song Y (2024a) Learning traffic as videos: Short-term traffic flow prediction usingmixed-pointwise convolution and channel attention mechanism. Expert Systems with Applications240:122468





Feng S, Miao C, Zhang Z, et al (2024b) Latent diffusion transformer for probabilistic time seriesforecasting. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp 11979–11987





Freund Y, Schapire RE (1997) A decision-theoretic generalization of on-line learning and anapplication to boosting. Journal of computer and system sciences 55(1):119–139





Friedman JH (2001) Greedy function approximation: a gradient boosting machine. Annals of statisticspp 1189–1232





Fu DY, Dao T, Saab KK, et al (2023) Hungry hungry hippos: Towards language modeling with statespace models. In The International Conference on Learning Representations (ICLR)





Fukushima K (1980) Neocognitron: A self-organizing neural network model for a mechanism ofpattern recognition unaffected by shift in position. Biological cybernetics 36(4):193–202





Gallifant J, Fiske A, Levites Strekalova YA, et al (2024) Peer review of gpt-4 technical report andsystems card. PLOS Digital Health 3(1):e0000417





Ganaie MA, Hu M, Malik AK, et al (2022) Ensemble deep learning: A review. EngineeringApplications of Artificial Intelligence 115:105151





Garc´ıa MV, Aznarte JL (2020) Shapley additive explanations for no2 forecasting. EcologicalInformatics 56:101039





Godahewa R, Bergmeir C, Webb G, et al (2021a) Solar power dataset (4 seconds observations).https://doi.org/10.5281/zenodo.4656027, URL https://doi.org/10.5281/zenodo.4656027





Godahewa R, Bergmeir C, Webb G, et al (2021b) Wind farms dataset (with missing values). https://doi.org/10.5281/zenodo.4654909, URL https://doi.org/10.5281/zenodo.4654909





Godahewa R, Bergmeir C, Webb G, et al (2021c) Wind farms dataset (without missing values).https://doi.org/10.5281/zenodo.4654858, URL https://doi.org/10.5281/zenodo.4654858





Godahewa R, Bergmeir C, Webb G, et al (2021d) Wind power dataset (4 seconds observations).https://doi.org/10.5281/zenodo.4656032, URL https://doi.org/10.5281/zenodo.4656032





Godahewa R, Bergmeir C, Webb G, et al (2021e) Australian electricity demand dataset. https://doi.org/10.5281/zenodo.4659727, URL https://doi.org/10.5281/zenodo.4659727





Godahewa R, Bergmeir C, Webb G, et al (2021f) Bitcoin dataset with missing values. https://doi.org/10.5281/zenodo.5121965, URL https://doi.org/10.5281/zenodo.5121965





Godahewa R, Bergmeir C, Webb G, et al (2021g) Bitcoin dataset without missing values. https://doi.org/10.5281/zenodo.5122101, URL https://doi.org/10.5281/zenodo.5122101





Godahewa R, Bergmeir C, Webb G, et al (2021h) Rideshare dataset with missing values. https://doi.org/10.5281/zenodo.5122114, URL https://doi.org/10.5281/zenodo.5122114





Godahewa R, Bergmeir C, Webb G, et al (2021i) Rideshare dataset without missing values. https://doi.org/10.5281/zenodo.5122232, URL https://doi.org/10.5281/zenodo.5122232





Godahewa R, Bergmeir C, Webb G, et al (2021j) Temperature rain dataset with missing values.https://doi.org/10.5281/zenodo.5129073, URL https://doi.org/10.5281/zenodo.5129073





Godahewa R, Bergmeir C, Webb G, et al (2021k) Temperature rain dataset without missing values.https://doi.org/10.5281/zenodo.5129091, URL https://doi.org/10.5281/zenodo.5129091





Godahewa RW, Bergmeir C, Webb GI, et al (2021l) Monash time series forecasting archive. In:Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track(Round 2), URL https://openreview.net/forum?id=wEc1mgAjU-





Gong Z, Tang Y, Liang J (2023) Patchmixer: A patch-mixing architecture for long-term time seriesforecasting. arXiv preprint arXiv:231000655





Granger CW (1969) Investigating causal relations by econometric models and cross-spectral methods.Econometrica: journal of the Econometric Society pp 424–438





Graziani M, Dutkiewicz L, Calvaresi D, et al (2023) A global taxonomy of interpretable ai: unifyingthe terminology for the technical and social sciences. Artificial intelligence review 56(4):3473–3504





Gruver N, Finzi MA, Qiu S, et al (2023) Large language models are zero-shot time series fore-casters. In: Thirty-seventh Conference on Neural Information Processing Systems, URL https://openreview.net/forum?id=md68e8iZK1





Gu A, Dao T (2024) Mamba: Linear-time sequence modeling with selective state spaces. In: FirstConference on Language Modeling, URL https://openreview.net/forum?id=tEYskw1VY2





Gu A, Johnson I, Goel K, et al (2021) Combining recurrent, convolutional, and continuous-timemodels with linear state space layers. Advances in neural information processing systems 34:572–585





Gu A, Goel K, Re C (2022) Efficiently modeling long sequences with structured state spaces. In:International Conference on Learning Representations, URL https://openreview.net/forum?id=uYLFoz1vlAC





Hahn Y, Langer T, Meyes R, et al (2023) Time series dataset survey for forecasting with deeplearning. Forecasting 5(1):315–335





Hamon R, Junklewitz H, Sanchez I, et al (2022) Bridging the gap between ai and explainability inthe gdpr: towards trustworthiness-by-design in automated decision-making. IEEE ComputationalIntelligence Magazine 17(1):72–85





Han H, Kim S, Choi HS, et al (2023a) On the impact of knowledge distillation for model inter-pretability. In: Proceedings of the 40th International Conference on Machine Learning. JMLR.org,ICML’23





Han H, Park S, Min S, et al (2023b) Improving generalization performance of electrocardiogramclassification models. Physiological Measurement 44(5):054003





Han L, Chen XY, Ye HJ, et al (2024) SOFTS: Efficient multivariate time series forecasting withseries-core fusion. In: The Thirty-eighth Annual Conference on Neural Information ProcessingSystems, URL https://openreview.net/forum?id=89AUi5L1uA





Hasson H, Maddix DC, Wang B, et al (2023) Theoretical guarantees of learning ensembling strategieswith applications to time series forecasting. In: International Conference on Machine Learning,PMLR, pp 12616–12632





Ho J, Jain A, Abbeel P (2020) Denoising diffusion probabilistic models. Advances in neuralinformation processing systems 33:6840–6851





Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural computation 9(8):1735–1780





Holt CC (1957) Forecasting trends and seasonals by exponentially weighted averages. carnegieinstitute of technology. Pittsburgh ONR memorandum





Hopfield JJ (1982) Neural networks and physical systems with emergent collective computationalabilities. Proceedings of the national academy of sciences 79(8):2554–2558





Hou H, Yu FR (2024) Rwkv-ts: Beyond traditional recurrent neural network for time series tasks.arXiv preprint arXiv:240109093





Hounie I, Porras-Valenzuela J, Ribeiro A (2024) Transformers with loss shaping constraints forlong-term time series forecasting. In: Forty-first International Conference on Machine Learning





Hu J, Lan D, Zhou Z, et al (2024a) Time-ssm: Simplifying and unifying state space models for timeseries forecasting. CoRR abs/2405.16312. URL https://doi.org/10.48550/arXiv.2405.16312





Hu Y, Liu P, Zhu P, et al (2024b) Adaptive multi-scale decomposition framework for time seriesforecasting. CoRR abs/2406.03751. URL https://doi.org/10.48550/arXiv.2406.03751





Huang NE, Shen Z, Long SR, et al (1998) The empirical mode decomposition and the hilbert spectrumfor nonlinear and non-stationary time series analysis. Proceedings of the Royal Society of LondonSeries A: mathematical, physical and engineering sciences 454(1971):903–995





Huang Q, Shen L, Zhang R, et al (2024a) Hdmixer: Hierarchical dependency with extendable patchfor multivariate time series forecasting. In: Proceedings of the AAAI Conference on ArtificialIntelligence, pp 12608–12616





Huang S, Liu Y, Zhang F, et al (2024b) Crosswavenet: A dual-channel network with deep cross-decomposition for long-term time series forecasting. Expert Systems with Applications 238:121642





Huber PJ (1992) Robust estimation of a location parameter. In: Breakthroughs in statistics:Methodology and distribution. Springer, p 492–518





Ilbert R, Odonnat A, Feofanov V, et al (2024) SAMformer: Unlocking the potential of transform-ers in time series forecasting with sharpness-aware minimization and channel-wise attention. In:Salakhutdinov R, Kolter Z, Heller K, et al (eds) Proceedings of the 41st International Conferenceon Machine Learning, Proceedings of Machine Learning Research, vol 235. PMLR, pp 20924–20954,URL https://proceedings.mlr.press/v235/ilbert24a.html





Ioffe S, Szegedy C (2015) Batch normalization: Accelerating deep network training by reducinginternal covariate shift. In: International conference on machine learning, pmlr, pp 448–456





Jhin SY, Kim S, Park N (2024) Addressing prediction delays in time series forecasting: A continuousgru approach with derivative regularization. In: Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining, pp 1234–1245





Jia Y, Lin Y, Hao X, et al (2024) Witran: Water-wave information transmission and recurrent accel-eration network for long-range time series forecasting. Advances in Neural Information ProcessingSystems 36





Jin M, Wen Q, Liang Y, et al (2023) Large models for time series and spatio-temporal data: A surveyand outlook. arXiv preprint arXiv:231010196





Jin M, Wang S, Ma L, et al (2024) Time-LLM: Time series forecasting by reprogramming largelanguage models. In: International Conference on Learning Representations (ICLR)





Jung D, Lee J, Yi J, et al (2020) icaps: An interpretable classifier via disentangled capsule networks.In: European Conference on Computer Vision, Springer, pp 314–330





Kalman RE (1960) A new approach to linear filtering and prediction problems. Transactions of theASME–Journal of Basic Engineering 82(1):35–45





Kang BG, Lee D, Kim H, et al (2024) Introducing spectral attention for long-range dependency intime series forecasting. In: Advances in Neural Information Processing Systems





Ke G, Meng Q, Finley T, et al (2017) Lightgbm: A highly efficient gradient boosting decision tree.Advances in neural information processing systems 30





Khosravi A, Mazloumi E, Nahavandi S, et al (2011) Prediction intervals to account for uncertaintiesin travel time prediction. IEEE Transactions on Intelligent Transportation Systems 12(2):537–547





Kim D, Baek JG (2022) Bagging ensemble-based novel data generation method for univariate timeseries forecasting. Expert Systems with Applications 203:117366





Kim D, Park J, Lee J, et al (2024) Are self-attentions effective for time series forecasting? In:The Thirty-eighth Annual Conference on Neural Information Processing Systems, URL https://openreview.net/forum?id=iN43sJoib7





Kim S, Yi J, Kim E, et al (2020) Interpretation of NLP models through input marginalization. In:Webber B, Cohn T, He Y, et al (eds) Proceedings of the 2020 Conference on Empirical Methodsin Natural Language Processing (EMNLP). Association for Computational Linguistics, Online,pp 3154–3167, https://doi.org/10.18653/v1/2020.emnlp-main.255, URL https://aclanthology.org/2020.emnlp-main.255/





Kim T, Kim J, Tae Y, et al (2021) Reversible instance normalization for accurate time-seriesforecasting against distribution shift. In: International Conference on Learning Representations





Kipf TN, Welling M (2017) Semi-supervised classification with graph convolutional networks. In:International Conference on Learning Representations, URL https://openreview.net/forum?id=SJU4ayYgl





Kitaev N, Kaiser L, Levskaya A (2020) Reformer: The efficient transformer. In: InternationalConference on Learning Representations, URL https://openreview.net/forum?id=rkgNKkHtvB





Koenker R, Bassett Jr G (1978) Regression quantiles. Econometrica: journal of the EconometricSociety pp 33–50





Kollovieh M, Ansari AF, Bohlke-Schneider M, et al (2024) Predict, refine, synthesize: Self-guiding dif-fusion models for probabilistic time series forecasting. Advances in Neural Information ProcessingSystems 36





Kong Z, Ping W, Huang J, et al (2021) Diffwave: A versatile diffusion model for audio synthesis. In:International Conference on Learning Representations, URL https://openreview.net/forum?id=a-xFK8Ymz5J





Kontopoulou VI, Panagopoulos AD, Kakkos I, et al (2023) A review of arima vs. machine learningapproaches for time series forecasting in data driven networks. Future Internet 15(8):255





LeCun Y, Bottou L, Bengio Y, et al (1998) Gradient-based learning applied to document recognition.Proceedings of the IEEE 86(11):2278–2324





Li J, Li D, Savarese S, et al (2023) Blip-2: Bootstrapping language-image pre-training with frozenimage encoders and large language models. In: International conference on machine learning,PMLR, pp 19730–19742





Li J, Xu X, Ding X, et al (2024a) Bayesian spatio-temporal graph convolutional network for railwaytrain delay prediction. IEEE Transactions on Intelligent Transportation Systems





Li S, Jin X, Xuan Y, et al (2019) Enhancing the locality and breaking the memory bottleneck oftransformer on time series forecasting. Advances in neural information processing systems 32





Li Y, Yu R, Shahabi C, et al (2018) Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In: International Conference on Learning Representations, URL https://openreview.net/forum?id=SJiHXGWAZ





Li Y, Chen W, Hu X, et al (2024b) Transformer-modulated diffusion models for probabilis-tic multivariate time series forecasting. In: The Twelfth International Conference on LearningRepresentations, URL https://openreview.net/forum?id=qae04YACHs





Li Y, Xu J, Anastasiu D (2024c) Learning from polar representation: An extreme-adaptive model forlong-term time series forecasting. In: Proceedings of the AAAI Conference on Artificial Intelligence,pp 171–179





Li Z, Qin Y, Cheng X, et al (2024d) Ftmixer: Frequency and time domain representations fusion fortime series modeling. arXiv preprint arXiv:240515256





Liang A, Jiang X, Sun Y, et al (2024a) Bi-mamba4ts: Bidirectional mamba for time series forecasting.arXiv preprint arXiv:240415772





Liang D, Zhang H, Yuan D, et al (2024b) Minusformer: Improving time series forecasting byprogressively learning residuals. arXiv preprint arXiv:240202332





Liang Y, Wen H, Nie Y, et al (2024c) Foundation models for time series analysis: A tutorial andsurvey. arXiv preprint arXiv:240314735





Lim B, Zohren S (2021) Time-series forecasting with deep learning: a survey. PhilosophicalTransactions of the Royal Society A 379(2194):20200209





Lim B, Arık SO, Loeff N, et al (2021) Temporal fusion transformers for interpretable multi-horizon¨time series forecasting. International Journal of Forecasting 37(4):1748–1764





Lin K, Huang C (2022) Ensemble learning applications in multiple industries: A review. Inf DynAppl 1(1):44–58





Lin L, Li Z, Li R, et al (2024a) Diffusion models for time-series applications: a survey. Frontiers ofInformation Technology & Electronic Engineering 25(1):19–41





Lin S, Lin W, Wu W, et al (2024b) Sparsetsf: Modeling long-term time series forecasting with 1kparameters. International Conference on Machine Learning





Lin S, Lin W, Wu W, et al (2024c) PETformer: Long-term time series forecasting via placeholder-enhanced transformer. URL https://openreview.net/forum?id=u3RJbzzBZj





Lipton ZC (2018) The mythos of model interpretability: In machine learning, the concept ofinterpretability is both important and slippery. Queue 16(3):31–57





Liu H, Li C, Wu Q, et al (2024a) Visual instruction tuning. Advances in neural information processingsystems 36





Liu J, Liu C, Woo G, et al (2024b) Unitst: Effectively modeling inter-series and intra-seriesdependencies for multivariate time series forecasting. arXiv preprint arXiv:240604975





Liu S, Yu H, Liao C, et al (2021a) Pyraformer: Low-complexity pyramidal attention for long-rangetime series modeling and forecasting. In: International conference on learning representations





Liu Y, Wu H, Wang J, et al (2022) Non-stationary transformers: Exploring the stationarity in timeseries forecasting. Advances in Neural Information Processing Systems 35:9881–9893





Liu Y, Hu T, Zhang H, et al (2024c) itransformer: Inverted transformers are effective for timeseries forecasting. In: The Twelfth International Conference on Learning Representations, URLhttps://openreview.net/forum?id=JePfAI8fah





Liu Y, Li C, Wang J, et al (2024d) Koopa: Learning non-stationary time series dynamics withkoopman predictors. Advances in Neural Information Processing Systems 36





Liu Z, Zhu Z, Gao J, et al (2021b) Forecast methods for time series data: a survey. Ieee Access9:91896–91912





Liu Z, Cheng M, Li Z, et al (2024e) Adaptive normalization for non-stationary time series forecasting:A temporal slice perspective. Advances in Neural Information Processing Systems 36





Lu J, Han X, Sun Y, et al (2024) Cats: Enhancing multivariate time series forecasting by constructingauxiliary time series as exogenous variables. International Conference on Machine Learning





Lu Han DCZHan-Jia Ye (2024) The capacity and robustness trade-off: Revisiting the channel inde-pendent strategy for multivariate time series forecasting. IEEE Transactions on Knowledge andData Engineering pp 1–14





Lundberg SM, Lee SI (2017) A unified approach to interpreting model predictions. In: GuyonI, Luxburg UV, Bengio S, et al (eds) Advances in Neural Information Processing Systems,vol 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf





Luo D, Wang X (2024) Moderntcn: A modern pure convolution structure for general time seriesanalysis. In: The Twelfth International Conference on Learning Representations





Ma X, Li X, Fang L, et al (2024) U-mixer: An unet-mixer architecture with stationarity correctionfor time series forecasting. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp14255–14262





Ma Y, Guo Z, Ren Z, et al (2020) Streaming graph neural networks. In: Proceedings of the 43rdinternational ACM SIGIR conference on research and development in information retrieval, pp719–728





Makridakis S, Hibon M (2000) The m3-competition: results, conclusions and implications. Interna-tional journal of forecasting 16(4):451–476





Makridakis S, Andersen A, Carbone R, et al (1982) The accuracy of extrapolation (time series)methods: Results of a forecasting competition. Journal of forecasting 1(2):111–153





Makridakis S, Spiliotis E, Assimakopoulos V (2020) The m4 competition: 100,000 time series and 61forecasting methods. International Journal of Forecasting 36(1):54–74





Marisca I, Alippi C, Bianchi FM (2024) Graph-based forecasting with missing data throughspatiotemporal downsampling. International Conference on Machine Learning





Masini RP, Medeiros MC, Mendes EF (2023) Machine learning advances for time series forecasting.Journal of economic surveys 37(1):76–111





Massaoudi M, Refaat SS, Chihi I, et al (2021) A novel stacked generalization ensemble-based hybridlgbm-xgb-mlp model for short-term load forecasting. Energy 214:118874





Matheson JE, Winkler RL (1976) Scoring rules for continuous probability distributions. Managementscience 22(10):1087–1096





McCracken MW, Ng S (2016) Fred-md: A monthly database for macroeconomic research. Journal ofBusiness & Economic Statistics 34(4):574–589





McLeod A, Gweon H (2013) Optimal deseasonalization for monthly and daily geophysical time series.Journal of Environmental statistics 4(11):1–11





Meijer C, Chen LY (2024) The rise of diffusion models in time-series forecasting. arXiv preprintarXiv:240103006





Mounir N, Ouadi H, Jrhilifa I (2023) Short-term electric load forecasting using an emd-bi-lstmapproach for smart grid energy management system. Energy and Buildings 288:113022





Mu B, Jiang X, Yuan S, et al (2023) Nao seasonal forecast using a multivariate air–sea coupled deeplearning model combined with causal discovery. Atmosphere 14(5):792





Nason GP, Von Sachs R, Kroisandt G (2000) Wavelet processes and adaptive estimation of theevolutionary wavelet spectrum. Journal of the Royal Statistical Society: Series B (StatisticalMethodology) 62(2):271–292





Nawrot P, Tworkowski S, Tyrolski M, et al (2021) Hierarchical transformers are more efficientlanguage models. arXiv preprint arXiv:211013711





Ni Z, Yu H, Liu S, et al (2024) Basisformer: Attention-based time series forecasting with learnableand interpretable basis. Advances in Neural Information Processing Systems 36





Nie T, Mei Y, Qin G, et al (2024) Channel-aware low-rank adaptation in time series forecasting.Conference on Information and Knowledge Management





Nie Y, Nguyen NH, Sinthong P, et al (2023) A time series is worth 64 words: Long-term forecastingwith transformers. In: The Eleventh International Conference on Learning Representations, URLhttps://openreview.net/forum?id=Jbdc0vTOcol





Nti IK, Teimeh M, Nyarko-Boateng O, et al (2020) Electricity load forecasting: a systematic review.Journal of Electrical Systems and Information Technology 7:1–19





Oord Avd, Li Y, Vinyals O (2018) Representation learning with contrastive predictive coding. arXivpreprint arXiv:180703748





Oreshkin BN, Carpov D, Chapados N, et al (2020) N-beats: Neural basis expansion analysis forinterpretable time series forecasting. In: International Conference on Learning Representations,URL https://openreview.net/forum?id=r1ecqn4YwB





Pan SJ, Yang Q (2009) A survey on transfer learning. IEEE Transactions on knowledge and dataengineering 22(10):1345–1359





Papapetrou P, Lee Z (2024) Interpretable and explainable time series mining. In: 2024 IEEE 11thInternational Conference on Data Science and Advanced Analytics (DSAA), IEEE, pp 1–3





Park C, Lee C, Bahng H, et al (2020) St-grat: A novel spatio-temporal graph attention networksfor accurately forecasting dynamically changing road speed. In: Proceedings of the 29th ACMinternational conference on information & knowledge management, pp 1215–1224





Passalis N, Tefas A, Kanniainen J, et al (2019) Deep adaptive input normalization for time seriesforecasting. IEEE transactions on neural networks and learning systems 31(9):3760–3765





Patro BN, Agneeswaran VS (2024a) Mamba-360: Survey of state space models as transformeralternative for long sequence modelling: Methods, applications, and challenges. arXiv preprintarXiv:240416112





Patro BN, Agneeswaran VS (2024b) Simba: Simplified mamba-based architecture for vision andmultivariate time series. arXiv preprint arXiv:240315360





Patwardhan N, Marrone S, Sansone C (2023) Transformers in the real world: A survey on nlpapplications. Information 14(4):242





Pavlyshenko BM (2020) Using bayesian regression for stacking time series predictive models. In: 2020IEEE Third International Conference on Data Stream Mining & Processing (DSMP), pp 305–309,https://doi.org/10.1109/DSMP47368.2020.9204312





Pearl J (1995) Causal diagrams for empirical research. Biometrika 82(4):669–688





Pearl J (1998) Graphs, causality, and structural equation models. Sociological Methods & Research27(2):226–284





Pearl J, et al (2000) Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress19(2):3





Peng Zj, Zhang C, Tian Yx (2023) Crude oil price time series forecasting: a novel approach based onvariational mode decomposition, time-series imaging, and deep learning. IEEE Access





Petropoulos F, Hyndman RJ, Bergmeir C (2018) Exploring the sources of uncertainty: Why doesbagging for time series forecasting work? European Journal of Operational Research 268(2):545–554





Piao X, Chen Z, Murayama T, et al (2024) Fredformer: Frequency debiased transformer for timeseries forecasting. In: KDD, pp 2400–2410, URL https://doi.org/10.1145/3637528.3671928





Portet S (2020) A primer on model selection using the akaike information criterion. Infectious DiseaseModelling 5:111–128





Qi S, Wen L, Li Y, et al (2024) Enhancing multivariate time series forecasting with mutualinformation-driven cross-variable and temporal modeling. arXiv preprint arXiv:240300869





Qian J, Wang Q, Wu Y, et al (2023) Causality-based deep learning forecast of the kuroshio volumetransport in the east china sea. Earth and Space Science 10(2):e2022EA002722





Qin Y, Song D, Chen H, et al (2017) A dual-stage attention-based recurrent neural network for timeseries prediction. In: Proceedings of the Twenty-Sixth International Joint Conference on ArtificialIntelligence, IJCAI-17, pp 2627–2633, https://doi.org/10.24963/ijcai.2017/366, URL https://doi.org/10.24963/ijcai.2017/366





Quinlan JR (1986) Induction of decision trees. Machine learning 1:81–106





Radford A, Kim JW, Hallacy C, et al (2021) Learning transferable visual models from naturallanguage supervision. In: International conference on machine learning, PMLR, pp 8748–8763





Ramesh A, Dhariwal P, Nichol A, et al (2022) Hierarchical text-conditional image generation withclip latents. arXiv preprint arXiv:220406125 1(2):3





Rasul K, Seward C, Schuster I, et al (2021) Autoregressive denoising diffusion models for multivariateprobabilistic time series forecasting. In: International Conference on Machine Learning, PMLR,pp 8857–8868





Rasul K, Ashok A, Williams AR, et al (2024) Lag-llama: Towards foundation models for probabilistictime series forecasting. 2310.08278





Ribeiro MT, Singh S, Guestrin C (2016) ” why should i trust you?” explaining the predictions ofany classifier. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledgediscovery and data mining, pp 1135–1144





Rombach R, Blattmann A, Lorenz D, et al (2022) High-resolution image synthesis with latent dif-fusion models. In: Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp 10684–10695





Rosenbaum PR, Rubin DB (1983) The central role of the propensity score in observational studiesfor causal effects. Biometrika 70(1):41–55





Rossi E, Chamberlain B, Frasca F, et al (2020) Temporal graph networks for deep learning ondynamic graphs. In: ICML 2020 Workshop on Graph Representation Learning





Rumelhart DE, Hinton GE, Williams RJ (1986) Learning representations by back-propagating errors.nature 323(6088):533–536





Runge J, Nowack P, Kretschmer M, et al (2019) Detecting and quantifying causal associations inlarge nonlinear time series datasets. Science advances 5(11):eaau4996





Saharia C, Chan W, Saxena S, et al (2022) Photorealistic text-to-image diffusion models with deeplanguage understanding. Advances in neural information processing systems 35:36479–36494





S¸AHiN E, Arslan NN, Ozdemir D (2024) Unlocking the black box: an in-depth review on inter- ¨pretability, explainability, and reliability in deep learning. Neural Computing and Applications pp1–107





Sankaranarayanan S, Balaji Y, Castillo CD, et al (2018) Generate to adapt: Aligning domains usinggenerative adversarial networks. In: Proceedings of the IEEE conference on computer vision andpattern recognition, pp 8503–8512





Scarselli F, Gori M, Tsoi AC, et al (2008) The graph neural network model. IEEE transactions onneural networks 20(1):61–80





Sch¨olkopf B, Locatello F, Bauer S, et al (2021) Toward causal representation learning. Proceedingsof the IEEE 109(5):612–634





Shabani MA, Abdi AH, Meng L, et al (2023) Scaleformer: Iterative multi-scale refining transformersfor time series forecasting. In: The Eleventh International Conference on Learning Representations,URL https://openreview.net/forum?id=sCrnllCtjoE





Shafer G (1976) A Mathematical Theory of Evidence. Princeton University Press





Sharma K, Dwivedi YK, Metri B (2022) Incorporating causality in energy consumption forecastingusing deep neural networks. Annals of Operations Research pp 1–36





Sharma N, Mangla M, Mohanty SN, et al (2021) Employing stacked ensemble approach for timeseries forecasting. International Journal of Information Technology 13:2075–2080





Shen L, Kwok J (2023) Non-autoregressive conditional diffusion models for time series prediction.In: International Conference on Machine Learning, PMLR, pp 31016–31029





Shen L, Chen W, Kwok J (2024) Multi-resolution diffusion models for time series forecasting. In:The Twelfth International Conference on Learning Representations





Shi X, Chen Z, Wang H, et al (2015) Convolutional lstm network: A machine learning approach forprecipitation nowcasting. Advances in neural information processing systems 28





Shih SY, Sun FK, Lee Hy (2019) Temporal pattern attention for multivariate time series forecasting.Machine Learning 108:1421–1441





Shu Y, Lampos V (2024) Deformtime: Capturing variable dependencies with deformable attentionfor time series forecasting. arXiv preprint arXiv:240607438





Smyl S (2020) A hybrid method of exponential smoothing and recurrent neural networks for timeseries forecasting. International journal of forecasting 36(1):75–85





Smyl S, Oreshkin BN, Pe lka P, et al (2024) Any-quantile probabilistic forecasting of short-termelectricity demand. arXiv preprint arXiv:240417451





Sohl-Dickstein J, Weiss E, Maheswaranathan N, et al (2015) Deep unsupervised learning usingnonequilibrium thermodynamics. In: International conference on machine learning, PMLR, pp2256–2265





Song C, Lin Y, Guo S, et al (2020) Spatial-temporal synchronous graph convolutional networks:A new framework for spatial-temporal network data forecasting. In: Proceedings of the AAAIconference on artificial intelligence, pp 914–921





Song Y, Ermon S (2019) Generative modeling by estimating gradients of the data distribution.Advances in neural information processing systems 32





Song Y, Sohl-Dickstein J, Kingma DP, et al (2021) Score-based generative modeling throughstochastic differential equations. In: International Conference on Learning Representations, URLhttps://openreview.net/forum?id=PxTIG12RRHS





Soyiri IN, Reidpath DD (2013) An overview of health forecasting. Environmental health andpreventive medicine 18:1–9





Sparks AH, Carroll J, Goldie J, et al (2020) Bomrang: Australian government bureau of meteorology(bom) data client. R package version 07 0 URL https://CRAN R-project org/package= bomrang





Srivastava N, Hinton G, Krizhevsky A, et al (2014) Dropout: a simple way to prevent neural networksfrom overfitting. The journal of machine learning research 15(1):1929–1958





Stˇepniˇcka M, Burda M (2017) On the results and observations of the time series forecasting compe- ˇtition cif 2016. In: 2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), IEEE,pp 1–6





Sui M, Zhang C, Zhou L, et al (2024) An ensemble approach to stock price prediction using deeplearning and time series models. In: 2024 IEEE 6th International Conference on Power, IntelligentComputing and Systems (ICPICS), IEEE, pp 793–797





Sun F, Hao W, Zou A, et al (2024a) A survey on spatio-temporal series prediction with deep learning:taxonomy, applications, and future directions. Neural Computing and Applications pp 1–25





Sun Y, Xie Z, Chen D, et al (2024b) Hierarchical classification auxiliary network for time seriesforecasting. arXiv preprint arXiv:240518975





Taieb SB, Bontempi G, Atiya AF, et al (2012) A review and comparison of strategies for multi-step ahead time series forecasting based on the nn5 forecasting competition. Expert systems withapplications 39(8):7067–7083





Tang P, Zhang W (2024) Pdmlp: Patch-based decomposed mlp for long-term time series forecastin.arXiv preprint arXiv:240513575





Tashiro Y, Song J, Song Y, et al (2021) Csdi: Conditional score-based diffusion models for probabilis-tic time series imputation. Advances in Neural Information Processing Systems 34:24804–24816





Tay Y, Dehghani M, Bahri D, et al (2020) Efficient transformers: A survey. ACM Computing Surveys55:1 – 28. URL https://api.semanticscholar.org/CorpusID:221702858





Thompson PA (1990) An mse statistic for comparing forecast accuracy across series. InternationalJournal of Forecasting 6(2):219–227





Touvron H, Martin L, Stone K, et al (2023) Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:230709288





Truchan H, Kalfar C, Ahmadi Z (2024) Ltboost: Boosted hybrids of ensemble linear and gradientalgorithms for the long-term time series forecasting. In: Proceedings of the 33rd ACM InternationalConference on Information and Knowledge Management, pp 2271–2281





Ulyanov D, Vedaldi A, Lempitsky V (2016) Instance normalization: The missing ingredient for faststylization. arXiv preprint arXiv:160708022





Van Den Oord A, Dieleman S, Zen H, et al (2016) Wavenet: A generative model for raw audio. arXivpreprint arXiv:160903499 12





Vapnik V, Golowich S, Smola A (1996) Support vector method for function approximation, regressionestimation and signal processing. Advances in neural information processing systems 9





Vaswani A, Shazeer N, Parmar N, et al (2017) Attention is all you need. Advances in neuralinformation processing systems 30





Veliˇckovi´c P, Cucurull G, Casanova A, et al (2018) Graph attention networks. In: InternationalConference on Learning Representations, URL https://openreview.net/forum?id=rJXMpikCZ





Wachter S, Mittelstadt B, Russell C (2017) Counterfactual explanations without opening the blackbox: Automated decisions and the gdpr. Harv JL & Tech 31:841





Wang J, Jatowt A, Yoshikawa M (2022a) Timebert: Extending pre-trained language representationswith temporal information. arXiv preprint arXiv:220413032





Wang L, Adiga A, Chen J, et al (2022b) Causalgnn: Causal-based graph neural networks for spatio-temporal epidemic forecasting. In: Proceedings of the AAAI conference on artificial intelligence,pp 12191–12199





Wang S, Wu H, Shi X, et al (2024a) Timemixer: Decomposable multiscale mixing for time seriesforecasting. International Conference on Learning Representations





Wang W, Li H, Ding Z, et al (2023a) Rethinking maximum mean discrepancy for visual domainadaptation. IEEE Transactions on Neural Networks and Learning Systems 34(1):264–277. https://doi.org/10.1109/TNNLS.2021.3093468





Wang X, Zhou T, Wen Q, et al (2024b) Card: Channel aligned robust blend transformer for timeseries forecasting. In: The Twelfth International Conference on Learning Representations





Wang Y, Wu H, Dong J, et al (2024c) Timexer: Empowering transformers for time series forecast-ing with exogenous variables. In: The Thirty-eighth Annual Conference on Neural InformationProcessing Systems, URL https://openreview.net/forum?id=INAeUQ04lT





Wang Z, Miliou I, Samsten I, et al (2023b) Counterfactual explanations for time series forecasting.In: 2023 IEEE International Conference on Data Mining (ICDM), IEEE, pp 1391–1396





Wang Z, Kong F, Feng S, et al (2024d) Is mamba effective for time series forecasting? arXiv preprintarXiv:240311144





Wang Z, Ruan S, Huang T, et al (2024e) A lightweight multi-layer perceptron for efficient multivariatetime series forecasting. Knowledge-Based Systems 288:111463





Wei C, Niu C, Tang Y, et al (2022) Npenas: Neural predictor guided evolution for neural architecturesearch. IEEE Transactions on Neural Networks and Learning Systems 34(11):8441–8455





Wen Q, Zhou T, Zhang C, et al (2023) Transformers in time series: A survey. In: Elkind E (ed)Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23. International Joint Conferences on Artificial Intelligence Organization, pp 6778–6786, https://doi.org/10.24963/ijcai.2023/759, URL https://doi.org/10.24963/ijcai.2023/759, survey Track





Wen R, Torkkola K, Narayanaswamy B, et al (2017) A multi-horizon quantile recurrent forecaster.arXiv preprint arXiv:171111053





Winters PR (1960) Forecasting sales by exponentially weighted moving averages. Management science6(3):324–342





Wolpert DH (1992) Stacked generalization. Neural networks 5(2):241–259





Wong SC, Gatt A, Stamatescu V, et al (2016) Understanding data augmentation for classifica-tion: when to warp? In: 2016 international conference on digital image computing: techniques andapplications (DICTA), IEEE, pp 1–6





Woo G, Liu C, Kumar A, et al (2024) Unified training of universal time series forecasting transformers.In: Forty-first International Conference on Machine Learning





Wu B, Wang L (2024) Two-stage decomposition and temporal fusion transformers for interpretablewind speed forecasting. Energy 288:129728





Wu B, Yu S, Peng L, et al (2024a) Interpretable wind speed forecasting with meteorological featureexploring and two-stage decomposition. Energy 294:130782





Wu H, Xu J, Wang J, et al (2021) Autoformer: Decomposition transformers with auto-correlation forlong-term series forecasting. Advances in neural information processing systems 34:22419–22430





Wu H, Hu T, Liu Y, et al (2023) Timesnet: Temporal 2d-variation modeling for general time seriesanalysis. International Conference on Learning Representations





Wu Z, Huang NE (2009) Ensemble empirical mode decomposition: a noise-assisted data analysismethod. Advances in adaptive data analysis 1(01):1–41





Wu Z, Gong Y, Zhang A (2024b) Dtmamba: Dual twin mamba for time series forecasting. arXivpreprint arXiv:240507022





Xiong Q, Tang K, Ma M, et al (2024) Tdt loss takes it all: Integrating temporal dependencies amongtargets into non-autoregressive time series forecasting. arXiv preprint arXiv:240604777





Xu B, Sheng Y, Li P, et al (2019) Causes and classification of emd mode mixing. VibroengineeringProcedia 22:158–164





Xu X, Chen Z, Xu D, et al (2022) Mixing signals: Data augmentation approach for deep learningbased modulation recognition. arXiv preprint arXiv:220403737





Xu X, Liang Y, Huang B, et al (2024a) Integrating mamba and transformer for long-short rangetime series forecasting. arXiv preprint arXiv:240414757





Xu Z, Bian Y, Zhong J, et al (2024b) Beyond trend and periodicity: Guiding time series forecastingwith textual cues. arXiv preprint arXiv:240513522





Xu Z, Zeng A, Xu Q (2024c) Fits: Modeling time series with $1 0 k$ parameters. International Conferenceon Learning Representations





Xua B, Yang G (2024) Interpretability research of deep learning: A literature survey. InformationFusion p 102721





Xue H, Salim FD (2023) Promptcast: A new prompt-based learning paradigm for time seriesforecasting. IEEE Transactions on Knowledge and Data Engineering





Yan S, Xiong Y, Lin D (2018) Spatial temporal graph convolutional networks for skeleton-basedaction recognition. In: Proceedings of the AAAI conference on artificial intelligence





Yan T, Zhang H, Zhou T, et al (2021) Scoregrad: Multivariate probabilistic time series forecastingwith continuous energy-based generative models. arXiv preprint arXiv:210610121





Yan T, Gong H, Yongping H, et al (2024) Probabilistic time series modeling with decomposabledenoising diffusion model. In: Salakhutdinov R, Kolter Z, Heller K, et al (eds) Proceedings of the41st International Conference on Machine Learning, Proceedings of Machine Learning Research,vol 235. PMLR, pp 55759–55777, URL https://proceedings.mlr.press/v235/yan24b.html





Yang F, Li X, Wang M, et al (2023a) Waveform: Graph enhanced wavelet learning for long sequenceforecasting of multivariate time series. In: Proceedings of the AAAI Conference on ArtificialIntelligence, pp 10754–10761





Yang M, Xu C, Bai Y, et al (2023b) Investigating black-box model for wind power forecasting usinglocal interpretable model-agnostic explanations algorithm: Why should a model be trusted? CSEEJournal of Power and Energy Systems





Yang Y, Yang Y (2020) Hybrid method for short-term time series forecasting based on eemd. IEEEAccess 8:61915–61928





Yang Y, Jin M, Wen H, et al (2024a) A survey on diffusion models for time series and spatio-temporaldata. arXiv preprint arXiv:240418886





Yang Y, Zhu Q, Chen J (2024b) Vcformer: Variable correlation transformer with inherent lagged cor-relation for multivariate time series forecasting. In: Larson K (ed) Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence, IJCAI-24. International Joint Conferenceson Artificial Intelligence Organization, pp 5335–5343, https://doi.org/10.24963/ijcai.2024/590,URL https://doi.org/10.24963/ijcai.2024/590, main Track





Ye J, Zhang W, Yi K, et al (2024) A survey of time series foundation models: Generalizing timeseries representation with large language mode. arXiv preprint arXiv:240502358





Yi K, Zhang Q, Fan W, et al (2024) Frequency-domain mlps are more effective learners in time seriesforecasting. Advances in Neural Information Processing Systems 36





Yu C, Wang F, Shao Z, et al (2023) Dsformer: A double sampling transformer for multivariatetime series long-term prediction. In: Proceedings of the 32nd ACM international conference oninformation and knowledge management, pp 3062–3072





Yu G, Zou J, Hu X, et al (2024) Revitalizing multivariate time series forecasting: Learnable decompo-sition with inter-series dependencies and intra-series variations modeling. International Conferenceon Machine Learning





Yuan X, Qiao Y (2024) Diffusion-TS: Interpretable diffusion for general time series generation. In:The Twelfth International Conference on Learning Representations, URL https://openreview.net/forum?id=4h1apFjO99





Zeng A, Chen M, Zhang L, et al (2023) Are transformers effective for time series forecasting? In:Proceedings of the AAAI conference on artificial intelligence, pp 11121–11128





Zeng C, Liu Z, Zheng G, et al (2024) C-mamba: Channel correlation enhanced state space modelsfor multivariate time series forecasting. arXiv preprint arXiv:240605316





Zhan T, He Y, Li Z, et al (2024) Time evidence fusion network: Multi-source view in long-term timeseries forecasting. arXiv preprint arXiv:240506419





Zhang D, Wang Y (2024) Adaptive extraction network for multivariate long sequence time-seriesforecasting. arXiv preprint arXiv:240512038





Zhang J, Zheng Y, Qi D (2017) Deep spatio-temporal residual networks for citywide crowd flowsprediction. In: Proceedings of the AAAI conference on artificial intelligence





Zhang K, Wen Q, Zhang C, et al (2024a) Self-supervised learning for time series analysis: Taxonomy,progress, and prospects. IEEE Transactions on Pattern Analysis and Machine Intelligence





Zhang K, Zou X, Tang Y (2024b) Caformer: Rethinking time series analysis from causal perspective.arXiv preprint arXiv:240308572





Zhang M, Sun Y, Liang F (2024c) Sparse deep learning for time series data: theory and applications.Advances in Neural Information Processing Systems 36





Zhang Q, Chang J, Meng G, et al (2020) Spatio-temporal graph structure learning for trafficforecasting. In: Proceedings of the AAAI conference on artificial intelligence, pp 1177–1185





Zhang S, Chen Y, Xiao J, et al (2021a) Hybrid wind speed forecasting model based on multivariatedata secondary decomposition approach and deep learning algorithm with attention mechanism.Renewable Energy 174:688–704





Zhang S, Chen Y, Zhang W, et al (2021b) A novel ensemble deep learning model with dynamic errorcorrection and multi-objective ensemble pruning for time series forecasting. Information Sciences544:427–445





Zhang X, Chowdhury RR, Gupta RK, et al (2024d) Large language models for time series: Asurvey. In: Larson K (ed) Proceedings of the Thirty-Third International Joint Conference on Artifi-cial Intelligence, IJCAI-24. International Joint Conferences on Artificial Intelligence Organization,pp 8335–8343, https://doi.org/10.24963/ijcai.2024/921, URL https://doi.org/10.24963/ijcai.2024/921, survey Track





Zhang Y, Yan J (2023) Crossformer: Transformer utilizing cross-dimension dependency for multivari-ate time series forecasting. In: The eleventh international conference on learning representations





Zhang Y, Ma L, Pal S, et al (2024e) Multi-resolution time-series transformer for long-term forecasting.In: International Conference on Artificial Intelligence and Statistics, PMLR, pp 4222–4230





Zhao S, Ma T, Ermon S (2020) Individual calibration with randomized forecasting. In: InternationalConference on Machine Learning, PMLR, pp 11387–11397





Zheng C, Fan X, Wang C, et al (2020) Gman: A graph multi-attention network for traffic prediction.In: Proceedings of the AAAI conference on artificial intelligence, pp 1234–1241





Zhou H, Zhang S, Peng J, et al (2021) Informer: Beyond efficient transformer for long sequence time-series forecasting. In: Proceedings of the AAAI conference on artificial intelligence, pp 11106–11115





Zhou T, Ma Z, Wen Q, et al (2022) Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In: International conference on machine learning, PMLR, pp 27268–27286





Zhou T, Niu P, Wang X, et al (2023) One fits all: Power general time series analysis by pre-trained LM. In: Thirty-seventh Conference on Neural Information Processing Systems, URLhttps://openreview.net/forum?id=gMS6FVZvmF

