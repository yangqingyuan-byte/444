第一章 绪论
• 1.1 研究背景与意义
• 1.2 国内外研究现状
  • 1.2.1 基于统计学及浅层机器学习的方法
  • 1.2.2 基于循环与卷积神经网络的方法
  • 1.2.3 基于 Transformer 与注意力机制的方法
  • 1.2.4 基于大语言模型(LLM)的方法
• 1.3 本文研究目标与主要内容
• 1.4 论文组织结构

第二章 相关理论基础与关键技术
• 2.1 时间序列预测基础
  • 2.1.1 时间序列预测问题定义
  • 2.1.2 两种评价指标的数学特性与适用场景
• 2.2 大语言模型基础理论
  • 2.2.1 Transformer 架构与预训练机制
  • 2.2.2 Qwen3-1.4B 模型架构分析
• 2.3 时间序列频域分析理论
  • 2.3.1 傅里叶变换及其局限性
  • 2.3.2 小波包分解原理
  • 2.3.3 频域神经网络原理
• 2.4 门控注意力机制原理
  • 2.4.1 传统注意力机制的局限性
  • 2.4.2 门控注意力的数学原理与优势
• 2.5 数据集介绍
  • 2.5.1 数据集描述
  • 2.5.2 数据预处理方法

第三章 基于 Qwen3 与门控注意力的统一改进框架
• 3.1 整体架构设计
• 3.2 冻结大模型的适配器设计
  • 3.2.1 跨模态特征投影与维度对齐
  • 3.2.2 时序位置编码与语义嵌入设计
  • 3.2.3 适配器参数量与训练效率分析
• 3.3 门控注意力池化模块
• 3.4 基线实验与统一改进有效性验证
  • 3.4.1 实验设置
  • 3.4.2 Qwen3-1.4B 与 GPT-2 的对比实验
  • 3.4.3 门控注意力机制的有效性验证
• 3.5 本章小结

第四章 融合小波包分解的时频域增强方法
• 4.1 问题分析与研究动机
• 4.2 模型架构设计
  • 4.2.1 小波包多尺度分解策略
  • 4.2.2 频域子带特征与 LLM 嵌入空间的映射机制
  • 4.2.3 多尺度特征融合与预测头设计
• 4.3 实验设置与结果分析
  • 4.3.1 实验设置与对比基线
  • 4.3.2 MSE 指标表现及显著性分析
  • 4.3.3 极端波动场景下的预测可视化分析
  • 4.3.4 消融实验：小波包分解层数的影响
• 4.4 本章小结

第五章 融合频域前馈网络的全局频域增强方法
• 5.1 问题分析与研究动机
• 5.2 模型架构设计
  • 5.2.1 频域前馈网络结构设计
  • 5.2.2 复数域特征提取与门控权重融合机制
  • 5.2.3 频域-时域特征交互设计
• 5.3 实验设置与结果分析
  • 5.3.1 实验设置与对比基线
  • 5.3.2 MAE 指标表现及稳健性分析
  • 5.3.3 长期预测趋势平滑度分析
  • 5.3.4 消融实验：频域前馈网络深度的影响
• 5.4 本章小结

第六章 MSE 与 MAE 优化分化的机理分析与模型选择策略
• 6.1 综合性能对比分析
  • 6.1.1 MSE 与 MAE 指标的对比结果
  • 6.1.2 不同数据集上的表现差异
• 6.2 误差特性机理分析
  • 6.2.1 小波包分解对大误差抑制的机理分析
  • 6.2.2 频域前馈网络对平均误差优化的机理分析
  • 6.2.3 指标分化的理论解释
• 6.3 计算复杂度与收敛性分析
  • 6.3.1 计算复杂度对比
  • 6.3.2 训练收敛速度与稳定性分析
• 6.4 模型选择策略
  • 6.4.1 基于误差敏感度的场景分类
  • 6.4.2 模型选择建议
• 6.5 本章小结

第七章 总结与展望
• 7.1 研究工作总结
• 7.2 研究局限性分析
• 7.3 未来研究方向

参考文献

致谢

# 摘要
多元时间序列预测作为xxx的有效方法，在深度学习中获得了广泛关注。其核心在于通过xxx机制，使模型在xxx条件下可以xxx，这一特性使其在xxx等领域展现出重要的应用价值当前多元时间序列预测的框架通常基于xxx的假设构建，然而xxx会出现xxx因素，而本研究聚焦于xxx，基于xxx问题，对多元时间序列预测进行研究。
本研究面向多元时间序列预测任务，围绕xxx问题进行研究，结合自注意力机制，频域分析方法和大语言模型，提出一种基于xxx的xxx框架，具体研究内容如下：
(1) 针对xxx的问题，本研究构建了xxx网络，旨在xxx。基于xxx，创新性地设计了xxx，一方面xxx，另一方面xxx。
(2) 针对xxx问题，本研究提出了基于xxx的xxx框架。基于xxx的特点设计了xxx模块：首先xxx，继而xxx。对xxx进行xxx处理，实现xxx，避免xxx。
(3) 针对xxx的问题，本研究提出了xxx范式，通过集成xxx，构建xxx，在xx层面，设计xxx；在损失优化层面创新性引入xxx，建立xxx机制。
本研究在ETT，ECL，Weather，ILI，Exchange共7个数据集上对所提出的xxx网络进行了全面的评估，对比实验显示xxx在xxx相较于xxx方法均展现出更优秀的性能，验证了其xxx，能够xxx，对多元时间序列预测的研究起到了一定的推动作用。


多元时间序列预测作为刻画复杂动态系统演化过程的重要手段，在能源调度、交通管理、金融风控和气象预报等领域得到广泛应用。近年来，基于深度学习的时间序列预测模型在非线性建模和长程依赖刻画方面取得了显著进展，但现有方法通常依赖单一模态或单一结构假设：一方面，纯时域模型在捕捉多尺度周期性与局部突变方面仍存在不足；另一方面，将大语言模型（Large Language Model, LLM）直接引入时间序列预测又面临模态对齐困难、计算开销较大等问题。针对上述矛盾，本文聚焦于多元长程时间序列预测任务，围绕如何在可控计算复杂度下高效利用大语言模型并融合时频信息展开研究。
本研究面向多元时间序列预测任务，结合自注意力机制、频域分析方法与大语言模型，提出一种基于 Qwen3 与时频域增强的统一改进框架。具体研究内容如下：
（1）针对多元时间序列与大语言模型语义空间难以直接对齐的问题，构建了基于冻结 Qwen3 的统一改进网络，设计轻量级适配器模块实现时间序列特征向语言嵌入空间的映射，并引入门控注意力池化机制，在降低额外参数与训练开销的前提下，有效抑制冗余信息、突出关键时间步与重要变量对预测结果的贡献。
（2）针对长程预测中长期趋势与局部波动难以兼顾的问题，提出融合小波包分解的时频域增强方法。该方法首先对多元时间序列进行多尺度小波包分解，提取不同频段的子带特征；随后将子带特征映射至大语言模型嵌入空间，与时域特征在统一框架下进行多尺度融合，从而提升模型在复杂周期性和极端波动场景下的预测精度与稳定性。
（3）针对全局周期结构刻画不足的问题，提出融合频域前馈网络的全局频域增强方法。在频域中构建复数域频域前馈网络以捕捉全局谱结构，并通过频域–时域特征交互模块将全局频谱信息反馈至时域预测头，在不显著增加计算复杂度的前提下有效改善 MAE 等平均误差指标表现。
（4）针对不同评价指标下模型表现分化缺乏系统分析的问题，从 MSE 与 MAE 的优化特性出发，对所提出各模块在多数据集上的误差分布与收敛特性进行对比研究，揭示频域增强与门控机制对大误差抑制与整体误差收敛的作用机理，并据此提出基于误差敏感度的场景分类与模型选择策略，为实际应用中根据业务需求（如对峰值误差或平均误差的不同偏好）选择合适的预测模型提供参考。
本文在多个公开多元时间序列基准数据集（ETT、ECL、Weather、ILI、Exchange 等）上对所提出的统一改进框架进行了系统评估。实验结果表明，相较于主流基线模型，本文方法在长程预测任务中整体取得更优的 MSE 和 MAE 指标表现，并在极端波动与长序列场景下展现出更高的稳健性和泛化能力。其中，融合小波包分解的时频域增强方法在 MSE 指标上普遍表现更佳，更有利于抑制少量大误差样本；而融合频域前馈网络的全局频域增强方法在 MAE 指标上具有更明显的优势，更有助于降低整体平均误差，二者在误差优化上的互补性为后续基于场景需求的模型选择提供了依据。上述结果验证了将大语言模型与时频域增强相结合用于多元时间序列预测的有效性和应用潜力。




# 第一章 绪论

## 1.1 研究背景与意义
多元时间序列是刻画客观世界动态演化过程的重要数据形式，广泛存在于金融市场波动、气象环境变化、城市交通流量、电力与可再生能源负荷、工业设备状态监测等众多领域。随着传感器、物联网和移动计算等技术的普及，海量时间序列数据被持续、高频地采集与存储，使得基于历史观测对未来趋势和状态进行预测，成为支撑智能决策和精细化管理的关键技术之一。通过对时间序列的建模与预测，研究者和工程实践者能够更早地感知潜在风险、优化资源配置、提升系统运行效率，因此时间序列预测不仅具有重要的理论价值，也具有显著的工程应用意义。
传统的时间序列预测方法主要建立在统计学框架之上，如自回归模型（AR）、移动平均模型（MA）、自回归积分滑动平均模型（ARIMA）等。这类方法在处理平稳或近似线性的单变量时间序列时具有较好的解释性和可实现性，能够有效刻画趋势性和周期性等基本特征。然而，真实场景中的时间序列往往呈现出显著的非线性、多尺度、多变量耦合以及突发性等复杂特征，并伴随噪声干扰、结构性变化和缺失数据等问题。随着数据维度和时间跨度的不断扩展，基于线性假设和先验结构设定的传统模型在表达能力、鲁棒性以及高维关联建模方面的局限性日益凸显，难以满足复杂场景下对预测精度和稳定性的双重要求。
近年来，深度学习技术的迅猛发展为时间序列预测提供了新的思路。循环神经网络（RNN）、长短期记忆网络（LSTM）、门控循环单元（GRU）等模型利用循环结构刻画时间依赖关系，在一定程度上缓解了传统方法难以建模非线性和长依赖的问题；卷积神经网络（CNN）依托局部感受野和权值共享机制，能够高效抽取局部时序模式与多尺度特征；基于注意力机制的 Transformer 模型则通过自注意力结构显式建模远距离时间步之间的依赖，在并行计算和长序列建模方面展现出明显优势。围绕长程时间序列预测任务，大量工作从架构设计、序列分块、趋势–季节分解、频域变换等角度提出了丰富的改进方法，在多个公开数据集和实际应用场景中取得了优于传统统计模型的性能表现。
在深度学习推动时间序列预测性能持续提升的同时，如何在更大尺度、更复杂结构的多元时间序列上兼顾预测精度与计算效率，成为当前研究的核心挑战之一。一方面，长程预测任务要求模型同时捕获局部短期波动、全局长期趋势以及多变量之间的复杂交互，模型表达能力不足会导致长期预测快速退化；另一方面，深层结构和全局注意力机制往往带来较高的时间与空间复杂度，在大规模、多场景应用中容易遭遇训练和推理开销过大的瓶颈。此外，随着时间序列预测任务从“单一模态建模”走向“多模态与先验知识融合”，如何在引入更多信息源的同时避免模态信息的相互干扰和表示纠缠，保障特征表示的判别性与鲁棒性，也成为新的研究难点。
大型预训练语言模型（Large Language Model, LLM）的出现，为时间序列预测提供了新的研究范式。得益于在大规模语料上的预训练，LLM 具备强大的表征学习和跨任务泛化能力，一些工作开始尝试将时间序列映射到语言空间，或通过提示（prompt）等方式将任务语义、场景先验与时间序列数据进行联合建模，以期获得更丰富、更具语义结构的时间序列表征。这类方法在多元时间序列预测中展现出较好性能，但同时也暴露出诸如模态表示纠缠、跨模态对齐不充分、冗余计算开销较大等问题：一方面，时间序列特征与文本提示特征在简单拼接或混合后容易发生信息干扰，削弱了对关键时序模式的刻画能力；另一方面，直接将大模型引入预测框架会显著增加参数规模和推理成本，难以满足实时性或资源受限场景的应用需求。
在以上背景下，如何在充分利用大语言模型表征能力的同时，有效整合时间域、频域以及语义域等多种模态信息，构建既具表达力又具计算效率的多元时间序列预测框架，成为值得深入研究的重要问题。一方面，需要设计更合理的跨模态对齐与特征解耦机制，使时间序列模态与语言模态能够在统一表示空间中形成互补而非相互干扰的关系，从而提升多模态融合后的预测性能和鲁棒性；另一方面，需要结合频域分析、小波包分解、频域前馈网络等时频建模手段，挖掘时间序列在不同频段上的周期性与局部变动特征，以缓解长程预测中长期趋势与局部波动难以兼顾的问题，同时在结构设计和参数配置上兼顾计算复杂度与模型精度之间的平衡。
在此研究背景下，本文围绕多元长程时间序列预测任务，面向“提升预测精度”和“提高运行效率”两个核心目标展开。通过引入大语言模型与时间序列专用编码结构相结合的统一框架，一方面利用预训练大模型的语义表征能力和跨任务迁移能力，增强多元时间序列在高维表示空间中的判别性与泛化性；另一方面，结合门控注意力机制、小波包分解与频域前馈网络等结构，从时域与频域两个视角刻画不同时间尺度上的关键特征，力图在保证模型表达能力的同时控制整体计算开销，从而为实际应用场景下的长程预测提供一种具有可行性的解决方案。
从理论层面来看，本文的研究具有以下意义：第一，通过在统一框架下引入大语言模型、时域编码与频域建模模块，有助于系统性地探索“时间序列–语言–频域”多模态融合的建模范式，丰富现有时间序列预测方法在模型结构与表征空间设计方面的研究视角；第二，通过对不同结构在 MSE、MAE 等评价指标下表现差异及其误差机理的分析，可为长程时间序列预测中评价指标的选择与模型结构的权衡提供一定的理论参考；第三，通过跨模态对齐与门控注意力等机制的设计，有望推动关于“如何在引入大模型的同时控制计算复杂度与收敛特性”的相关研究，为后续将预训练大模型迁移到其他时序任务提供方法论借鉴。
从应用层面来看，本文的研究工作同样具有重要的现实意义。一方面，更高精度和更稳定的多元时间序列预测模型，有助于在能源调度、城市交通管理、环境监测、金融风险控制等典型场景中提供更可靠的前瞻性信息支持，提升系统运行的安全性与经济性；另一方面，通过在结构设计中显式考虑计算复杂度与部署开销，本文提出的框架有望更好地适应实际工程环境中的资源约束与实时性需求，为在真实系统中落地基于大模型的时间序列预测方法提供可行路径。综上所述，围绕多元长程时间序列预测构建面向大语言模型与时频增强的统一改进框架，不仅能够丰富时间序列预测领域的理论研究体系，也有望在多行业智能化决策与运行优化中发挥积极作用。



## 1.2 国内外研究现状

### 1.2.1 基于统计学及浅层机器学习的方法
在机器学习与深度学习方法兴起之前，时间序列预测主要依赖传统统计建模框架，其核心思想是通过参数化的概率模型刻画数据中的趋势性、周期性和波动性等基本特征。早期的指数平滑方法由 Brown、Holt 与 Winters 等学者提出，通过对历史观测值施加指数递减的权重，实现对未来值的递推预测[20]。该类方法形式简单、计算开销小，适用于对最新观测更加敏感的短期预测任务。随后发展起来的自回归（AR）[17]、滑动平均（MA）[18] 以及自回归滑动平均（ARMA）模型[19]，利用历史值及其误差项的线性组合刻画序列的自相关结构，在处理线性和平稳时间序列方面具有良好的建模能力。Box–Jenkins 体系提出的自回归积分滑动平均（ARIMA）模型在 ARMA 基础上引入差分算子，对含趋势或弱非平稳序列进行预处理，从而扩展了统计模型在经济、金融等领域的应用范围，而其季节性扩展形式 SARIMA 进一步通过季节差分和季节项刻画周期性模式[11,29]。总体来看，统计模型依托明确的建模假设，具备可解释性强、实现简洁等优点，长期以来为时间序列预测提供了坚实的理论与方法基础。

然而，上述统计模型普遍建立在线性和平稳性的前提之上，对于现实中普遍存在的非线性、突变、多尺度等复杂动态特征刻画能力有限。随着传感器与信息系统的发展，时间序列数据在维度与规模上不断扩展，多变量间存在复杂的相互依赖关系，传统统计模型在高维场景下往往需要较强的先验设定与特征工程支持，其长期预测性能和泛化能力也随之受限。这一局限促使研究者逐步引入浅层机器学习方法，以缓解线性假设过强、模型表达能力不足等问题。典型代表包括支持向量机（SVM）[21]、决策树及其集成模型（随机森林、梯度提升树等）[22–24]，以及基于 K 近邻思想的预测方法[25,26]。其中，SVM/支持向量回归（SVR）通过核函数在高维特征空间中学习非线性映射，在处理非线性回归任务中表现出较好的鲁棒性；决策树及其集成模型依托树结构划分与集成学习框架，能够在无需严格平稳性假设的前提下挖掘特征与目标之间的复杂关系，并在含噪数据环境下保持一定的稳定性；K 近邻方法则通过度量当前样本与历史样本之间的相似性，利用相似片段的局部统计特性进行预测，在具有显著局部模式的时间序列任务中具有直观性和易用性。

尽管统计模型与浅层机器学习方法在一定程度上提升了对时间序列结构的刻画能力，并在诸多实际场景中获得成功应用，但二者在面对现代大规模、多变量、强非线性时间序列时仍存在明显不足。一方面，统计模型对线性和平稳性的依赖，使其难以捕捉复杂的非线性动态和长程依赖结构；另一方面，浅层机器学习虽然具备更强的非线性拟合能力，但通常依赖人工构造特征，对长时间跨度的时序依赖建模能力有限，且在数据规模不断扩张的背景下，训练与更新的计算成本迅速增加。随着时间序列预测任务在精度、鲁棒性与实时性上的需求不断提升，这些局限性逐渐凸显，为后续基于深度学习的序列建模方法提供了重要发展空间和研究动机。基于此，研究者开始尝试引入具有更强表示能力的神经网络结构，以在数据驱动的框架下自动学习时间依赖与复杂模式。

### 1.2.2 基于循环与卷积神经网络的方法
在传统统计模型与浅层机器学习方法的基础上，深度学习通过端到端的特征学习与强大的非线性拟合能力，为时间序列预测提供了新的技术路径。随着反向传播算法和多层感知机（MLP）的提出，神经网络开始被引入时间序列建模，但早期 MLP 更适合处理定长向量，对序列中的时间依赖刻画不足，且在深层结构中易出现梯度消失问题，再叠加当时数据规模和算力条件的限制，使得基于 MLP 的方法在长序列建模上难以取得突破[15–17]。为更好地处理随时间演化的数据，循环神经网络（Recurrent Neural Network, RNN）被提出作为第一类能够显式建模序列依赖关系的神经网络结构[18]。RNN 通过在时间维度上引入隐藏状态的递归传递，使当前时刻的输出不仅依赖当前输入，还依赖以往历史信息，实现了对时间上下文的自然建模，并凭借参数共享机制在理论上可以处理任意长度的序列，因此在早期的时间序列预测、语音建模和自然语言处理等任务中得到广泛应用[18,19]。

然而，基础 RNN 在实践中暴露出明显短板：一是梯度在时间反向传播过程中易出现消失或爆炸，导致模型难以有效学习远距离时间步之间的长程依赖；二是串行计算特性使其难以充分利用现代并行硬件，训练效率受限[18,25]。为缓解这些问题，长短期记忆网络（Long Short-Term Memory, LSTM）和门控循环单元（Gated Recurrent Unit, GRU）相继被提出，通过在循环结构中引入记忆单元与门控机制，有选择地“保留”或“遗忘”历史信息，显著缓解了梯度消失问题，使得基于 RNN/LSTM/GRU 的模型在较长时间跨度的预测任务中表现出更好的稳定性和精度[18,19]。在此基础上，大量变体陆续提出，例如通过膨胀连接扩大感受野的 Dilated RNN、通过双阶段注意力机制自适应选择关键时间步与特征的 DA-RNN，以及结合序列到序列框架与分位数回归以给出区间预测的 MQ-RNN 等，这些方法进一步提升了循环结构在长序列、带不确定性的预测任务中的适用性[15–17]。

与 RNN 系列侧重沿时间轴递归建模不同，卷积神经网络（Convolutional Neural Network, CNN）最初主要服务于图像等空间数据，通过局部感受野与权值共享机制高效提取局部模式[20,21]。随后研究者将一维卷积推广到时间轴上，发展出适用于时间序列的 1D CNN 结构，通过滑动卷积核对局部时间窗口进行特征提取，从而挖掘短期模式与局部依赖关系。代表性工作如 WaveNet 利用一维膨胀卷积在语音信号建模中显著提升了对长程依赖的捕捉能力，展示了基于卷积的时序建模在长序列任务中的潜力[20]。在此基础上，Temporal Convolutional Network（TCN）通过多层一维卷积堆叠与膨胀因子的逐层扩展，在保持严格因果性的前提下获得了大感受野，兼具并行计算优势与长程建模能力，在时间序列预测、信号处理和序列建模等任务中均取得了优异表现[16,21]。

在循环结构与卷积结构各自发展的同时，学界也开始探索 RNN 与 CNN 的融合架构，以兼顾局部模式提取与长程依赖建模。典型做法是首先利用 CNN 在时间维或时空图上抽取局部/空间特征，再将提取到的高层次特征序列输入 LSTM/GRU 等循环单元进行时间建模。例如，面向交通流量预测的 DCRNN 将交通网络建模为有向图，利用扩散卷积刻画空间依赖关系，再结合门控循环结构捕捉时间动态，从而在时空依赖建模方面取得显著优势；TPA-LSTM 则通过卷积模块提取局部时间模式，并结合 LSTM 与注意力机制学习关键时间片段，提高了在复杂周期性和多尺度波动场景下的预测精度[15,16]。综合来看，基于 RNN 与 CNN 的方法在较长一段时间内构成了深度学习时间序列预测的主流方案，相较于传统统计与浅层机器学习方法，显著提升了对非线性关系、长程依赖和多尺度结构的建模能力，为后续基于注意力机制和 Transformer 的模型奠定了重要基础[17–19]。

### 1.2.3 基于 Transformer 与注意力机制的方法
在循环与卷积结构推动时间序列预测取得显著进展的基础上，Transformer 的提出进一步改变了序列建模的范式。作为一种以自注意力机制为核心的编码–解码架构，Transformer 最初在机器翻译等自然语言处理任务中取得突破性进展[22]。其通过多头自注意力在全局范围内建模序列各位置间的依赖关系，并依托完全并行的计算模式显著提升了长序列建模的效率与表达能力。与递归结构依赖时间步串行计算不同，自注意力可以在单层中同时访问完整的上下文信息，根据相关性为不同时间步分配不同权重，从而在理论上更适合捕捉长程依赖与复杂模式，这使得 Transformer 很快被引入时间序列预测领域[23,24]。

在时间序列预测，尤其是长程时间序列预测（Long-Term Series Forecasting, LTSF）任务中，基于 Transformer 的模型近年来成为研究热点[23,27]。一方面，自注意力机制在时间域上可同时考虑所有历史时间步，并通过学习的注意力权重自动突出关键时间片，有助于挖掘多尺度趋势与周期模式；另一方面，多头结构与残差连接等设计增强了模型对高维、多变量时序数据的表示能力。然而，原始 Transformer 直接应用于长序列时会面临自注意力计算与存储复杂度随序列长度呈二次增长的问题，且自回归式解码容易导致误差在多步预测中累积。为缓解这些问题，大量改进型 Transformer 被提出：一类工作通过稀疏注意力、局部窗口或金字塔结构减少注意力连接数，从而将复杂度从 \(O(L^2)\) 降低到近似 \(O(L\log L)\)，典型代表包括基于日志稀疏模式的 LogTrans、引入局部敏感哈希与可逆残差结构的 Reformer，以及通过 ProbSparse 注意力与生成式解码提升长序列效率的 Informer 等；另一类工作则从时间序列特性出发，将趋势–季节分解、频域表示等先验融入 Transformer 结构，如 Autoformer 将序列分解与自相关机制内嵌于网络模块，Pyraformer 采用金字塔式注意力以分层方式建模长依赖，FEDformer 则利用 Fourier/Wavelet 稀疏基在频域中压缩表示长程周期信息，在多个公共数据集上相较基础 Transformer 与传统深度模型取得了更优的长程预测性能[23,24]。

尽管上述基于注意力机制的模型在长程依赖建模和性能上取得了重要进展，Transformer 在时间序列预测中的应用仍存在若干共性瓶颈。一方面，即便采用稀疏或分块注意力，其核心计算模式仍然显著重于线性模型或卷积模型，难以彻底消除随序列长度增长带来的时间与空间开销，在资源受限或实时性要求较高的场景中部署成本较高[23]。另一方面，近期研究表明，简单增加时间窗口长度并不总能提升 Transformer 在长程预测任务中的表现，反而可能导致模型在过长输入上过拟合噪声，难以真正提取有效的远程时序信息[27]。此外，归一化与残差等设计在处理非平稳时间序列时可能削弱关键幅值信息，促使后续工作提出去平稳化注意力、频域增强等改进方向，以提升 Transformer 在复杂时序分布下的稳健性。总体来看，基于 Transformer 与自注意力机制的方法在时间序列预测领域已经形成丰富的模型家族，为长程依赖建模提供了强大工具，但在效率、鲁棒性以及与其他结构（如线性模型、频域网络等）的协同方面仍存在改进空间，也为本文后续工作提供了重要的研究基础与对比对象。
### 1.2.4 基于大语言模型(LLM)的方法
近年来，大语言模型（Large Language Model, LLM）在自然语言处理和多模态理解领域取得了突破性进展，研究者开始尝试将其强大的表征与推理能力迁移到时间序列分析中，用于预测、分类、异常检测、填补缺失等多种任务。与前述仅针对数值序列设计的深度模型不同，LLM 方法的核心挑战在于如何弥合“离散文本预训练”与“连续数值时间序列”之间的模态鸿沟，并在此基础上有效地复用预训练模型中蕴含的大规模先验知识。围绕这一目标，现有工作大致可以分为五类路径：直接提示、量化建模、模态对齐、视觉桥接以及工具集成，每一类都对应对标准 LLM 流水线中不同阶段的改造。
第一类是基于提示（prompting）的直接调用方法，将时间序列以文本形式序列化后直接输入现成的 LLM，通过设计合适的提示模板实现零样本或小样本的预测与分类。例如，将温度、电价或传感器读数按时间顺序写入自然语言描述，再让模型输出未来趋势或类别。这类方法实现成本低、无需额外训练，适合缺乏标注数据的场景，但将精细数值信息转成长文本既拉长上下文，又容易丢失数值间的精确关系，在高维、多变量或高精度预测任务中效率与效果均受限。
第二类是基于量化（quantization）的离散化方法，通过向量量化（如 VQ-VAE）、K-Means 聚类或离散分箱等手段，将连续时间序列映射到有限的“代码本”索引或符号类别，再作为 token 输入 LLM。一方面，这种离散表示使时间序列更贴近 LLM 原生的离散输入形式，便于复用现有架构；另一方面，通过在时间或频率域上降采样与编码，可以显著压缩长序列长度，缓解上下文窗口和计算复杂度压力。然而，这类方法通常需要额外训练编码器/解码器，存在两阶段训练带来的子最优问题，同时量化误差和代码本设计也会影响下游任务性能。
第三类是基于模态对齐（aligning）的方法，通过显式学习时间序列与语言空间之间的一致性来迁移 LLM 能力。一部分工作采用对比学习、最优传输等目标，将时间序列编码器输出与文本描述或报告的嵌入对齐，学习共享语义空间，从而支持跨模态检索、报告生成等任务；另一部分工作则以预训练 LLM（如 GPT 系列、LLaMA 系列）为主干，在其输入前叠加专门的时间序列编码层，将数值序列投影到适配 LLM 的嵌入空间，并通过微调或适配器实现统一处理多种时序任务（预测、分类、异常检测等）。这一路线的优势在于能够端到端地保留 LLM 的语言知识和推理能力，但模型设计与训练过程相对复杂，对计算和数据资源要求较高。
第四类是以视觉模态为桥梁的多模态方法，将时间序列转化为图像或与视觉信号配对，再利用预训练视觉–语言模型或多模态 LLM 完成对时间序列的理解与生成。例如，把时间序列绘制为折线图、频谱图等输入视觉–语言模型，让其生成趋势描述、风险提示等文本，或者通过多模态预训练将 IMU、EEG 等时序信号对齐到图像–文本共享空间。这种方法能够间接利用成熟的视觉多模态模型，但额外引入了图像渲染和视觉编码过程，难以精确保留原始数值信息，也并非适用于所有类型的时间序列数据。
第五类是以 LLM 作为外部工具调度器（tool integration）的方法，并不直接让 LLM对时间序列做前向预测，而是让其根据文本指令或先验知识生成代码、配置或 API 调用，再由专用时序模型执行具体分析。例如，利用 LLM 自动生成损失函数、特征工程代码或调用天气、金融等外部时序 API 进行推理。此类方法充分发挥了 LLM 在自然语言理解与程序生成方面的优势，为时间序列任务提供“元建模”与“自动化建模”的能力，但整体流程往往难以端到端联合优化。
总体来看，基于大语言模型的时间序列方法一方面展示了在多任务、少样本甚至零样本场景下的强大潜力，能够将丰富的语言与领域知识引入数值时序建模；另一方面也面临数值表示不稳定、上下文长度与计算开销受限、模态对齐仍不充分等挑战。如何在保持 LLM 语义与推理优势的同时，设计更高效的数值表示与跨模态对齐机制，并结合频域分析、时序分解等领域知识构建统一的时序–语言建模框架，已成为当前研究的重点方向，也为本文后续基于大语言模型的多元时间序列预测框架提供了重要的理论与方法基础。
## 1.3 本文研究目标与主要内容
综上所述，现有时间序列预测方法在非线性关系建模、长程依赖刻画、多模态信息融合以及计算效率等方面仍存在一定不足，尤其是在多元长程预测场景下，如何在合理的计算开销内兼顾预测精度、稳健性与可迁移性，仍是一个具有挑战性的研究问题。为此，本文以多元长程时间序列预测任务为核心，围绕“充分利用大语言模型的表征能力”“融合时频域信息增强序列建模”“从评价指标视角解析模型行为”三个方面开展研究，具体目标与主要工作概括如下：

（1）构建面向多元时间序列预测的统一改进框架，引入冻结的大语言模型 Qwen3 作为语义表征骨干，通过适配器与门控注意力机制实现时间序列特征与语言嵌入空间的高效对齐与交互。一方面，在不显著增加参数量与训练开销的前提下挖掘大模型中蕴含的通用知识；另一方面，通过门控注意力池化抑制冗余信息、强化关键时间片段与变量的贡献，从而在多数据集上系统评估该统一改进框架的有效性。

（2）针对长程预测中长期趋势与局部波动难以兼顾的问题，设计融合小波包分解的时频域增强方法。通过对多元时间序列进行多尺度小波包分解，提取不同频段子带特征，并将其映射到大语言模型嵌入空间，与时域特征在统一框架下进行多尺度融合与联合预测。重点考察不同分解深度、子带选择策略对 MSE 等指标的影响，分析频域增强对极端波动场景下预测稳定性的作用机理。

（3）面向全局结构建模需求，提出融合频域前馈网络的全局频域增强方法。通过在频域中构建复数域前馈网络以刻画全局周期性与谐波结构，并设计频域–时域特征交互模块，将全局频谱信息反馈至时域预测头，以提升 MAE 等平均误差指标表现。通过系列消融实验与复杂度分析，评估不同网络深度与结构配置在精度–效率权衡下的利弊。

（4）从评价指标与误差机理的视角，系统分析 MSE 与 MAE 在不同模型与数据集下的优化分化特性，并结合计算复杂度与收敛行为，提出面向实际应用场景的模型选择策略。通过构建基于误差敏感度的场景分类与模型推荐方案，为在不同业务需求（如对大误差敏感或对整体稳定性敏感）的场景中合理选择预测模型提供参考。

总体而言，本文通过构建基于 Qwen3 与门控注意力的统一改进框架，并在此基础上引入小波包分解与频域前馈网络两类时频增强机制，同时结合 MSE/MAE 指标分化机理与复杂度分析，旨在为多元长程时间序列预测提供一种兼具预测精度、计算效率与理论解释性的系统性解决方案。


## 1.4 论文组织结构
本文围绕基于大语言模型与时频域增强的多元长程时间序列预测方法展开研究，全文结构安排如下：  

第一章为绪论，介绍多元时间序列预测的研究背景与意义，综述基于统计学、浅层机器学习、循环与卷积神经网络、Transformer 与注意力机制以及大语言模型等方法的国内外研究现状，明确本文的研究目标与主要内容，并对全文结构进行总体说明。  

第二章为相关理论基础与关键技术，主要介绍时间序列预测的基本概念与评价指标，梳理 Transformer 与大语言模型的基础理论，给出小波包分解与频域神经网络等频域分析方法以及门控注意力机制的数学原理，同时对本文所采用的数据集及其预处理方式进行说明，为后续方法设计与实验奠定理论与数据基础。  

第三章提出基于 Qwen3 与门控注意力的统一改进框架，给出整体架构设计，详细阐述冻结大语言模型条件下的适配器结构、时序位置编码与语义嵌入设计以及门控注意力池化模块，并通过与典型基线模型的对比实验验证统一改进框架在多元时间序列预测任务中的有效性。  

第四章提出融合小波包分解的时频域增强方法，从问题动机出发，引入多尺度小波包分解与子带特征映射策略，构建时频特征融合与预测头设计，并在多个数据集上通过对比实验、显著性分析与消融实验系统评估小波包分解层数等因素对模型性能和鲁棒性的影响。  

第五章提出融合频域前馈网络的全局频域增强方法，围绕频域前馈网络结构、复数域特征提取与门控权重融合以及频域–时域特征交互等关键设计，构建面向长程预测的全局频域增强框架，并通过实验对比与消融分析探讨网络深度等结构超参数对 MAE 指标及长期趋势建模能力的影响。  

第六章从 MSE 与 MAE 两类指标的优化行为出发，对不同模型在多数据集上的综合性能与误差特性进行对比分析，结合计算复杂度与收敛性研究，揭示小波包分解与频域前馈网络在大误差抑制与平均误差优化方面的机理差异，并据此提出面向实际应用场景的模型选择策略与推荐方案。  

第七章对全文工作进行总结，归纳本文的主要创新点和研究结论，分析当前研究的局限性，并展望未来在更大规模预训练模型、更加高效的时频域建模以及多任务多模态时间序列分析等方向的进一步研究空间。

# 第二章 相关理论基础与关键技术

## 2.1 时间序列预测基础
时间序列数据是按照时间顺序采集的一组观测值，通常表示为一维标量序列 \(\{x_t\}_{t=1}^T\) 或多维向量序列 \(\{\mathbf{x}_t\}_{t=1}^T\)，其中 \(t\) 表示时间步，\(T\) 为序列长度。对于单变量时间序列，有 \(x_t \in \mathbb{R}\)；对于多元时间序列，有 \(\mathbf{x}_t \in \mathbb{R}^C\)，其中 \(C\) 表示通道（变量）数。时间序列通常同时包含长期趋势、季节性（周期性）、局部波动以及随机噪声等成分，这些成分在时间轴上叠加并相互作用，使得对其进行建模与预测具有较高难度。通过从历史观测中挖掘这些潜在模式，可以为需求规划、资源调度、风险预警和策略制定提供关键支持。  

一般地，时间序列预测（Time Series Forecasting, TSF）的目标是利用历史观测值估计未来一段时间内的序列取值，可形式化为从过去窗口到未来窗口的映射。对于单变量时间序列预测问题，给定长度为 \(p\) 的历史窗口 \(\{x_{t-p}, \dots, x_t\}\)，预测未来 \(h\) 个时间步的取值可写为  
\[
 \hat{x}_{t+1:t+h} = f_\theta\bigl(x_{t-p:t}\bigr),
\]
其中 \(f_\theta(\cdot)\) 表示由参数 \(\theta\) 控制的预测模型，\(\hat{x}_{t+1:t+h} = (\hat{x}_{t+1}, \dots, \hat{x}_{t+h})\)。当 \(h=1\) 时为单步预测（single-step），关注一步前的短期预测；当 \(h>1\) 时为多步预测（multi-step），需要同时考虑误差在时间上的累积效应。  

在多元时间序列预测（Multivariate Time Series Forecasting, MTSF）中，模型同时利用多个相关变量的历史信息。设 \(\mathbf{X}_{t-p:t} = [\mathbf{x}_{t-p}, \dots, \mathbf{x}_t]^\top \in \mathbb{R}^{(p+1)\times C}\) 表示过去 \(p+1\) 个时间步的多元观测矩阵，\(\hat{\mathbf{X}}_{t+1:t+h} \in \mathbb{R}^{h\times C}\) 表示未来 \(h\) 个时间步的多元预测，则有  
\[
 \hat{\mathbf{X}}_{t+1:t+h} = f_\theta\bigl(\mathbf{X}_{t-p:t}\bigr)。
\]
与单变量情形相比，多元建模可以显式利用不同通道之间的相关性与协同关系，在能源、交通、金融等场景中往往能够获得更高的预测精度，但同时也带来了更高的维度、建模复杂度以及过拟合风险。  

从预测时间跨度的角度，时间序列预测可粗略分为短期时间序列预测（Short-Term Time Series Forecasting, STSF）和长期时间序列预测（Long-Term Time Series Forecasting, LTSF）。前者通常对应较小的 \(h\)，用于实时控制、库存管理等需要快速响应的场景，对近期局部模式的建模能力要求更高；后者则对应较大的 \(h\)，往往需要模型在较长时间范围内同时把握长期趋势与多尺度周期结构，对模型的表达能力、误差累积控制以及泛化性能提出更高要求。本文聚焦的多元长程时间序列预测任务，即是在上述 MTSF 框架下、针对较大预测步长 \(h\) 的情形，对模型在复杂动态与长程依赖条件下的建模能力进行系统研究。

### 2.1.1 时间序列预测问题定义
形式上，时间序列预测可以视为从过去观测窗口到未来预测窗口的映射学习问题。设给定的数据集为  
\[
\mathcal{D} = \bigl\{ \mathbf{X}^{(n)}_{1:T_n} \bigr\}_{n=1}^N,
\]
其中第 \(n\) 个样本为长度为 \(T_n\) 的多元时间序列 \(\mathbf{X}^{(n)}_{1:T_n} = \{\mathbf{x}^{(n)}_t\}_{t=1}^{T_n}\)，\(\mathbf{x}^{(n)}_t \in \mathbb{R}^C\)。在给定预测步长 \(h\) 和历史窗口长度 \(p\) 的情况下，多元长程时间序列预测任务可定义为：对任意时间索引 \(t\)（满足 \(p \le t \le T_n-h\)），利用历史窗口  
\[
\mathbf{X}^{(n)}_{t-p:t} = [\mathbf{x}^{(n)}_{t-p}, \dots, \mathbf{x^{(n)}_t}]^\top \in \mathbb{R}^{(p+1)\times C}
\]
预测未来 \(h\) 个时间步的序列  
\[
\hat{\mathbf{X}}^{(n)}_{t+1:t+h} = f_\theta\bigl(\mathbf{X}^{(n)}_{t-p:t}\bigr) \in \mathbb{R}^{h\times C},
\]
其中 \(f_\theta : \mathbb{R}^{(p+1)\times C} \to \mathbb{R}^{h\times C}\) 为带参数的预测模型，\(\theta\) 表示待学习的参数。训练阶段的目标是利用训练集 \(\mathcal{D}_{\mathrm{train}}\) 最小化某一损失函数（如 MSE 或 MAE）在所有样本与时间位置上的经验风险，即  
\[
\min_{\theta} \; \mathcal{L}(\theta) = \frac{1}{|\mathcal{S}|} \sum_{(n,t)\in \mathcal{S}} \ell\!\left(\hat{\mathbf{X}}^{(n)}_{t+1:t+h}, \mathbf{X}^{(n)}_{t+1:t+h}\right),
\]
其中 \(\mathcal{S}\) 表示所有可用的样本–时间对索引集合，\(\ell(\cdot,\cdot)\) 为逐样本损失函数。推理阶段，在给定新的历史窗口 \(\mathbf{X}_{t-p:t}\) 的情况下，模型根据学习得到的参数 \(\hat{\theta}\) 输出对应的未来预测 \(\hat{\mathbf{X}}_{t+1:t+h} = f_{\hat{\theta}}(\mathbf{X}_{t-p:t})\)。在后续小节中，本文将围绕 MSE 与 MAE 两种常用评价指标的数学特性与适用场景，对上述损失函数 \(\ell(\cdot,\cdot)\) 进行更为细致的讨论。
### 2.1.2 两种评价指标的数学特性与适用场景
在时间序列预测任务中，均方误差（Mean Squared Error, MSE）和平均绝对误差（Mean Absolute Error, MAE）是最常用的两类评价指标，它们从不同角度刻画预测值与真实值之间的偏差，对模型训练目标与应用场景均具有重要影响。设在某一数据集上共有 \(N\) 个样本，每个样本对应预测区间内共有 \(H\) 个时间步、\(C\) 个变量，真实值与预测值分别记为 \(\mathbf{Y}\in\mathbb{R}^{N\times H\times C}\)、\(\hat{\mathbf{Y}}\in\mathbb{R}^{N\times H\times C}\)，其中 \(\mathbf{Y}_{n,h,c}\)、\(\hat{\mathbf{Y}}_{n,h,c}\) 表示第 \(n\) 个样本在第 \(h\) 个时间步、第 \(c\) 个通道上的真实值与预测值。  

**（1）均方误差（MSE）的定义与特性。** MSE 定义为  
\[
\mathrm{MSE} = \frac{1}{NHC}\sum_{n=1}^{N}\sum_{h=1}^{H}\sum_{c=1}^{C}\bigl(\hat{\mathbf{Y}}_{n,h,c}-\mathbf{Y}_{n,h,c}\bigr)^2。
\]
从统计学角度看，MSE 等价于误差项的二阶矩估计，在假设噪声服从零均值高斯分布时，对应于最大似然意义下的最优估计量。其一大特点是对大误差更加敏感：平方运算会显著放大少数极端偏差样本在整体损失中的权重，因此在训练阶段采用 MSE 作为目标函数时，模型会倾向于优先压制大幅偏离点，从而降低整体误差方差。这一性质使 MSE 特别适用于对大偏差高度敏感的场景，例如电力负荷峰值预测、金融风险控制以及对安全性要求较高的工业过程监控等。然而，MSE 对异常值同样较为敏感，当数据中存在较多噪声或离群点时，可能导致模型过度拟合局部异常，从而影响对整体分布的刻画。  

**（2）平均绝对误差（MAE）的定义与特性。** MAE 定义为  
\[
\mathrm{MAE} = \frac{1}{NHC}\sum_{n=1}^{N}\sum_{h=1}^{H}\sum_{c=1}^{C}\bigl|\hat{\mathbf{Y}}_{n,h,c}-\mathbf{Y}_{n,h,c}\bigr|。
\]
从稳健统计的角度来看，MAE 对应于误差服从拉普拉斯分布时的最大似然估计，其梯度大小在误差幅值增大时保持常数，不会像 MSE 那样随误差平方增长。因此，MAE 对异常值相对不敏感，更侧重于整体的平均偏差水平，能够在存在噪声和局部异常的情况下提供更稳健的性能评估。MAE 的另一个优点在于其量纲与原始数据一致，便于直接解释，为业务方理解“平均误差大约为多少物理量单位”提供直观参考，适用于对整体预测偏差水平敏感、但允许少量大误差存在的应用场景，例如长期趋势预测、资源规划和高噪声环境下的粗粒度预估等。  

**（3）两类指标的互补性与适用场景。** 由于 MSE 与 MAE 在误差放大机制上的差异，二者在度量模型性能时往往呈现互补特性：在同一模型与数据集上，若 MSE 显著高于 MAE，通常意味着误差分布中存在少量幅度较大的“尾部”样本；反之，当两者数值接近时，则表明误差整体分布较为均匀。在模型训练时选择不同指标作为优化目标，会引导模型在不同方向上进行权衡：以 MSE 为主的优化更强调大误差惩罚，适合需要严格控制极端偏差的高风险场景；以 MAE 为主的优化则更注重整体平均水平的稳健性，更适合关注整体表现而对少数异常点容忍度较高的任务。  

在多元长程时间序列预测任务中，预测步长 \(H\) 较大、变量数 \(C\) 较多，MSE 与 MAE 在不同模型与结构下往往会出现明显分化：某些结构可能在 MSE 指标上表现突出（即显著抑制极端误差），而在 MAE 上优势有限；另一些结构则可能在 MAE 上更具优势但对峰值误差控制能力有限。正是基于这种分化特性，本文在后续章节中分别从 MSE 与 MAE 的角度对不同时频增强结构进行分析，并据此提出面向不同误差敏感度场景的模型选择建议。

## 2.2 大语言模型基础理论
大语言模型（Large Language Model, LLM）通常指参数规模达到亿级甚至百亿级以上、在大规模文本语料上通过自监督预训练得到的神经网络模型。典型代表包括以自回归语言建模为目标的 GPT 系列模型[Radford et al., 2019; Brown et al., 2020]，以及以掩码语言建模和序列到序列重构为目标的 BERT、BART、T5 等[Devlin et al., 2018; Lewis et al., 2019; Raffel et al., 2020]。这类模型以 Transformer 架构为基础[22]，通过在海量语料上学习词序列的条件分布，获得了强大的自然语言理解与生成能力，并在对话、翻译、摘要、问答等任务中取得了远超传统方法的性能。近年来，随着算力与数据规模的进一步提升，多模态大模型（Large Multimodal Model, LMM）将文本与图像、音频、时间序列等模态统一到共享表示空间中，使得跨模态理解与推理成为可能[Zhang et al., 2024d]。  

从一般形式上看，LLM 可以抽象为对离散 token 序列的条件概率建模：给定输入序列 \(\mathbf{w} = (w_1,\dots,w_L)\)，模型通过嵌入层与若干 Transformer 块，将每个 token 映射为隐藏表示 \(\mathbf{h}_t \in \mathbb{R}^d\)，并基于自注意力机制聚合上下文信息，从而估计下一个 token 的条件分布 \(p_\theta(w_{t+1} \mid w_{\leq t})\) 或被掩码 token 的条件分布 \(p_\theta(w_t \mid \mathbf{w}_{\setminus t})\)。预训练阶段通常通过最小化交叉熵损失  
\[
\mathcal{L}_{\text{LM}}(\theta) = - \sum_{t} \log p_\theta(w_t \mid \mathbf{w}_{< t}) \quad \text{或} \quad - \sum_{t\in \mathcal{M}} \log p_\theta(w_t \mid \mathbf{w}_{\setminus t}),
\]
在大规模语料上学习参数 \(\theta\)。得益于 Transformer 的全局自注意力结构，LLM 能够在单层中访问完整上下文，并通过多头机制建模不同语义子空间下的依赖关系，使其具备较强的长程依赖建模与语义抽象能力。预训练完成后，LLM 可以通过微调、指令调优（instruction tuning）、人类反馈强化学习（RLHF）等方式适配下游具体任务。  

针对时间序列等非文本模态，将 LLM 能力迁移过来的关键在于“模态对齐”与“表示桥接”。一方面，需要通过合适的 token 化、量化或编码方式，将连续数值序列 \(\mathbf{x}_{1:T}\) 映射为 LLM 可接受的离散 token 或嵌入序列，例如将数值离散化为符号、利用向量量化（VQ-VAE）构造代码本，或通过专门的时间序列编码器 \(g_\phi(\cdot)\) 将序列嵌入到与文本嵌入同维度的表示空间[Zhang et al., 2024d]；另一方面，需要在语言空间中保持或增强时间序列的语义结构，使得 LLM 在进行预测或推理时既能利用预训练中积累的世界知识，又不过度破坏数值间的精细关系。近年来，针对时间序列的 LLM 研究大致沿着直接提示（prompting）、数值量化（quantization）、模态对齐（aligning）、视觉桥接（vision as bridge）以及工具集成（tool integration）等方向展开，围绕如何在计算复杂度可控的前提下，高效利用 LLM 的表征与推理能力，构建统一的时序–语言建模框架[Zhang et al., 2024d]。本文后续方法即是在这一总体范式下，结合特定任务需求与频域/时域结构设计，对大语言模型在多元时间序列预测中的应用进行进一步探索。
### 2.2.1 Transformer 架构与预训练机制
Transformer 模型由 Vaswani 等人提出[22]，其核心思想是完全摒弃循环和卷积结构，改用基于自注意力（Self-Attention）的并行计算框架来建模序列中任意位置之间的依赖关系。标准 Transformer 主要由嵌入层、位置编码、多层堆叠的编码器（Encoder）和解码器（Decoder）构成，后续在语言建模和时间序列等任务中常采用仅编码器或仅解码器的变体。  

**（1）输入表示与位置编码。** 对于长度为 \(L\) 的离散 token 序列 \(\mathbf{w} = (w_1,\dots,w_L)\)，首先通过词嵌入矩阵 \(\mathbf{E}\in\mathbb{R}^{V\times d_{\text{model}}}\) 将每个 token 映射为 \(d_{\text{model}}\) 维的向量表示  
\[
\mathbf{e}_t = \mathbf{E}[w_t], \quad t=1,\dots,L,
\]
其中 \(V\) 为词表大小。由于自注意力机制本身不显式编码位置信息，Transformer 引入位置编码（Positional Encoding）\(\mathbf{p}_t\) 与词嵌入相加，得到最终输入表示  
\[
\mathbf{x}_t = \mathbf{e}_t + \mathbf{p}_t, \quad t=1,\dots,L。
\]
原始工作中采用了基于正余弦函数的固定位置编码：对第 \(t\) 个位置、第 \(2i\)、\(2i+1\) 维分别定义为  
\[
\mathrm{PE}(t,2i)   = \sin\!\left(\frac{t}{10000^{2i / d_{\text{model}}}}\right),\quad
\mathrm{PE}(t,2i+1) = \cos\!\left(\frac{t}{10000^{2i / d_{\text{model}}}}\right),
\]
从而在不同时间步与不同频率尺度上编码位置信息。后续研究中也广泛使用可学习位置编码或相对位置编码，以适应更复杂的序列结构。  

**（2）多头自注意力机制。** Transformer 的核心计算单元是多头自注意力（Multi-Head Self-Attention, MHSA）。对任意一层的输入序列表示 \(\mathbf{H} = [\mathbf{h}_1,\dots,\mathbf{h}_L]^\top \in \mathbb{R}^{L\times d_{\text{model}}}\)，首先通过线性变换得到查询（Query）、键（Key）、值（Value）矩阵：  
\[
\mathbf{Q} = \mathbf{H}\mathbf{W}^Q,\quad
\mathbf{K} = \mathbf{H}\mathbf{W}^K,\quad
\mathbf{V} = \mathbf{H}\mathbf{W}^V,
\]
其中 \(\mathbf{W}^Q,\mathbf{W}^K,\mathbf{W}^V \in \mathbb{R}^{d_{\text{model}}\times d_k}\) 为可学习参数。单头缩放点积注意力（Scaled Dot-Product Attention）的输出为  
\[
\mathrm{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \mathrm{Softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right)\mathbf{V},
\]
其中 \(\frac{1}{\sqrt{d_k}}\) 用于缓解高维内积导致的梯度不稳定问题。多头注意力通过并行计算 \(H\) 个头：  
\[
\mathrm{head}_i = \mathrm{Attention}(\mathbf{H}\mathbf{W}_i^Q,\mathbf{H}\mathbf{W}_i^K,\mathbf{H}\mathbf{W}_i^V), \quad i=1,\dots,H,
\]
再将各头输出在特征维上拼接并线性变换：  
\[
\mathrm{MultiHead}(\mathbf{H}) = \mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_H)\mathbf{W}^O,
\]
其中 \(\mathbf{W}^O \in \mathbb{R}^{Hd_v\times d_{\text{model}}}\)。多头机制允许模型在不同子空间中并行关注不同的依赖模式，如局部邻域关系、长程依赖或特定语义模式等。  

**（3）编码器–解码器结构与前馈网络。** 标准 Transformer 编码器由若干相同结构的层堆叠而成，每一层包含多头自注意力子层与位置前馈网络（Position-wise Feed-Forward Network, FFN）子层，并在每个子层外部配备残差连接与 LayerNorm：  
\[
\tilde{\mathbf{H}} = \mathrm{LayerNorm}\bigl(\mathbf{H} + \mathrm{MultiHead}(\mathbf{H})\bigr),
\]
\[
\mathbf{H}' = \mathrm{LayerNorm}\bigl(\tilde{\mathbf{H}} + \mathrm{FFN}(\tilde{\mathbf{H}})\bigr)。
\]
其中 FFN 对每个时间步的表示独立应用两层全连接与非线性激活：  
\[
\mathrm{FFN}(\mathbf{h}) = \sigma(\mathbf{h}\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2,
\]
\(\sigma(\cdot)\) 常选用 ReLU、GELU 等。解码器在此基础上引入掩码自注意力（Masked Self-Attention），通过对注意力矩阵施加上三角掩码，确保当前时刻只能访问过去的 token，用于自回归生成；同时还包含编码器–解码器注意力，用于在解码阶段从编码器输出中检索源序列信息。在许多语言建模与时间序列预测工作中，实际使用的是仅编码器（如 BERT）或仅解码器（如 GPT）结构。  

**（4）自回归与掩码语言建模预训练。** 在自然语言处理中，Transformer 主要通过两类预训练目标学习通用表征：  

- **自回归语言建模（Autoregressive LM）**：以 GPT 为代表，对给定前缀序列 \(\mathbf{w}_{<t}\) 预测下一个 token \(w_t\)，目标是最小化  
  \[
  \mathcal{L}_{\text{AR}}(\theta) = - \sum_{t=1}^{L} \log p_\theta(w_t \mid \mathbf{w}_{<t})。
  \]
  这种方式天然适合生成式任务，也便于迁移到时间序列等自回归预测问题。  

- **掩码语言建模（Masked LM）与序列到序列预训练**：以 BERT、BART、T5 等为代表，通过随机掩盖部分 token，要求模型根据上下文恢复被掩盖内容，或将损坏的输入序列重构为原始序列，其损失形式可写为  
  \[
  \mathcal{L}_{\text{MLM}}(\theta) = - \sum_{t\in\mathcal{M}} \log p_\theta(w_t \mid \mathbf{w}_{\setminus t}),
  \]
  其中 \(\mathcal{M}\) 为被掩码位置集合。此类目标强调对全局上下文的双向建模，有利于下游理解类任务。  

通过在大规模通用语料上的预训练，Transformer 模型能够学习到跨任务、跨领域可迁移的上下文表示。在此基础上，通过微调（fine-tuning）、指令调优（instruction tuning）和人类反馈强化学习（RLHF）等技术，进一步对齐模型行为与特定任务需求或人类偏好，从而形成当前广泛使用的大语言模型。对于时间序列任务而言，Transformer 架构提供了统一的序列建模基础，使得在适当的编码和对齐机制下，可以在数值序列上复用其强大的长程依赖建模与表征能力，这也是本文在后续章节中将 Qwen3 等大语言模型引入多元时间序列预测框架的基础。
### 2.2.2 Qwen3-1.4B 模型架构分析
本文采用的大语言模型基座为 Qwen3 系列中的小规模稠密模型变体 Qwen3-1.4B。整体上，Qwen3-1.4B 延续了 Qwen2.5 的 Transformer 解码器架构，并在注意力、归一化与位置编码等关键组件上进行了一系列针对性改进，以在较小参数规模下获得更强的表征能力和更稳定的训练特性[Qwen3 Tech Report, 2025]。  

在网络结构方面，Qwen3 系列稠密模型采用典型的单向自回归 Transformer 堆叠结构，由多层解码器块组成，每层包含多头注意力子层和前馈网络子层，并使用预归一化（pre-norm）形式的 RMSNorm 取代传统 LayerNorm，从而提升深层网络的数值稳定性与训练效率。多头注意力部分引入 Grouped Query Attention（GQA）机制，即查询头数大于键值头数（如 “Q/KV 头数比”为 16/8 或 32/8），通过共享部分键值头以减少注意力计算成本，在保持注意力表达能力的同时有效降低显存与计算开销。前馈网络则采用 SwiGLU 激活结构，相比标准的 ReLU/GeLU FFN，在相同参数量下具有更强的非线性表达能力和训练稳定性。  

在位置编码与长上下文建模方面，Qwen3 系列统一采用基于旋转位置编码（Rotary Positional Embedding, RoPE）的相对位置表示方式，通过在查询与键向量的低维子空间上施加旋转变换，将位置信息隐式融入注意力计算中，有利于捕捉长程依赖和周期结构。结合增广基频（ABF）、YARN 等技术，Qwen3 在较大模型上将最大上下文长度扩展至 32K/128K token，即便在小规模变体上，也能继承这一长上下文能力的设计思想，使其在处理长序列时间序列输入时具备良好的结构基础。  

在注意力归一化与稳定性方面，Qwen3 移除了早期版本中的 QKV bias，并在注意力模块中引入 QK-Norm，对查询与键向量进行额外的归一化处理，以减轻深层网络中注意力分布过于尖锐或梯度不稳定的问题。这一改动在大规模预训练实践中被证明有助于提升训练稳定性，特别是在长上下文与多语言混合数据条件下。全系列模型共享统一的 Qwen 分词器，采用字节级 BPE（BBPE）方案，词表规模约为 15 万，有利于同时覆盖多语言文本与代码等多种符号空间。  

总体而言，Qwen3-1.4B 虽然仅包含约十亿级别参数，但得益于 GQA、SwiGLU、RoPE、RMSNorm 以及 QK-Norm 等一系列架构改进，其在推理效率与表示能力之间取得了较好的平衡。对本文而言，将 Qwen3-1.4B 作为冻结的大语言模型骨干，通过外接适配器与跨模态对齐模块，将多元时间序列嵌入到其语义表示空间中，可以在较低计算成本下复用大模型在预训练阶段习得的通用知识与推理能力，为后续的时频域增强与门控注意力设计提供坚实的模型基础。

## 2.3 时间序列频域分析理论
时间序列不仅可以在时域上从“值随时间变化”的角度进行建模，也可以在频域上从“不同频率成分的能量分布”角度进行分析。频域方法通过将原始序列分解为一系列正弦/余弦基函数或其他频率基（如小波基），刻画序列中长期趋势、季节性周期以及高频波动等在频谱上的分布特征，有助于识别隐含的周期结构与多尺度模式。对于多元时间序列，通过在频域中分析不同变量在各频段上的能量与相干性，还可以更清晰地揭示跨通道的耦合关系。基于此，近年来大量长程时间序列预测工作开始引入傅里叶变换、小波变换以及频域神经网络等技术，将频域特征与时域深度模型相结合，以提升模型对长期周期性和复杂波动模式的刻画能力。  

### 2.3.1 傅里叶变换及其局限性
傅里叶变换是最经典的频域分析工具之一，其基本思想是将时间序列表示为一系列不同频率正弦和余弦函数的叠加。对于连续时间信号 \(x(t)\)，其连续傅里叶变换定义为  
\[
X(\omega) = \int_{-\infty}^{+\infty} x(t)\, e^{-j\omega t}\,\mathrm{d}t,
\]
其中 \(\omega\) 为角频率，\(X(\omega)\) 表示信号在频率 \(\omega\) 处的复数谱值。对于长度为 \(T\) 的离散时间序列 \(\{x_t\}_{t=0}^{T-1}\)，常用的离散傅里叶变换（Discrete Fourier Transform, DFT）定义为  
\[
X(k) = \sum_{t=0}^{T-1} x_t\, e^{-j 2\pi kt / T}, \quad k = 0,1,\dots,T-1,
\]
其逆变换为  
\[
x_t = \frac{1}{T}\sum_{k=0}^{T-1} X(k)\, e^{j 2\pi kt / T}, \quad t = 0,1,\dots,T-1。
\]
在实际应用中，快速傅里叶变换（Fast Fourier Transform, FFT）可以在 \(O(T\log T)\) 时间内高效计算 DFT，从而广泛应用于频谱估计、滤波和周期性分析等任务。通过观测幅度谱 \(|X(k)|\) 与功率谱 \(|X(k)|^2\)，可以识别序列中的主要频率成分及其能量分布，为后续的特征构造与模型设计提供重要先验。  

尽管傅里叶变换在刻画全局周期结构方面具有坚实的理论基础和良好性能，但其在处理真实复杂时间序列时也存在若干局限性：第一，傅里叶基函数在整个时间轴上具有全局支撑，频谱表示缺乏时间局部性，难以刻画“何时”发生频率变化，即对非平稳或局部突变序列的时变特性表达能力有限；第二，傅里叶变换本质上假设信号在分析窗口内是平稳或近似平稳的，当序列统计特性随时间显著变化时，单一全局频谱难以反映局部结构差异；第三，标准傅里叶变换主要刻画幅度与相位信息，对于具有显著非线性、多尺度和间歇性特征的复杂信号，需要配合额外的时频局部化工具（如短时傅里叶变换、小波变换、小波包分解等）才能更全面地刻画其结构；第四，在深度学习框架下，直接在频域上进行操作时，如何在频谱稀疏性、相位信息利用与可微性之间取得平衡，也会对模型设计提出额外挑战。  

基于上述原因，许多面向时间序列预测的频域方法往往并非仅依赖单一傅里叶变换，而是将其与小波（包）分解、频域前馈网络以及时域注意力/卷积结构相结合，一方面利用傅里叶频谱捕捉全局周期成分，另一方面通过更灵活的时频局部化手段增强对局部突变与多尺度结构的刻画。本文在第四、五章中分别引入小波包分解与频域前馈网络，正是试图在傅里叶频域分析的基础上，进一步缓解上述局限性，为多元长程时间序列预测提供更具表达力的时频建模框架。
### 2.3.2 小波包分解原理
为克服傅里叶变换缺乏时间局部性的不足，小波及其变体提供了一种兼具时域与频域局部化能力的时频分析工具。标准离散小波变换（Discrete Wavelet Transform, DWT）通过一组尺度函数 \(\varphi(t)\) 与小波函数 \(\psi(t)\) 在不同尺度 \(j\) 和位移 \(k\) 上的伸缩和平移，将信号分解为一系列近似系数和细节系数：  
\[
x(t) = \sum_{k} a_{J,k}\,\varphi_{J,k}(t) + \sum_{j=1}^{J}\sum_{k} d_{j,k}\,\psi_{j,k}(t),
\]
其中 \(\varphi_{J,k}(t) = 2^{-J/2}\varphi(2^{-J}t-k)\)、\(\psi_{j,k}(t) = 2^{-j/2}\psi(2^{-j}t-k)\)，系数 \(a_{J,k}\) 和 \(d_{j,k}\) 分别通过对原始信号与对应基函数的内积得到。对离散时间序列而言，DWT 可通过正交滤波器组实现：每一层对输入序列先后通过低通滤波器和高通滤波器，并在滤波后进行下采样，从而得到低频近似分量与高频细节分量。  

然而，标准小波变换在每一层仅对低频近似分量继续分解，而对高频细节分量不再细分，这在某些应用中会限制对高频结构的刻画能力。小波包分解（Wavelet Packet Decomposition, WPD）在此基础上进行推广：不仅对低频近似分量进行进一步分解，同时也对各层的高频细节分量进行同样的滤波与下采样操作，从而在每一层构建出一棵完备的二叉频带划分树。对于长度为 \(T\) 的离散序列 \(\{x_t\}_{t=0}^{T-1}\)，在第 \(j\) 层小波包分解中，可得到 \(2^j\) 个子带序列 \(\{w^{(j,b)}_t\}\)（\(b=0,\dots,2^j-1\)），每个子带对应原始频带的一个等宽或近似等宽子区间，实现对整体频谱的细粒度划分。与 DWT 相比，小波包通过“近似+细节”的双向递归分解，在相同分解层数下能获得更丰富的频带信息，更适合捕捉复杂信号中的多尺度高频与中频结构。  

在实现层面，小波包分解同样可以用滤波器组迭代实现：对当前所有子带序列同时应用一对低通/高通滤波器并下采样，生成下一层的子带系数，直到达到预设分解层数 \(J\)。对应的小波包重构则通过逆滤波与上采样操作，自叶子节点子带系数恢复到原始序列。对于多元时间序列，本研究对每个通道独立进行多层小波包分解，获得 \((N_{\text{sub}}\times C)\) 组子带序列，并在后续模型中将这些子带视作频率维度上的“多通道输入”，一方面在频域上显式区分不同频段的能量分布，突出与大误差相关的关键频带，另一方面在时域上保留子带系数的时间顺序，从而实现较好的时频局部化。正是基于小波包在刻画多尺度、高频细节与局部突变方面的优势，本文第四章在统一框架中引入小波包分解模块，并重点考察其对 MSE 等二阶误差指标的影响与机理。

### 2.3.3 频域神经网络原理
在频域视角下，时间序列可以通过离散傅里叶变换映射为一组复数频谱系数，从而将原本在时域中较难直接建模的长期周期性与全局模式转化为在频率轴上的能量分布与相位关系。传统深度模型多在时域上通过卷积或自注意力建模局部与长程依赖，而频域神经网络（尤其是频域多层感知机，Frequency-domain MLP, FreMLP）则尝试直接在频谱空间中进行特征变换与通道交互，利用频域的能量集中性（energy compaction）与全局视角，构建更高效的时序建模机制[FreTS]。  

一般而言，对长度为 \(L\) 的多元时间序列 \(\mathbf{X}\in\mathbb{R}^{B\times L\times C}\)（\(B\) 为批大小，\(C\) 为通道数），沿时间维进行一维快速傅里叶变换（FFT），可得到复数频谱表示  
\[
\mathbf{F} = \mathcal{F}(\mathbf{X}) = \mathrm{FFT}(\mathbf{X}, \text{dim}=1) \in \mathbb{C}^{B\times L'\times C},
\]
其中 \(L' = \lfloor L/2\rfloor + 1\)（对应实序列的半谱）。记 \(\mathbf{F} = \Re(\mathbf{F}) + j\,\Im(\mathbf{F})\)，则频域神经网络的核心思想是在实部与虚部上分别施加可学习的线性变换与非线性映射，从而在频域中显式建模不同变量之间、不同频率分量之间的联合关系。以本文采用的 FreMLP 形式为例，将 \(\mathbf{F}\) 视作形状为 \([B,L',C]\) 的复值张量，对每个频率位置上的通道维进行复线性变换：  
\[
\begin{aligned}
 \mathbf{O}_{\text{real}} &= \sigma\!\bigl(\Re(\mathbf{F})\,\mathbf{W}_r - \Im(\mathbf{F})\,\mathbf{W}_i + \mathbf{b}_r\bigr),\\
 \mathbf{O}_{\text{imag}} &= \sigma\!\bigl(\Im(\mathbf{F})\,\mathbf{W}_r + \Re(\mathbf{F})\,\mathbf{W}_i + \mathbf{b}_i\bigr),
\end{aligned}
\]
其中 \(\mathbf{W}_r,\mathbf{W}_i \in \mathbb{R}^{C\times C}\) 为可学习的实权重矩阵，\(\mathbf{b}_r,\mathbf{b}_i\in\mathbb{R}^{C}\) 为偏置，\(\sigma(\cdot)\) 为逐点非线性激活（如 ReLU）。上述结构等价于对每个频率上的通道向量施加一个复值仿射变换，从而在频域中显式建模变量间的全局交互。随后可将实部和虚部重新组合为复数：  
\[
\mathbf{G} = \mathbf{O}_{\text{real}} + j\,\mathbf{O}_{\text{imag}},
\]
并通过逆快速傅里叶变换（IFFT）回到时域：  
\[
\hat{\mathbf{X}} = \mathcal{F}^{-1}(\mathbf{G}) = \mathrm{IFFT}(\mathbf{G}, \text{dim}=1),
\]
从而得到经频域 MLP 变换后的时域特征。由于 FFT/IFFT 均为可微操作，上述过程可以无缝嵌入端到端深度网络中进行联合训练。  

为了增强频域特征的稀疏性与鲁棒性，实际实现中还可以在复值输出 \(\mathbf{G}\) 的实部与虚部分别施加诸如 soft-shrink 等稀疏化算子，对幅度较小的频谱系数进行压缩或截断，以抑制高频噪声与不重要的频带成分。结合注意力池化和门控机制，还可以在频域–时域之间引入自适应的权重分配，使模型能够根据预测步长、变量数等上下文信息动态调节频域增强的强度。本文第五章在统一框架中引入基于 FreMLP 的频域前馈网络，通过在频域中显式建模全局周期结构与跨通道交互，并在时域中与大语言模型分支及门控注意力机制进行融合，实验表明该类频域神经网络对 MAE 等平均误差指标具有更明显的优化效果，能够有效改善整体预测偏差水平。


## 2.4 门控注意力机制原理
### 2.4.1 传统注意力机制的局限性
自注意力机制（Scaled Dot-Product Attention, SDPA）是 Transformer 及大语言模型中的核心算子，其通过对查询（Query）、键（Key）、值（Value）之间的相似度进行加权求和来聚合上下文信息。对单个注意力头而言，给定查询矩阵 \(\mathbf{Q}\in\mathbb{R}^{L_q\times d_k}\)、键矩阵 \(\mathbf{K}\in\mathbb{R}^{L_k\times d_k}\)、值矩阵 \(\mathbf{V}\in\mathbb{R}^{L_k\times d_v}\)，标准缩放点积注意力可写为  
\[
\mathrm{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \underbrace{\mathrm{Softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right)}_{\mathbf{A}}\mathbf{V},
\]
其中 \(\mathbf{A}\in\mathbb{R}^{L_q\times L_k}\) 为注意力权重矩阵。尽管该机制在捕捉长程依赖方面表现出色，但在大模型和长上下文场景下也暴露出若干局限性：  

（1）**注意力分布过度集中的 “attention sink” 现象。** 实证研究表明，在预训练后的大模型中，某些特定位置（例如序列的首 token 或部分标点等“非信息性” token）容易在多层多头注意力中获得异常高的注意力权重，即“注意力汇聚”或“attention sink”现象[Gu et al., 2024]。这会导致大量上下文信息被无效地聚集到少数位置，降低注意力在有用上下文间的分辨能力，影响模型对细粒度模式的建模。  

（2）**线性变换 + Softmax 的表达能力与稳定性瓶颈。** 标准注意力在 \(\mathbf{Q}\mathbf{K}^\top\) 之后仅通过 Softmax 引入单一的归一化非线性，缺乏更灵活的输入依赖变换空间。一方面，这可能限制模型对复杂依赖关系的刻画；另一方面，在大规模训练（特别是 BF16 等低精度训练）时，注意力输出与残差流中容易出现“massive activation”（极大激活值），增加数值不稳定与梯度爆炸风险[Sun et al., 2024]。  

（3）**难以自适应过滤无关上下文。** 传统注意力中，每个头在给定层内的 token 交互模式仅由 \(\mathbf{Q}\mathbf{K}^\top\) 和 Softmax 决定，虽然注意力权重本身依赖输入，但缺乏显式的、可解释的“门控”机制来对不同头、不同位置的输出进行幅度调节，从而在需要时主动压制与当前查询无关的上下文贡献。尤其在长上下文和多模态融合场景下，如何在不破坏有用依赖的前提下抑制冗余信息，是标准注意力难以直接解决的问题。  

（4）**上下文长度扩展时的适配困难。** 在通过 RoPE 频率缩放、插值等技术扩展上下文长度时，原有注意力分布（尤其是 attention sink 模式）可能与新的位置编码不匹配，导致性能在长上下文下明显退化[Chen et al., 2023; Peng et al., 2023]。缺乏显式控制信息流的手段，使得模型在“零样本扩展”到更长序列时难以及时调整注意力模式。  

综合来看，传统注意力的这些局限性在大模型、长序列和多模态任务中尤为突出。引入门控机制，在保持原有注意力结构优势的基础上，对注意力输出进行输入依赖、头依赖的显式调制，有望缓解注意力汇聚、数值不稳定和冗余信息干扰等问题。  

### 2.4.2 门控注意力的数学原理与优势
门控注意力（Gated Attention）在标准注意力输出之上引入可学习的门控函数，通过对不同位置、不同注意力头的输出施加输入依赖的缩放因子，实现对信息流的精细控制。以对缩放点积注意力输出进行逐元素门控为例，记标准注意力输出为  
\[
\mathbf{H} = \mathrm{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V}) \in \mathbb{R}^{L_q\times d_v},
\]
门控注意力在此基础上引入一个非线性映射  
\[
\mathbf{G} = \sigma\bigl(f_{\phi}(\mathbf{X})\bigr) \in (0,1)^{L_q\times d_v},
\]
并得到门控后的输出  
\[
\tilde{\mathbf{H}} = \mathbf{G} \odot \mathbf{H},
\]
其中 \(\mathbf{X}\) 通常取为当前层的查询输入或其线性变换，\(f_{\phi}(\cdot)\) 为带参数的前馈网络或线性映射，\(\sigma(\cdot)\) 为 Sigmoid 等压缩到 \((0,1)\) 区间的非线性函数，\(\odot\) 表示逐元素乘法。对于多头注意力，可在每个头上分别引入独立的门控：  
\[
\tilde{\mathbf{H}}^{(h)} = \mathbf{G}^{(h)} \odot \mathbf{H}^{(h)}, \quad h=1,\dots,H,
\]
其中 \(\mathbf{G}^{(h)}\) 由对应头的输入独立生成，从而实现**头特定（head-specific）**的门控调制。  

从数学与优化角度看，门控注意力具有以下几个关键优势：  

（1）**引入输入依赖的非线性与稀疏性。** 通过 \(\mathbf{G}=\sigma(f_{\phi}(\mathbf{X}))\) 与逐元素乘法，门控在原有线性 + Softmax 结构之外增加了一层输入依赖的非线性映射。当 Sigmoid 输出接近 0 时，对应位置的注意力输出被显著抑制；当接近 1 时，则基本保留原始信息。实证结果表明，良好的门控设计往往会学习到**稀疏的门控分布**（大部分位置门控值接近 0，小部分接近 1），从而在不改变注意力权重归一化性质的前提下，实现对冗余信息的 “软选择” 和过滤。  

（2）**缓解 attention sink 并降低大激活值。** 将门控作用于 SDPA 输出（而非仅作用于值向量）时，门控分数直接控制每个查询位置对聚合上下文的“接收强度”。实验表明，**基于查询的、头特定的门控**可以显著降低分配给首 token 等 sink 位置的注意力得分，并削弱残差流中的 massive activation，从而减轻 attention sink 现象并提升数值稳定性[Gu et al., 2024]。与之相对，仅对值向量或共享门控进行调制时，往往难以同时兼顾稀疏性与性能。  

（3）**提升长上下文扩展能力与鲁棒性。** 在上下文长度扩展场景中，带门控的注意力模型对 RoPE 等位置编码变化更不敏感：由于信息流的强弱主要由输入依赖的门控分数控制，而非过度依赖特定位置的注意力模式，模型在零样本扩展到更长上下文时往往能保持更平滑的性能退化甚至在长上下文段上取得更优表现。这一点在大规模实验中得到验证：在相同训练长度下，加入门控的模型在 64K、128K 等超长上下文上的表现明显优于未门控基线。  

（4）**与其他模块易于组合并保持计算友好。** 就实现而言，门控注意力仅在标准注意力输出之后增加一个小型前馈网络与逐元素乘法，不改变注意力权重计算的复杂度，也不会破坏现有 Transformer 框架的并行性。门控结构可以与多模态对齐、频域增强、残差连接等模块自然组合，形成在时域、频域与语义空间中多层次受控的信息流。  

在本文中，我们在基于 Qwen3 的统一改进框架中引入门控注意力池化和门控 Transformer 编码层，其核心思想与上述门控注意力一致：一方面在时间序列–语言跨模态对齐与池化阶段，通过门控模块抑制冗余头和冗余时间步的贡献；另一方面在时域与频域分支内，通过门控机制控制不同尺度特征在残差流中的注入强度，从而在保证表达能力的同时提升训练稳定性，并为后续 MSE/MAE 分化机理分析提供结构基础。

## 2.5 数据集介绍
为了系统评估本文提出的多元长程时间序列预测框架及其各个时频增强模块，本章选取了覆盖工业能耗、电力负荷、气象环境、公共卫生与金融市场等多种典型场景的公开基准数据集，并在统一的划分与预处理策略下进行对比实验。合理的数据集选择与规范的数据处理流程对于保证实验结论的客观性与可复现性至关重要，因此本节首先对所使用的数据集进行简要描述，随后介绍具体的数据划分方式与预处理步骤。
### 2.5.1 数据集描述
本文在多元长程时间序列预测领域具有代表性的公开基准数据集上对所提出方法进行评估，主要包括 ETTm1、ETTm2、ETTh1、ETTh2、ECL、Weather、ILI 和 Exchange 八个数据集。这些数据集覆盖了工业能耗、电力负荷、气象环境、公共卫生和金融市场等多个典型应用场景，具有不同的采样频率、变量维度和序列长度配置，有助于全面检验模型在不同类型时序任务中的泛化能力。  

- **ETT 系列（ETTm1、ETTm2、ETTh1、ETTh2）**：Electricity Transformer Temperature（ETT）数据集来源于电力变压器场景，记录了油温（Oil Temperature）以及一组相关负荷特征（如负荷率、环境温度等）随时间变化的多元时间序列。ETTm1 与 ETTm2 为**分钟级采样**（15 min 级别），分别使用不同时间段的数据；ETTh1 与 ETTh2 为**小时级采样**，同样对应不同时间区间。四个子数据集均包含 7 个变量，通常将其中的油温作为预测目标，其余变量作为外生特征，广泛用于评估模型在工业场景下对长期趋势、多尺度周期性及负荷波动的建模能力。  

- **ECL（Electricity Consumption Load）**：ECL 数据集来自某地区 321 个用户（或电表）的**小时级**用电负荷记录，每个通道对应一个用户的用电量。该数据集维度较高、存在明显的日周期与周周期结构，同时伴随节假日等外部因素导致的异常波动，常用于检验模型在高维多变量电力负荷预测任务中的表现，尤其是对季节性与突变混合模式的刻画能力。  

- **Weather**：Weather 数据集收集了气象站点在一段时间内的多种气象要素观测，包括温度、湿度、气压、风速、风向等二十余个变量，采样频率通常为**小时级**。该数据集具有明显的季节性、日周期与多变量相关性，是评估模型在气象环境类多元时间序列预测任务中综合性能的重要基准，尤其适合检验模型在长预测步长下对复杂周期性与缓慢演化趋势的建模能力。  

- **ILI（Influenza-like Illness）**：ILI 数据集记录了类流感就诊比例（Influenza-like Illness Rate）等公共卫生指标的**周度**时间序列，维度相对较低但序列较长，且受季节性流行、突发疫情及防控措施等多种因素影响。该数据集常用于评估模型在医疗与公共卫生场景中对长期季节性波动与异常峰值的预测能力，尤其适合观察不同模型在大误差控制与平均误差水平上的差异。  

- **Exchange**：Exchange 数据集包含多个主要货币（如欧元、英镑、日元等）对美元的**日度**汇率时间序列，每个通道对应一种货币对。该数据集波动性较强，受宏观经济与金融事件影响显著，序列中既包含中长期趋势变化，又存在大量短期噪声与局部峰值，是检验模型在金融场景中对非平稳、多尺度波动建模能力的经典数据集。  

在后续实验中，本文遵循主流工作对上述数据集进行标准化划分（训练集、验证集、测试集）与归一化预处理，具体划分比例与预处理细节将在第 2.5.2 小节中进行说明。
### 2.5.2 数据预处理方法
为保证不同模型在各数据集上的对比公平性，本文在数据预处理阶段遵循长程时间序列预测（LTSF）领域的主流设置，对各数据集采用统一的时间划分、归一化和窗口切分策略。  

**（1）时间顺序划分训练集、验证集与测试集。** 对每个数据集，首先按照时间顺序将原始序列划分为训练集、验证集与测试集，确保验证集和测试集的样本始终位于训练集之后，避免未来信息“泄漏”到训练过程。划分比例与公开基线工作保持一致：对于 ETT 系列、电力与气象数据集（ETTm1/ETTm2/ETTh1/ETTh2、ECL、Weather），采用“前段数据用于训练、中段用于验证、末段用于测试”的固定比例划分；对于 ILI 与 Exchange 等较长单序列数据集，同样按照时间先后划分为训练、验证和测试三段，以保证评估环节始终在“未见过的未来”上进行。  

**（2）按变量维度进行可逆归一化（RevIN）。** 为消除不同变量量纲和数值尺度差异的影响，本文在模型内部集成了可逆实例归一化（Reversible Instance Normalization, RevIN）模块，对输入时间序列按变量维度进行标准化处理。具体而言，在训练阶段，对每个样本 \(\mathbf{X}\in\mathbb{R}^{L\times C}\) 计算其在时间维上的均值 \(\boldsymbol{\mu}\in\mathbb{R}^{1\times C}\) 与标准差 \(\boldsymbol{\sigma}\in\mathbb{R}^{1\times C}\)，并进行归一化：  
\[
\mathbf{X}^{\text{norm}} = \frac{\mathbf{X} - \boldsymbol{\mu}}{\boldsymbol{\sigma}+\varepsilon},
\]
其中 \(\varepsilon\) 为数值稳定性常数。模型完成预测后，再通过反归一化操作恢复到原始尺度：  
\[
\hat{\mathbf{Y}} = \mathbf{Y}^{\text{norm}} \odot \boldsymbol{\sigma} + \boldsymbol{\mu},
\]
从而保证损失计算与评价指标（MSE、MAE）均在物理量意义明确的原始空间中进行。RevIN 的引入不仅降低了不同变量间尺度差异带来的训练困难，也在一定程度上缓解了数据分布漂移对模型性能的影响。  

**（3）滑动窗口切分与预测设置。** 在构造训练样本时，本文采用滑动窗口方式从长序列中抽取输入–输出片段对。设历史窗口长度为 \(L\)、预测步长为 \(H\)，则对每个数据集从训练序列中构造形如 \((\mathbf{X}_{t-L+1:t}, \mathbf{X}_{t+1:t+H})\) 的样本对，其中 \(\mathbf{X}_{t-L+1:t}\in\mathbb{R}^{L\times C}\) 作为模型输入，\(\mathbf{X}_{t+1:t+H}\in\mathbb{R}^{H\times C}\) 作为监督信号。长程预测实验中，本文主要采用 \(L=96\)，\(H\in\{96,192,336,720\}\) 等常见配置，与 ETT、ECL、Weather、ILI、Exchange 等数据集上的主流工作保持一致，用以评估模型在不同预测跨度下的表现。  

**（4）时间戳与外部特征处理。** 对于提供时间戳或日期信息的数据集（如 ETT、ECL、Weather 等），本文按照现有基线做法，将时间戳映射为小时、星期、月份等周期性时间特征，并通过 one-hot 或正余弦编码形式作为额外输入；对于 ILI、Exchange 等未显式提供额外时间标记的数据集，仅使用序列本身进行建模。所有预处理步骤均不引入未来时刻的信息，确保训练与评估过程满足因果性要求。  

通过上述统一的预处理流程，本文在不同数据集与模型间保证了评估设置的一致性，使得第四章与第五章中关于小波包分解与频域前馈网络在 MSE/MAE 指标上的表现差异与机理分析具有可比性与可复现性。

# 第三章 基于 Qwen3 与门控注意力的统一改进框架

## 3.1 整体架构设计
本章提出的统一改进框架面向多元长程时间序列预测任务，在整体设计上采用“**时域–频域–大语言模型语义域**”三分支结构，并通过门控注意力与跨模态对齐模块实现多分支信息的统一融合。整体上，框架由输入归一化与滑动窗口模块、时域分支、频域增强分支（第四、五章分别实例化为小波包分解与频域前馈网络）、大语言模型语义分支（基于冻结 Qwen3-1.4B）、跨模态注意力融合模块以及共享解码与预测头组成。  

设输入为长度为 \(L\)、变量数为 \(C\) 的多元时间序列片段 \(\mathbf{X}\in\mathbb{R}^{B\times L\times C}\)，对应的 LLM 嵌入为 \(\mathbf{E}\in\mathbb{R}^{B\times d_{\text{LLM}}\times C}\)，预测步长为 \(H\)。在第 2.5 节介绍的滑动窗口与 RevIN 预处理后，得到归一化序列 \(\mathbf{X}^{\text{norm}}\in\mathbb{R}^{B\times L\times C}\)，并将其转置为节点优先形式 \(\mathbf{X}^{\prime}\in\mathbb{R}^{B\times C\times L}\)，以便将每个变量视作一个“节点”。整体架构的数据流可概括如下：  

（1）**时域分支（Time-domain Branch）**：时域分支以 \(\mathbf{X}^{\prime}\) 为输入，通过长度–通道映射层和若干层带门控的 Transformer 编码器提取跨时间的序列特征。具体地，首先将时间维映射到固定的通道维 \(d_{\text{ts}}\)：  
\[
\mathbf{H}^{\text{time}}_0 = \mathrm{Linear}_{L\rightarrow d_{\text{ts}}}(\mathbf{X}^{\prime}) \in \mathbb{R}^{B\times C\times d_{\text{ts}}},
\]
随后堆叠 \(E\) 层门控 Transformer 编码器层  
\[
\mathbf{H}^{\text{time}}_{l+1} = \mathrm{Gated\mbox{-}TFEncoder}_l(\mathbf{H}^{\text{time}}_{l}),\quad l=0,\dots,E-1,
\]
得到时域编码特征 \(\mathbf{H}^{\text{time}}_E \in \mathbb{R}^{B\times C\times d_{\text{ts}}}\)。其中，每一层内部通过自注意力与基于查询的门控机制（第 2.4 节）抑制无关时间步的贡献，增强对关键时间片的刻画。  

（2）**频域增强分支（Frequency-domain Branch）**：频域分支在统一框架下支持不同形式的时频增强模块：在第四章中实例化为小波包分解 + 门控编码器，在第五章中实例化为 FreMLP 频域前馈网络。抽象地，频域分支从 \(\mathbf{X}^{\prime}\) 中提取频域或时频特征 \(\mathbf{H}^{\text{freq}}\in\mathbb{R}^{B\times C\times d_{\text{freq}}}\)：  
\[
\mathbf{H}^{\text{freq}} = \Phi_{\text{freq}}(\mathbf{X}^{\prime};\,\psi),
\]
其中 \(\Phi_{\text{freq}}\) 表示频域处理算子，\(\psi\) 为其内部参数。在小波包版本中，\(\Phi_{\text{freq}}\) 首先对每个变量通道独立执行多层小波包分解，得到多子带序列，再通过门控 Transformer 编码器和池化操作将各子带特征聚合为固定维度表示；在频域 MLP 版本中，\(\Phi_{\text{freq}}\) 先沿时间维进行 FFT，将序列映射至复数频谱空间，再通过 FreMLP 在频谱通道维上施加复线性变换与非线性激活，并结合 soft-shrink 稀疏化与 IFFT 回到时域，最终通过注意力池化和门控编码器获得全局频域特征。得到的 \(\mathbf{H}^{\text{freq}}\) 在后续与时域特征进行融合，用以增强模型对周期性结构与整体能量分布的感知。  

（3）**大语言模型语义分支（LLM Semantic Branch）**：语义分支以冻结的大语言模型 Qwen3-1.4B 作为骨干，输入为针对每个变量构造的文本 Prompt，经预训练 LLM 处理后得到变量级语义嵌入 \(\mathbf{E}\in\mathbb{R}^{B\times d_{\text{LLM}}\times C}\)。在统一框架实现中，首先对 \(\mathbf{E}\) 做维度变换  
\[
\mathbf{E}' = \mathbf{E}^{\top} \in \mathbb{R}^{B\times C\times d_{\text{LLM}}},
\]
再通过若干层门控 Transformer 编码器进行轻量级适配，得到增强后的语义特征 \(\mathbf{H}^{\text{sem}}\in\mathbb{R}^{B\times C\times d_{\text{LLM}}}\)。这一分支承载了 LLM 在预训练阶段习得的通用语义与跨任务知识，为时间序列变量引入额外的先验信息。  

（4）**时频分支融合与门控机制**：在时域分支与频域分支提取出 \(\mathbf{H}^{\text{time}}_E\) 与 \(\mathbf{H}^{\text{freq}}\) 后，需要在变量–时间尺度上对两者进行自适应融合。本文采用门控融合模块，将时域特征、频域特征以及与预测步长相关的简单先验（如归一化的预测长度标量）拼接后，通过一小型前馈网络生成门控权重 \(\mathbf{G}^{\text{fusion}}\)：  
\[
\mathbf{Z} = \mathrm{Concat}\bigl(\mathbf{H}^{\text{time}}_E,\mathbf{H}^{\text{freq}},\mathbf{h}^{\text{horizon}}\bigr), \quad
\mathbf{G}^{\text{fusion}} = \sigma\bigl(f_{\text{fusion}}(\mathbf{Z})\bigr),
\]
得到融合后的时频特征  
\[
\mathbf{H}^{\text{tf}} = \mathbf{H}^{\text{time}}_E + \mathbf{G}^{\text{fusion}} \odot \mathbf{H}^{\text{freq}} \in \mathbb{R}^{B\times C\times d_{\text{ts}}}.
\]
其中，\(\mathbf{h}^{\text{horizon}}\) 为在变量维广播的预测步长编码，\(\sigma(\cdot)\) 为 Sigmoid 函数，\(f_{\text{fusion}}\) 为两层 MLP。通过该门控机制，模型可以根据预测跨度及局部上下文，自适应调整频域增强的注入强度，实现对不同比例时频信息的权衡。  

（5）**跨模态注意力对齐（Cross-modal Alignment）**：为将时频特征 \(\mathbf{H}^{\text{tf}}\) 与 LLM 语义特征 \(\mathbf{H}^{\text{sem}}\) 对齐，本文借鉴 TimeCMA 等工作的设计，引入跨模态注意力模块（CrossModal Attention, CMA）。在实现中，将 \(\mathbf{H}^{\text{tf}}\) 视作查询序列，将 \(\mathbf{H}^{\text{sem}}\) 视作键值序列，通过多层单头或多头跨模态注意力：  
\[
\mathbf{H}^{\text{cma}} = \mathrm{CMA}(\mathbf{H}^{\text{tf}}, \mathbf{H}^{\text{sem}}, \mathbf{H}^{\text{sem}})\in\mathbb{R}^{B\times C\times d_{\text{tf}}},
\]
并进一步通过多头输出的自适应加权（Adaptive Dynamic Heads）与残差插值参数 \(\boldsymbol{\alpha}\) 进行融合：  
\[
\mathbf{H}^{\text{cross}} = \boldsymbol{\alpha}\odot \mathbf{H}^{\text{cma}} + (1-\boldsymbol{\alpha})\odot \mathbf{H}^{\text{tf}},
\]
其中 \(\boldsymbol{\alpha}\in(0,1)^{1\times d_{\text{tf}}\times 1}\) 为可学习向量。该设计一方面利用 LLM 语义空间对时序变量进行重加权与关联建模，另一方面保持原始时频特征的稳定性与可控性。  

（6）**解码与预测头**：在跨模态对齐后，\(\mathbf{H}^{\text{cross}}\) 被视作变量维上的“编码表示”，通过若干层 Transformer 解码器捕捉变量间与时间步间的高层交互，然后经线性映射恢复到预测步长维度：  
\[
\mathbf{Z}^{\text{dec}} = \mathrm{TFDecoder}(\mathbf{H}^{\text{cross}}, \mathbf{H}^{\text{cross}}),\quad
\hat{\mathbf{Y}}^{\text{norm}} = \mathrm{Linear}_{d_{\text{ts}}\rightarrow H}(\mathbf{Z}^{\text{dec}}),
\]
最后通过 RevIN 反归一化得到最终预测 \(\hat{\mathbf{Y}}\in\mathbb{R}^{B\times H\times C}\)。训练阶段以 MSE 或 MAE 为损失函数，优化参数集合 \(\Theta = \{\theta_{\text{time}},\theta_{\text{freq}},\theta_{\text{fusion}},\theta_{\text{CMA}},\theta_{\text{dec}}\}\)，其中 Qwen3 参数保持冻结，仅通过外接适配器与注意力模块进行利用。  

综上所述，本章提出的统一改进框架在结构上将**时域建模、频域增强与大语言模型语义表征**有机结合，并通过门控注意力和跨模态对齐机制实现多源信息的统一融合作用于多元长程时间序列预测。第四章和第五章将在该框架下分别实例化小波包分解增强与频域前馈网络增强两种频域模块，并通过第六章对二者在 MSE/MAE 指标下的表现分化与机理进行系统分析。

## 3.2 冻结大模型的适配器设计
在统一改进框架中，本文采用冻结的大语言模型 Qwen3-1.4B 作为语义表征骨干，仅在其外部堆叠轻量级适配器（adapter）与跨模态对齐模块，以避免对超大规模参数进行端到端微调所带来的训练成本与稳定性问题。一方面，通过时间序列侧的编码网络与频域增强模块，构建适配于 LLM 语义空间的时频特征；另一方面，通过 LLM 侧的 Prompt 编码器和跨模态注意力模块，将时间序列特征与 Qwen3 的语义嵌入进行对齐与融合，从而在冻结大模型参数的前提下，充分利用其预训练知识服务于多元时间序列预测任务。  

### 3.2.1 跨模态特征投影与维度对齐
在跨模态对齐之前，时间序列分支与 LLM 分支各自处于不同的表示空间与张量形状。时间序列输入经 RevIN 归一化与维度变换后，得到节点优先形式 \(\mathbf{X}'\in\mathbb{R}^{B\times C\times L}\)；时域/频域分支分别输出统一通道维度的特征 \(\mathbf{H}^{\text{time}},\mathbf{H}^{\text{freq}}\in\mathbb{R}^{B\times C\times d_{\text{ts}}}\)，经门控融合后得到时频特征 \(\mathbf{H}^{\text{tf}}\in\mathbb{R}^{B\times C\times d_{\text{ts}}}\)。与此同时，冻结的 Qwen3 接收针对每个变量构造的文本 Prompt，输出变量级语义嵌入 \(\mathbf{E}\in\mathbb{R}^{B\times d_{\text{LLM}}\times C}\)，经转置后为 \(\mathbf{E}'\in\mathbb{R}^{B\times C\times d_{\text{LLM}}}\)。  

为在统一空间中进行注意力对齐，本文采用“两步式”对齐策略：  
- 一方面，在时间序列侧，通过 `length_to_feature`、`fre_projection` 等线性投影，将长度维度或频域输入映射到固定通道维 \(d_{\text{ts}}\)，保证所有时频特征在变量维上的形状统一为 \([B,C,d_{\text{ts}}]\)，便于后续与 LLM 语义特征在变量维逐通道对齐；  
- 另一方面，在 LLM 侧，通过轻量级的门控 Transformer 编码器（`prompt_encoder`）对 \(\mathbf{E}'\) 做适配，得到形状为 \([B,C,d_{\text{LLM}}]\) 的语义特征 \(\mathbf{H}^{\text{sem}}\)，再根据跨模态注意力模块 `CrossModal` 的设计，将其视为键/值序列，时频特征 \(\mathbf{H}^{\text{tf}}\) 视为查询序列。  

具体而言，跨模态注意力实现为  
\[
\mathbf{H}^{\text{cma}} = \mathrm{CMA}\bigl(\mathbf{H}^{\text{tf}},\mathbf{H}^{\text{sem}},\mathbf{H}^{\text{sem}}\bigr),
\]
在实现上通过多层 `TSTEncoderLayer` 形式的 Transformer 编码器完成，其中时间序列通道数 \(C\) 被视作“序列长度”，LLM 语义维度或其子空间作为通道维。为进一步提升跨头表示能力，框架引入多头 CMA 分支与自适应头融合模块 `AdaptiveDynamicHeadsCMA`，对不同头输出的变量级特征进行加权求和，得到最终的跨模态对齐表示 \(\mathbf{H}^{\text{cma}}\in\mathbb{R}^{B\times C\times d_{\text{ts}}}\)，并通过可学习权重 \(\boldsymbol{\alpha}\) 与原始时频特征 \(\mathbf{H}^{\text{tf}}\) 做残差插值，形成既保留时频结构又注入语义信息的联合表示。  

### 3.2.2 时序位置编码与语义嵌入设计
在时间序列分支中，时间顺序信息主要通过两种方式编码：其一，输入张量的维度排列始终保留真实的时间顺序（如 \([B,L,N]\)、\([B,N,L]\)），并在 `length_to_feature` / `fre_projection` 等线性映射中显式区分时间维与通道维，使 Transformer 编码器沿着“时间轴”进行自注意力计算；其二，在部分辅助模块中可引入正余弦位置编码或坐标型位置编码，对长度维进行显式编码，以增强模型对长程位置信息与相对位置关系的感知能力（第 2.1 节已给出相关形式）。  

在 LLM 语义分支中，Qwen3 的位置编码与语义嵌入由预训练模型内部完成：输入 Prompt 先经 Qwen3 的分词器映射为离散 token，再由内部的嵌入层和 RoPE 位置编码生成 token 级表示，经多层解码器堆叠后形成全局语义表征。本文不修改 Qwen3 内部的嵌入与位置编码模块，而是通过外部 `prompt_encoder` 对输出的变量级嵌入进行轻量级时序/变量结构适配：  
- 在变量维上，通过多头自注意力编码不同变量（如不同传感器、货币、地区）之间的相关性，使 LLM 语义向量更好地匹配时间序列的结构；  
- 在序列维度上，通过与时间序列分支的跨模态注意力对齐，使得 LLM 语义嵌入在统一框架中，扮演“高层语义先验”与“跨通道关系调节器”的角色，而非直接参与数值预测。  

这种设计使得 Qwen3 在语义空间内保持其原有的长上下文与多语言能力，而适配器仅负责在“变量–时间结构”和“语义空间”之间搭建桥梁，避免对大模型内部位置编码与嵌入机制做侵入式修改。  

### 3.2.3 适配器参数量与训练效率分析
与直接微调整个大语言模型不同，本文采用“冻结 Qwen3 + 训练外部适配器与时频分支”的方式，将可训练参数主要集中在以下几个部分：  
- 时间序列侧：RevIN 归一化参数、`length_to_feature` / `fre_projection` 等线性投影、时域门控 Transformer 编码器、WPD 或 FreMLP 等频域增强模块及其门控/池化层；  
- LLM 侧：`prompt_encoder` 中的少量门控 Transformer 层，用于对 Qwen3 输出嵌入做结构适配；  
- 跨模态与解码侧：`CrossModal` 跨模态注意力层、多头自适应融合模块 `AdaptiveDynamicHeadsCMA`、残差权重 \(\boldsymbol{\alpha}\) 以及下游 Transformer 解码器与预测头。  

从数量级上看，这些适配器与时频分支的参数量远小于 Qwen3-1.4B 本身的参数规模（通常仅占总参数量的极小一部分），带来以下几方面的训练效率优势：  
- **显著降低显存与计算开销**：冻结 Qwen3 后，其前向传播仍需一定资源，但无需存储反向梯度和二阶统计信息，显著减少训练显存占用；反向传播仅在适配器与时频分支上进行，使单卡可承载更大的 batch size 或更长的输入序列。  
- **加快收敛与减少调参成本**：适配器层通常深度较浅、结构相对简单，在给定预训练 LLM 语义先验的情况下，可以在较少训练轮数内收敛到较优解，避免对超大模型学习率、正则化等超参数进行复杂调优。  
- **便于模块化扩展与对比**：在统一框架下更换频域模块（如从 WPD 切换到 FreMLP）或调整门控策略，仅需改变部分子网络结构与参数，不影响 Qwen3 本体与其他子模块，便于开展系统的消融实验与 MSE/MAE 指标分化分析。  

综合而言，通过“冻结大模型 + 轻量适配器”的设计，本文在充分利用 Qwen3 语义能力的同时，将学习负担集中在一组参数规模可控、结构清晰的时序/频域/跨模态模块上，实现了在训练效率、资源开销与模型性能之间的较优平衡。

## 3.3 门控注意力池化模块

在统一改进框架中，时域与频域分支会输出长度不等的序列表示（如每个频段子带对应一段时序、或 FreMLP 后的时域序列），而跨模态对齐与解码器需要固定维度的变量级表示。门控注意力池化模块承担两项职责：其一，在时间或频段维度上通过注意力加权将变长序列压缩为定长向量，即**注意力池化**；其二，在池化前后或在对多路表示进行融合时引入**门控机制**，使池化与融合过程具有输入依赖性，从而抑制冗余信息、突出关键时间步或关键频段/头的贡献，与第 2.4 节所述门控注意力的思想一致[门控注意力论文]。  

**（1）注意力加权池化。** 设某一分支输出的序列表示为 \(\mathbf{H}\in\mathbb{R}^{B\times L\times C}\)（\(B\) 为 batch、\(L\) 为序列长度、\(C\) 为通道维），需在长度维 \(L\) 上聚合为单一向量。采用可学习的注意力池化：先通过小型前馈网络将每个时间步映射为标量得分，再经 Softmax 归一化得到注意力权重，最后对序列做加权求和：  
\[
\mathbf{a} = \mathrm{Softmax}\bigl(f_{\text{attn}}(\mathbf{H})\bigr)\in\mathbb{R}^{B\times L\times 1},\quad
\mathbf{h}_{\text{pooled}} = \sum_{t=1}^{L} a_t\,\mathbf{H}_{:,t,:} \in \mathbb{R}^{B\times C}.
\]
其中 \(f_{\text{attn}}\) 通常为两层 MLP（如 \(\mathrm{Linear}(d\to d/2)\to\mathrm{ReLU}\to\mathrm{Linear}(d/2\to 1)\)）。在小波包版本中，对每个子带分别进行上述注意力池化，得到各频段向量后再通过可学习的频段权重（node_weights）进行加权融合，使不同频段对最终表示的贡献可随数据与任务自适应；在频域 MLP 版本中，对 FreMLP 输出的时域序列直接做一次注意力池化，得到变量级全局表示后再送入后续门控编码器与跨模态模块。  

**（2）门控机制与 SDPA 输出门控。** 在进入池化之前，时域与频域分支内部均采用带门控的 Transformer 编码层（GatedTransformerEncoderLayer）。对每一层，记归一化后的输入为 \(\tilde{\mathbf{X}}\)，自注意力输出为 \(\mathbf{O}_{\text{attn}}\)，则门控后的输出为  
\[
\mathbf{G} = \sigma\bigl(\mathbf{W}_g\,\tilde{\mathbf{X}}\bigr),\quad
\mathbf{O}_{\text{out}} = \mathbf{O}_{\text{attn}} \odot \mathbf{G},
\]
其中 \(\mathbf{W}_g\) 为可学习线性层，\(\sigma\) 为 Sigmoid 函数，\(\odot\) 为逐元素乘法。该形式与门控注意力文献中“在 SDPA 输出后施加头特定、逐元素的可乘性门控”的设计一致：门控分数由当前层输入（查询侧信息）决定，具有输入依赖性，且 Sigmoid 将分数压至 \((0,1)\)，使大部分位置或通道的门控值接近 0，从而引入**稀疏性**，抑制无关上下文与“注意力汇聚（attention sink）”现象，并提升训练稳定性[门控注意力论文]。因此，池化模块所接收的序列表示已经是经过门控调制、更聚焦于关键时间步的特征，注意力池化在此基础上进一步在长度维上做压缩，形成“门控编码 + 注意力池化”的两段式设计。  

**（3）多头跨模态输出的门控融合。** 在跨模态对齐阶段，框架使用多个独立的 CMA 头（CrossModal），每个头输出变量级特征。为融合多头信息并避免冗余，采用自适应门控融合模块（AdaptiveDynamicHeadsCMA）：将各头输出在通道维拼接后，通过小型 MLP 为每个头生成标量门控分数，经 Softmax 归一化后对多头输出进行加权求和：  
\[
\mathbf{Z} = \mathrm{Concat}\bigl(\mathbf{H}^{\text{cma}}_1,\dots,\mathbf{H}^{\text{cma}}_H\bigr),\quad
\mathbf{g} = \mathrm{Softmax}\bigl(f_{\text{gate}}(\mathbf{Z})\bigr),\quad
\mathbf{H}^{\text{cma}}_{\text{fused}} = \sum_{h=1}^{H} g_h\,\mathbf{H}^{\text{cma}}_h.
\]
该操作等价于在“头”维度上的门控池化：不同样本、不同变量可得到不同的头权重，从而自适应地突出与当前查询更相关的头、抑制冗余头，与门控注意力文献中“头特定门控”带来的收益一致。  

综上，门控注意力池化模块通过**注意力加权池化**实现变长到定长的压缩，通过**编码层内的 SDPA 输出门控**与**多头 CMA 的门控融合**引入输入依赖的稀疏性与选择性，在保证表示能力的同时抑制冗余信息与注意力汇聚，为后续跨模态对齐与解码提供稳定、紧凑的变量级表示，并与第 3.4 节的消融实验相衔接，用于验证门控机制的有效性。

## 3.4 基线实验与统一改进有效性验证
### 3.4.1 实验设置
### 3.4.2 Qwen3-1.4B 与 GPT-2 的对比实验
### 3.4.3 门控注意力机制的有效性验证

## 3.5 本章小结

# 第四章 融合小波包分解的时频域增强方法

## 4.1 问题分析与研究动机

## 4.2 模型架构设计
### 4.2.1 小波包多尺度分解策略
### 4.2.2 频域子带特征与 LLM 嵌入空间的映射机制
### 4.2.3 多尺度特征融合与预测头设计

## 4.3 实验设置与结果分析
### 4.3.1 实验设置与对比基线
### 4.3.2 MSE 指标表现及显著性分析
### 4.3.3 极端波动场景下的预测可视化分析
### 4.3.4 消融实验：小波包分解层数的影响

## 4.4 本章小结

# 第五章 融合频域前馈网络的全局频域增强方法

## 5.1 问题分析与研究动机

## 5.2 模型架构设计
### 5.2.1 频域前馈网络结构设计
### 5.2.2 复数域特征提取与门控权重融合机制
### 5.2.3 频域-时域特征交互设计

## 5.3 实验设置与结果分析
### 5.3.1 实验设置与对比基线
### 5.3.2 MAE 指标表现及稳健性分析
### 5.3.3 长期预测趋势平滑度分析
### 5.3.4 消融实验：频域前馈网络深度的影响

## 5.4 本章小结

# 第六章 MSE 与 MAE 优化分化的机理分析与模型选择策略

## 6.1 综合性能对比分析
### 6.1.1 MSE 与 MAE 指标的对比结果
### 6.1.2 不同数据集上的表现差异

## 6.2 误差特性机理分析
### 6.2.1 小波包分解对大误差抑制的机理分析
### 6.2.2 频域前馈网络对平均误差优化的机理分析
### 6.2.3 指标分化的理论解释

## 6.3 计算复杂度与收敛性分析
### 6.3.1 计算复杂度对比
### 6.3.2 训练收敛速度与稳定性分析

## 6.4 模型选择策略
### 6.4.1 基于误差敏感度的场景分类
### 6.4.2 模型选择建议

## 6.5 本章小结

# 第七章 总结与展望

## 7.1 研究工作总结

## 7.2 研究局限性分析

## 7.3 未来研究方向
