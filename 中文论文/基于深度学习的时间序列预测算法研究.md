<table><tr><td>单位代码</td><td>10445</td></tr><tr><td>学号</td><td>2020010113</td></tr><tr><td>分类号</td><td>TP391</td></tr></table>

# 山东师范大学

# 博士学位论文

# 基于深度学习的时间序列预测算法研究

# Research on Time Series Forecasting Algorithms Based on Deep Learning

一级学科：计算机科学与技术

专业名称：计算机科学与技术

学习方式：全日制

研究 生：郝建华

指导教师：刘方爱教授

提交时间 :2024年12月

# 目录

# 第一章 绪论

1.1 研究背景与意义

1.2 时间序列预测研究现状 3

1.2.1 基于统计模型的时间序列预测方法 ..... 4

1.2.2 基于机器学习的时间序列预测方法 ..... 5

1.2.3 基于深度学习的时间序列预测方法

1.2.4 时间序列预测面临的挑战 11

1.3本文的主要内容及创新点 11

1.4 论文组织结构 13

# 第二章 相关理论和方法 ..... 15

2.1 时间序列预测基本理论 ..... 15

2.2 基于深度学习的相关理论 16

2.2.1 时间卷积网络 ..... 16

2.2.2 图卷积网络 ..... 18

2.2.3 Transformer 19

2.2.4 多层感知机 ..... 21

2.2.5 接收加权键值模型 ..... 22

2.2.6 预训练 24

2.3 时间序列处理方法 ..... 25

2.3.1 基于Loess的季节-趋势分解 ..... 25

2.3.2 快速傅里叶变换 ..... 26

2.3.3 通道独立策略 ..... 27

2.4 本章小结 ..... 29

# 第三章 基于季节-趋势分解的二维时间卷积密集网络预测模型……31

3.1 引言 31

3.2 基于季节-趋势分解的二维时间卷积密集网络 33

3.2.1 问题定义 ..... 33

3.2.2 模型框架 33

3.2.3 季节-趋势分解 34

3.2.4 二维时间卷积密集网络 ..... 35

3.3 实验 38

3.3.1 实验数据集 38

3.3.2 实验设置 38

3.3.3 实验结果分析 40

3.3.4 消融实验 42

3.4 本章小结 ..... 43

第四章 基于预训练的多头接收加权键值与多尺度图卷积网络预测模型 45

4.1 引言 45

4.2 基于预训练的多头接收加权键值与多尺度图卷积网络 ..... 47

4.2.1 基于 FFT 的多尺度周期性特征识别 ..... 47

4.2.2 多头时间混合模块 ..... 48

4.2.3 多尺度 GCN 49

4.2.4 基于通道独立策略的预训练 ..... 52

4.3 实验 53

4.3.1 实验数据集 ..... 53

4.3.2 实验设置 54

4.3.3 实验结果分析 55

4.3.4 消融实验 ..... 58

4.4 本章小结 ..... 59

第五章 基于多尺度接收加权键值与二维时间卷积网络的预测模型61

5.1 引言 61

5.2 基于多尺度接收加权键值与二维时间卷积网络 65

5.2.1 基于FFT的周期识别 65

5.2.2 多尺度时间混合模块 67

5.2.3 多尺度二维卷积网络 68

5.3 实验 69

5.3.1 实验数据集 69

5.3.2 实验设置 72

5.3.3 实验结果分析 73

5.3.4 消融实验 76

5.3.5 不同季节预测效果分析 76

5.3.6鲁棒性分析 79

5.3.7 在其他光伏发电数据集的性能评估 ..... 80

5.4 本章小结 ..... 81

第六章 基于分解聚合注意力的Transformer预测模型 83

6.1 引言 ..... 83

6.2 基于分解聚合注意力的Transformer模型 85

6.2.1 问题定义 ..... 86

6.2.2 分解聚合注意力机制 ..... 87

6.2.3 残差块和残差连接 ..... 88

6.3 实验 89

6.3.1 实验数据集 ..... 89

6.3.2 实验设置 91

6.3.3 实验结果分析 93

6.3.4 消融实验 94

6.4 本章小结 95

第七章 总结与展望 97

7.1 总结 97

7.2 研究展望 98

参考文献. 101

# 摘要

随着信息技术的迅猛发展，气象预测、工业生产、交通运输、能源调度和金融活动等领域产生大量的多样化、多维度且规模庞大的时间序列数据。这些数据中蕴藏着丰富的时间演变规律，深入分析这些数据能帮助预测未来趋势，为科学决策提供重要依据，进而优化资源配置、降低潜在风险，提高系统的运行效率。由于时间序列数据具有非线性、多周期性和随机性等复杂特征，传统预测方法难以捕捉这些数据内在的复杂依赖关系。因此，基于深度学习的时间序列预测方法逐渐成为研究热点，凭借其处理复杂非线性和高维数据的能力，展示出优异的建模性能。

本文基于深度学习理论，结合实际应用领域中的时间序列预测问题，提出四种时间序列预测算法，旨在提升预测的精度，为时间序列预测领域提供理论支持和技术保障。本文的主要工作如下：

（1）基于季节-趋势分解的二维时间卷积密集网络：针对长期多元时间序列预测任务中，长期的趋势和季节性波动特征难以准确捕捉的问题，本文提出了一种基于季节-趋势分解的二维时间卷积密集网络预测模型。该方法首先使用基于Loess的季节-趋势分解，将原始时间序列分解为季节性、趋势和残差三个子序列。随后，设计了结合多层感知机和二维时间卷积网络的密集网络结构，充分捕捉分解后的序列中的复杂相互依赖关系。实验结果表明，该模型在多个公开数据集上能够提高长期时间序列预测的精度。

（2）基于预训练的多头接收加权键值与多尺度图卷积网络：针对多元时间序列在不同时间尺度上表现出多样性的相关性特征，以及来自不相关序列的噪声干扰问题，本文提出了一种基于预训练的多头接收加权键值与多尺度图卷积网络预测模型。通过在预训练阶段应用通道独立策略，使得模型可以独立学习每个时间序列的时间特征，从而避免序列间的噪声干扰。然后，设计了一个多头时间混合模块来捕捉多尺度的时间依赖关系，并结合多尺度图卷积网络，捕捉序列间多尺度的空间依赖关系。实验表明，该模型在多

个真实数据集上的预测性能优于现有的先进模型，特别是在长期预测方面展现出显著优势。

（3）基于多尺度接收加权键值与二维时间卷积网络：针对短期光伏功率预测任务中，发电功率具有复杂的周期性且受天气条件影响显著的问题，本文提出了一种基于多尺度接收加权键值与二维时间卷积网络的预测模型。该模型首先通过快速傅里叶变换识别光伏发电历史数据中的周期性特征，然后基于接收加权键值模型设计了一个多尺度时间混合模块，提取多种时间尺度下的依赖关系，并利用多尺度二维时间卷积网络捕捉历史数据中的复杂相互依赖关系。在多个光伏发电功率数据集上进行了充分实验，实验结果显示，该模型在短期光伏功率预测任务中的表现优于现有的先进方法，获得较低的预测误差，并且具有较高的鲁棒性。

（4）基于分解聚合注意力的Transformer模型：针对长期光伏功率预测任务，为准确捕捉历史数据中长期趋势和波动，并降低注意力机制的计算复杂度，本文提出了一种基于分解聚合注意力的Transformer预测模型。该模型将目标时间序列分解为季节性、趋势和残差三个子序列，通过分解聚合注意力机制捕捉各子序列与其它变量之间的时空依赖关系。另外，模型引入基于多层感知机的残差块用于蒸馏操作，结合稀疏注意力机制，降低注意力机制的计算复杂度，并采用残差连接保留输入数据中的原始特征。在两个光伏发电数据集上进行了实验，结果表明该模型在长期预测中表现优异。

关键词：时间序列预测；深度学习；时间卷积网络；接收加权键值；光伏功率预测中图分类号：TP391

# Abstract

With the rapid development of information technology, various fields such as weather forecasting, industrial production, transportation, energy scheduling, and financial activities are generating large-scale, multi-dimensional, and diverse time series data. These data contain various temporal evolution patterns, and in-depth analysis of them can help predict future trends, providing important insights for scientific decision-making, optimizes resource allocation, mitigates potential risks, and enhances system operational efficiency. However, due to the complex characteristics of time series data, such as nonlinearity, multi-periodicity, and randomness, traditional forecasting methods struggle to capture these complex internal dependencies. Consequently, time series forecasting methods based on deep learning have gradually become a research hotspot, showing excellent modeling performance due to their ability to handle complex nonlinear and high-dimensional data.

Based on deep learning theory and combined with practical time series forecasting applications, this thesis proposes four time series forecasting algorithms to improve forecasting accuracy, offering theoretical support and technical guarantees for time series forecasting. The main contributions of this thesis are as follows:

(1) Seasonal-trend Decomposition-based 2-dimensional Temporal Convolutional Dense Network (STL-2DTCDN): To address the challenges in capturing long-term trends and seasonal fluctuations in long-term multivariate time series forecasting tasks, this thesis proposes the STL-2DTCDN. First, this model uses the Seasonal-trend Decomposition Based on Loess (STL) to decompose the original time series into three subseries: seasonal, trend, and residual. Then, a dense network structure combining Multi-layer Perceptron (MLP) and 2-dimensional Temporal Convolutional Network (TCN) is designed to capture the complex dependencies among multivariate time series. Experimental results demonstrate that this model significantly improves the accuracy of long-term time series forecasting across multiple public datasets.

(2) Pre-trained Multi-head Receptance Weighted Key Value and Multi-scale Graph Convolutional Network (PMRWKV-GCN): Multivariate time series exhibits diverse correlations across various time scales, and potential noise from unrelated series can obscure intra-series temporal dependencies. To address these challenges, this thesis introduces the PMRWKV-GCN. By adopting a channel-independent strategy during the pre-training phase, each time series feature can be independently learned, thus avoiding noise interference between sequences. A multi-head time-mixing module based on the Receptance Weighted Key Value (RWKV) model is then designed to learn temporal dependencies across diverse scales, combined with a multi-scale Graph Convolutional Network (GCN) to learn diverse inter-series dependencies across multiple time scales. Experiments show that this model outperforms state-of-the-art models on multiple real-world datasets, particularly excelling in long-sequence forecasting.

(3) Multi-scale Receptance Weighted Key Value with 2-dimensional Temporal Convolutional Network (MSRWKV-2DTCN): To tackle the challenges in short-term photovoltaic (PV) power forecasting, where PV power generation exhibits complex periodicity and is significantly influenced by weather conditions, this thesis proposes the MSRWKV-2DTCN. This method first identifies multiple periodic features in the historical PV generation data using Fast Fourier Transform (FFT). It then extracts multi-scale dependencies using an improved multi-scale time mixing block of RWKV and captures complex interdependencies in the data through a multi-scale 2D TCN. Comprehensive experiments conducted on multiple PV power datasets demonstrate that the proposed model outperforms existing state-of-the-art methods in short-term PV power forecasting tasks, significantly reducing prediction errors while exhibiting high robustness.

(4) Decomposition Aggregation Attention Transformer (DAformer): For the task of long-term PV power forecasting, to accurately capture long-term trends and fluctuations in historical data while reducing the computational complexity of the attention mechanism, this thesis proposes the DAformer. This model decomposes the target time series into three sub-series: seasonal, trend, and residual. Using a decomposition aggregation attention mechanism, the

model captures the spatiotemporal dependencies between each sub-series and other variables. Additionally, the model introduces residual blocks based on MLP for distillation, incorporates a sparse attention mechanism to reduce the computational complexity of the attention mechanism, and employs residual connections to preserve the original features of the input data. Experiments conducted on two PV power datasets demonstrate the superior performance of this model in long-term PV power forecasting.

Key words: Time Series Forecasting; Deep Learning; Temporal Convolutional Network; Receptance Weighted Key Value; Photovoltaic Power Forecasting

Classification: TP391

# 第一章绪论

# 1.1 研究背景与意义

时间序列数据是指一组按照时间顺序排列的观测数据，通常用于记录某一物理量随时间变化的过程。这类数据广泛存在于社会经济活动、自然现象以及工程应用等多个领域，涵盖从微观到宏观的动态演变过程。这些数据能够真实反映某一现象或系统随时间的变化趋势和状态，因而在分析和理解系统行为时具有重要作用[1]。例如，在金融领域，股票价格、交易量和市场指数等数据每天都会不断更新，这些数据构成的时间序列，可用于预测股票市场的波动和趋势；在气象学中，温度、湿度、降水量、风速等气象数据随着时间变化而被记录下来，形成气象时间序列，可用于天气预报及灾害预警；在工业制造领域，设备的运行状态数据由传感器实时采集，形成记录机器运作状态的时间序列数据，有助于预测设备故障和优化维护保养策略；在能源领域，太阳能发电站和风力发电站的发电功率会随着时间不断变化，从而产生大量时间序列数据，这些数据对于提高能源管理的效率和电网调度的优化有着重要意义。通过对这些时间序列数据的分析和研究，不仅可以揭示系统内部的复杂动态演变机制，还能够为预测未来发展趋势、制定科学决策提供关键依据[2]。

随着信息技术的飞速发展，特别是在大数据、云计算、物联网等技术的推动下，时间序列数据的规模和维度呈现爆炸式增长。海量数据的涌现催生了对时间序列分析与预测的强烈需求。无论是企业为优化生产流程、提升运营效率，还是政府部门为应对气象灾害、能源管理与调度，时间序列预测技术都在发挥着日益重要的作用[3]。例如，在智能制造中，利用时间序列预测技术可以提前预测设备的运行状态，进行预防性维护，从而降低停机时间和运营成本；在金融市场中，时间序列预测被用于预测股市、外汇、期货等金融产品的价格波动，帮助投资者制定合理的投资策略；在能源行业中，精确的时间序列预测可以帮助电力公司合理调配电力资源、优化发电计划，从而提升能源利用效率和保障电网稳定。

然而，随着时间序列数据的复杂性和规模的提升，传统的时间序列分析方法逐渐暴露出其局限性。自回归模型（Autoregressive Model, AR）[4]、移动平均模型（Moving Average Model, MA）[5]、以及自回归积分滑动平均模型（Autoregressive Integrated Moving Average, ARIMA）[6]等基于统计学的传统分析方法，尽管在处理单一变量的线性时间序列时有着良好的表现，能够捕捉数据中的线性趋势和周期性波动，但面对多种领域的复杂时间序列数据时，这些模型的局限性变得越来越明显。

首先，传统的时间序列分析方法往往基于线性假设，无法有效处理数据中的非线性关系。现实世界中的时间序列数据往往包含多种非线性动态过程，表现出复杂的周期性、多尺度性、突发性以及混沌特性。例如，气象数据中的温度和降雨量变化不仅存在日周期和年周期，还受突发天气事件的影响，这种非线性特征使得传统线性模型难以有效建模。其次，随着数据维度的增加，传统模型在处理高维数据时面临“维度灾难”的问题。传感器技术和数据采集手段的进步，使得系统的多个方面可以同时被监测，从而产生大量的多变量时间序列，而这些变量之间存在复杂的相互依赖关系。传统的时间序列分析方法难以在高维数据中有效挖掘不同变量间的依赖关系。此外，时间序列数据的噪声和缺失值处理、模型的可解释性和稳定性等方面，也对现有的预测方法提出更高的要求。

在此背景下，深度学习（Deep Learning）作为一种新兴的数据驱动方法，逐渐成为时间序列预测领域的研究热点。与传统的统计模型相比，深度学习方法具有处理复杂非线性关系的强大能力，能够自动从海量数据中学习到复杂的模式和特征，从而在没有明确规则或假设的前提下，实现高精度的预测[7,8,9]。深度学习模型，例如循环神经网络（Recurrent Neural Networks, RNN）、长短期记忆网络（Long Short-term Memory, LSTM）、门控循环单元（Gated Recurrent Unit, GRU）以及近年来迅速崛起的Transformer[10]模型，已经在多个应用领域中取得显著的成果。RNN[11,12]能够有效捕捉时间序列中的时间依赖关系，LSTM[13]和GRU[14]进一步缓解了传统RNN在长序列建模中的梯度消失问题，而Transformer模型由于具有并行处理的优势，能够学习全局的依赖关系，已逐渐成为处理长时间序列数据的主流方法[15,16]。

虽然深度学习已经在时间序列预测领域中展现出了广阔的前景，但实际应用中仍存在诸多挑战亟待解决。例如，如何有效提取时间序列中的长期依赖特征，以及学习高维

数据中的复杂依赖关系，都是研究中备受关注的问题。同时，提升模型的可解释性以增加决策透明度，已成为应用领域的重要需求。此外，深度学习算法对大量数据和计算资源的依赖，如何在保证预测精度的前提下提升计算效率、降低能耗，成为另一大技术难关。尤其是在实时性要求较高的场景中，模型的计算速度与快速响应能力则显得尤为重要，这也是未来研究亟需突破的方向之一。

综上所述，时间序列预测作为一个跨学科的研究领域，具有丰富的理论研究价值和广泛的应用前景。通过进一步研究和发展深度学习技术，设计出适应性更强、预测精度更高、计算复杂度更低的时间序列预测模型，可以有效提升各行业的决策水平和智能化程度。本研究将在这一背景下，围绕时间序列预测中的关键问题展开，提出一系列新的预测模型和算法，力求为复杂的时间序列数据预测任务提供更加坚实的理论基础和技术保障。

# 1.2时间序列预测研究现状

随着大数据时代的到来，时间序列预测方法经历了快速的发展与演变。传统的统计模型曾长期主导该领域，广泛应用于金融、经济、工程等多个领域。然而，随着数据复杂性的增加，尤其是非线性和非平稳特征的增多，传统统计方法逐渐暴露出其局限性，难以应对现代复杂的预测任务。为了克服这些局限性，研究者开始引入机器学习算法。这类方法无需依赖线性假设，能够更灵活地从数据中自动学习复杂的非线性关系，在处理高维、多变量的时间序列数据时展现出显著优势。同时，集成学习方法的出现进一步提升了预测的准确性与鲁棒性。随着深度学习的兴起，时间序列预测进入了一个新的发展阶段。相比于传统的统计模型和机器学习模型，深度学习模型在处理复杂数据方面更具优势，尤其在处理大规模、多变量数据时表现更加突出。总体来看，时间序列预测方法从传统的统计方法逐步演进到机器学习，再到深度学习，以更好地应对不同领域中的复杂预测需求。接下来将详细介绍这三类主要的时间序列预测方法及其在实际应用中的优势与局限性。

# 1.2.1 基于统计模型的时间序列预测方法

在时间序列预测中，统计模型依赖于数据的线性特征和历史数据之间的依赖关系，通常假设未来的趋势可以通过分析过去的模式来进行预测。这类方法适用于具有稳定规律性和依赖性的场景，能够很好地捕捉数据中的线性关系。用于时间序列预测的统计模型包括自回归模型（AR）[17]、滑动平均模型（MA）[18]、自回归滑动平均模型（Autoregressive Moving Average, ARMA）[19]、指数平滑法（Exponential Smoothing, ES）[20]等。这些模型通过数学公式描述时间序列中的变化趋势，能够有效捕捉序列内在的周期性与规律性。AR、MA和ARMA模型的优势在于其计算相对简单，并且有着坚实的理论基础，通过对模型参数的估计，这些模型能够有效捕捉时间序列中的短期依赖性和周期性趋势，适合处理线性关系较强的时间序列。ES模型适用于处理平稳或带有趋势性的数据，能够很好地预测未来的变化。在这些模型基础上，自回归积分滑动平均模型（ARIMA）进一步扩展了ARMA模型的应用，ARIMA模型通过差分运算将具有趋势或季节性变化的时间序列转化为平稳序列，可以用于处理带有趋势或非平稳性的时间序列，尤其适合较为复杂的线性数据。

尽管上述基于统计模型的方法具有坚实的理论基础和较强的解释性，能够清晰解释模型如何利用历史数据进行预测，但它们在处理复杂的时间序列数据时表现出一定的局限性。首先，这类模型通常基于线性假设，而现实中的许多时间序列数据往往具有显著的非线性特征，如金融市场的价格波动、气象数据的多变性以及工业生产领域的动态变化。由于这些数据的复杂性和多样性，传统统计模型在面对具有复杂非线性关系的时间序列时，预测能力相对较弱。其次，统计模型的长期预测准确性较低，特别是当时间序列存在高度不确定性时，预测结果往往与实际情况存在较大偏差。随着时间序列预测任务的复杂性日益增加，基于统计模型的传统方法在应对这些复杂预测场景时存在不足，难以满足现代应用中的需求。为了解决这些问题，近年来研究者们逐渐转向更加灵活和强大的机器学习与深度学习方法，以捕捉数据中的非线性特征和复杂模式，从而提升预测的准确性和适用性。

# 1.2.2 基于机器学习的时间序列预测方法

为了克服基于统计学的方法在复杂时间序列预测任务中的局限性，基于机器学习的方法逐渐成为该领域的研究重点。传统的统计模型，如ARMA等，通常依赖于线性假设，并假定时间序列的生成过程是稳定且易于建模的。然而，现实中的许多时间序列数据具有高度的非线性、复杂的变量交互以及不可忽视的噪声干扰，这使得传统模型在面对高维度和多变量数据时，预测能力显得不足。相比之下，机器学习方法能够自动学习数据中的复杂模式，尤其是在没有预先定义特征的情况下，可以捕捉数据中的隐含特征和非线性关系，因此更适合处理复杂的时间序列数据。凭借这些优势，机器学习模型已成为应对复杂数据的重要工具，为复杂预测任务提供了有效的解决方案。常用的机器学习模型包括支持向量机（Support Vector Machine, SVM）[21]、决策树（Decision Tree）[22]、随机森林（Random Forest）[23]和梯度提升树（Gradient Boosting Tree, GBT）[24]。这些机器学习模型在时间序列预测中的应用广泛，尤其是集成学习方法如随机森林和梯度提升树，通过结合多个弱学习器（如多个决策树）的预测结果，有效提升了模型的预测性能。决策树及其集成模型在处理具有噪声或复杂特征的时间序列数据时表现出较好的鲁棒性，适合在不满足线性假设的数据中进行预测。此外，K-近邻算法（K-Nearest Neighbors Algorithm, KNN）也常用于时间序列预测[25,26]。KNN通过查找与当前数据最相似的历史数据，根据相似的历史模式进行预测，具有简单直观的优势，能够直接利用历史数据的局部信息，适用于一些具有显著模式特征的时间序列任务。

然而，尽管机器学习方法在复杂的时间序列预测任务中表现出色，但其应用也面临一定的挑战。首先，随着数据维度和规模的增加，模型的训练时间和计算资源需求会急剧增长。尤其是在处理大规模、多元数据时，训练过程可能非常耗时，不仅导致计算成本增加，还可能影响模型的实时响应能力。此外，在实时数据预测任务中，数据的快速变化要求模型能够及时更新，而许多机器学习模型在面对新数据时缺乏足够的灵活性，难以快速适应变化的环境。为应对这些问题，研究者们提出了一些优化策略。例如，分布式计算技术可以将数据分配到多个节点并行处理，从而加速训练过程；同时，模型优化技术如提前停止、模型压缩和增量学习等，也有助于减少计算时间和资源消耗。然而，

即便如此，在处理超大规模数据集时，计算效率仍然是一个亟待解决的关键问题。特别是在对实时性要求高的应用场景，如金融市场预测和电力调度中，如何在保证模型精度的同时提高计算效率，仍然是研究者面临的主要挑战之一。因此，尽管机器学习在复杂的时间序列预测任务中展现出强大的能力，但随着数据规模和实时性需求的提升，其应用仍然面临诸多挑战。这促使研究者们不断探索新的解决方案，以提升模型的效率和适应性，确保其在更复杂和动态的环境中持续发挥作用。

# 1.2.3 基于深度学习的时间序列预测方法

在大数据时代，时间序列预测方法面临着处理高维数据和非线性复杂关系的巨大挑战。传统的统计模型和机器学习方法在应对这些问题时，难以充分捕捉数据中的深层次特征，尤其是在处理大规模、多元时间序列数据时，表现出明显的局限性。近年来，随着深度学习技术的崛起，时间序列预测领域取得了显著的进展。深度学习模型能够自从数据中提取复杂特征，尤其在应对长短期依赖和非线性结构方面表现出突出的优势。与基于统计模型和机器学习的方法相比，深度学习方法具有直接处理原始时序数据的能力，无需依赖复杂的特征工程即可提取时间序列中的关键模式。随着计算能力的提升以及大规模图形处理单元（Graphics Processing Unit, GPU）的广泛应用，深度学习模型能够在大数据环境中高效地进行训练，并被广泛应用于金融、能源、交通等多个领域的预测任务中。基于深度学习的时间序列预测方法具有以下优势：（1）处理复杂的非线性关系。深度学习模型能够捕捉数据中的复杂模式及长短期依赖关系，从而生成高精度的预测结果。（2）大规模数据的训练能力。深度学习模型能够高效处理庞大的数据集，并通过持续的迭代学习，捕捉输入与输出之间的复杂关系。（3）适应性强。深度学习模型可以通过调整架构、参数和训练方式，还可以与统计模型或机器学习模型结合，形成混合模型，适应不同领域的时间序列预测任务。

循环神经网络（RNN）在时间序列预测中应用广泛[11]。RNN通过引入递归结构，使得模型能够保留前一时间步的隐藏状态，从而捕捉时间序列中的短期和长期依赖关系。然而，RNN在处理长序列数据时，容易出现“梯度消失”和“梯度爆炸”问题，这极大地限制了其预测性能。为了缓解这一问题，长短期记忆网络（LSTM）被引入时间序列预

测任务中[13]。LSTM通过“遗忘门”、“输入门”和“输出门”机制，使得模型能够更好地保留长期依赖信息，因此在处理长时间依赖任务时表现出色。相比于RNN，LSTM不仅能够处理复杂的时间依赖性，还能应对非线性关系，因此被广泛应用于金融市场预测、气象预报和能源负荷预测等领域[27,28]。门控循环单元（GRU）[14]作为LSTM的简化版本，虽然去掉了LSTM中的部分结构（如遗忘门），但在许多时间序列任务中，GRU在计算效率和预测精度之间实现了较好的平衡，它能够在保持较高预测精度的同时显著提高训练速度，因此在处理高频更新的时间序列任务中展现出了独特的优势，特别是在计算资源有限的场景中表现出色。

卷积神经网络（Convolutional Neural Network, CNN）[29]最初主要用于图像处理，凭借其强大的特征提取能力，逐渐被应用于时间序列预测领域[30]。通过卷积操作，CNN能够有效捕捉时间序列中的局部特征，在处理局部依赖关系方面具有显著优势。与循环神经网络（RNN）和长短期记忆网络（LSTM）相比，CNN具备更强的并行计算能力，能够在大规模数据集上实现更高效的训练，因此在时间序列预测任务中具有显著优势。此外，在复杂的多元时间序列任务中，CNN通过卷积过程捕捉变量之间的交互特征，能够更好地识别变量之间的相互依赖关系，从而提升整体的预测性能。另外，通过与其他深度学习模型（如LSTM、GRU）的结合，CNN能够进一步提高预测精度，特别是在应对非线性和高维数据时表现突出。

时间卷积网络（Temporal Convolutional Network, TCN）[31]是CNN的一种变体，专门为序列建模任务设计。TCN的主要创新点在于其采用了扩展卷积（Dilated Convolution）技术，这种机制通过在卷积核之间引入间隔，能够在不显著增加计算复杂度的前提下大幅度扩展模型的感受野，从而捕捉更广泛的时间依赖关系[32]。与RNN和LSTM相比，TCN不仅具备更好的并行性，而且能够保持梯度的稳定性，避免梯度消失或爆炸问题，因此在处理长时间依赖任务时的表现优于传统递归模型。此外，TCN的无反馈结构使其在处理非因果序列时表现出更强的鲁棒性，尤其适合用于长期时间序列预测和信号处理任务。通过灵活调整网络架构和卷积核的大小，TCN能够建模不同尺度的时间序列变化，具有较强的泛化能力，能够广泛应用于多种实际场景中。例如，在金融领域，股票市场

预测、外汇交易量预测、市场趋势分析等场景中，TCN能通过扩展卷积有效捕捉市场的长期趋势和周期变化，从而提高预测的准确性[33]。在电力负荷预测领域，电力消耗具有明显的周期性和趋势特征，TCN能够通过因果卷积和扩展卷积结构有效捕捉日间及季节性的负荷模式，提高预测的精度，为电力调度和分配提供可靠的决策支持[34,35]。在气象预测领域，TCN能够处理复杂的气象特征之间的关系，例如温度、降水量、气压等，适用于短期和长期的气象预测[36,37]。在交通流量预测领域，TCN可以学习各交通站点的时间特征和站点之间的空间依赖关系，实现精准流量预测，为交通管理和调度提供有力的支持[38,39,40]。TCN还在生物医学信号处理领域展现出了应用潜力，例如在心电图（Electrocardiograph, ECG）信号分析中，TCN能够处理长时间的生物医学信号，帮助医生诊断疾病[41]。

随着图结构数据（Graph-structured Data）的广泛研究，图神经网络（Graph Neural Network, GNN）[42]逐渐被引入时间序列预测领域。GNN能够通过建模变量之间的关系图，捕捉多元时间序列变量之间的空间依赖性，学习各变量之间的动态交互关系。这在处理多维数据（如交通网络、传感器网络等）中具有显著优势。例如，在交通预测中[43,44,45]，GNN可以基于道路网络图，建模不同位置之间的时空交互关系，从而更加准确地预测交通流量变化。此外，GNN在金融时间序列预测中也展现出巨大的应用潜力。例如，Cao等人[46]通过GNN捕捉股票之间的隐含结构信息，建模不同股票之间的时空依赖关系，成功捕捉股票市场中的复杂动态变化，提升股票价格预测的准确性。另外，GNN也被用于处理医疗时间序列数据，Zhang等人[47]提出了基于GNN的Raindrop模型，该模型能够处理不规则的医疗时间序列数据，捕捉患者生命体征中的时空依赖性，为疾病预防和治疗决策提供重要的数据支持。

一些流行的 GNN 架构，如图注意力网络（Graph Attention Network, GAT）[48,49]、动态图神经网络（Dynamic Graph Neural Networks, DGNN）[50]、图卷积网络（Graph Convolutional Network, GCN）[51,52]等，已被广泛用于多种时间序列预测任务。GAT 通过引入注意力机制，动态调整邻居节点的权重，赋予不同邻居节点不同的重要性，从而使模型可以更加灵活地处理节点间复杂的关系；DGNN 进一步提升了模型学习节点间依赖关系的能力，通过动态更新图的拓扑结构和节点特征，能够更好地应对节点和边的动态

变化；GCN 通过在图结构数据上应用卷积操作，使模型能够聚合来自邻近节点的信息，这样可以更好地捕捉图中节点之间的相互依赖关系。作为图神经网络的一种实现形式，GCN 能够利用图的拓扑结构，学习时间和空间上的依赖关系，在多元时间序列的预测任务中表现出色。特别是在处理复杂的场景时，GCN 能够通过捕捉各变量之间的空间依赖性，可以显著提高模型的预测精度，例如（1）交通流量预测[53]。城市交通网络通常建模为图结构，每个节点代表交通站点，边表示站点之间的连接。GCN 能够捕捉站点间的空间依赖关系，并能够捕捉历史交通数据中的时间依赖关系，在处理复杂交通流量预测任务中表现出色，能够大幅度提高预测精度。（2）能源需求预测[54]。能源网络具有复杂的拓扑结构，不同区域和节点之间具有很强的依赖性。GCN 擅长建模这些区域与节点间的空间相关性，从而更准确地捕捉能源需求的变化趋势，提升能源需求预测的精度和鲁棒性。（3）风电功率预测[55]。风力发电场之间的相互关系可以用图来表示，通过 GCN 捕捉风电场之间的空间相关性，结合时间序列数据，模型可以更精准地预测未来的风电功率变化。虽然在时间序列预测任务中引入图结构，结合 GCN 能够适应于更加复杂的预测任务，但是 GCN 在实际应用中也面临一些挑战：（1）计算复杂度。随着图规模的增大，GCN 的计算复杂度也迅速增加，对计算资源的需求急剧增加。（2）过平滑问题。当 GCN 堆叠多层时，节点特征可能出现过度平滑，导致节点的个性化信息丧失，影响模型的表现。（3）动态图处理。在许多实际应用中，图结构是动态变化的，如何高效处理动态图仍然是一个挑战。

Transformer 模型是一种基于注意力机制的深度学习架构，由 Vaswani 等人[10]在 2017 年提出。近年来，Transformer 模型凭借其强大的自注意力机制（Self-attention Mechanism）在自然语言处理（Natural Language Processing, NLP）中取得了巨大成功，并逐渐被应用于时间序列预测任务。与传统的序列模型（如 RNN 和 LSTM）不同，Transformer 主要依赖于注意力机制，这使得 Transformer 在处理长序列时能够并行化计算，大大提高了计算效率。并且自注意力机制能够捕捉序列中任意两个位置之间的依赖关系，使其非常适合解决长期依赖问题，因此 Transformer 模型在时间序列预测中得到广泛应用，尤其在处理复杂、多变量和长时间依赖的序列时表现出色。另外，借助多头注意力机制（Multi-head

Attention Mechanism), Transformer 可以同时关注多个子空间的依赖关系, 在多元时间序列预测中表现尤为优异。基于上述优点, Transformer 模型在处理诸如金融市场数据[56]、气象数据[57]等复杂的应用场景非常有效。Transformer 还可以用于时间序列中的异常检测,识别出序列中不符合历史模式的点[58]。此外, Transformer 还可以用于生成时间序列数据,模拟出可能的未来场景[59]。然而, 虽然 Transformer 能够并行计算, 但注意力机制的计算复杂度是二次的 (O(n²)), 当处理非常长的序列时, 计算开销会急剧增加。并且 Transformer 通常需要大量的数据来训练, 以发挥其最佳性能。另外, 与传统的统计模型相比, Transformer 的内部机制较为复杂, 不容易解释模型的决策过程。

在最近的研究中，多层感知机（Multi-layer Perceptron，MLP）由于其结构简单且易于优化，在时间序列预测领域展示出了出色的性能[60]。尽管复杂的深度学习模型如LSTM、GRU和Transformer在处理序列数据时表现优异，MLP模型以其高效性和意外的准确性重新引起了众多研究者的关注[61,62,63]。MLP通过在隐藏层中引入非线性，能够有效捕捉时间序列中的复杂模式，例如趋势和周期性变化，这使得MLP在处理非线性特征突出的数据，尤其是带有不规则模式的时间序列时，展现出较强的适应性。虽然MLP的结构相对简单，近期研究表明它在时间序列预测中具有多方面的优势：（1）计算效率高。与复杂的RNN和Transformer相比，MLP结构简单，前向计算与训练过程更为高效，非常适合大规模数据集和资源受限的环境。（2）易于优化。由于不存在复杂的循环结构，MLP在梯度下降过程中更加稳定，避免梯度消失或爆炸的问题，训练相对容易。（3）预测性能优异。在许多时间序列预测任务中，MLP结构简单但效果优异，甚至在某些短期和中期预测任务中超过了众多复杂的深度学习模型。（4）通用性强。MLP适用于各种类型的时间序列预测任务，涵盖单变量和多变量数据，展现出良好的适应能力。综上所述，凭借其结构简洁和易于实现的特性，MLP在时间序列预测中重新获得研究者的关注。尽管它在建模复杂特征时不如一些复杂的深度学习模型，但其高效性和强大的泛化能力使其成为一种极具竞争力的工具。另外近期的研究还显示，通过合理的特征设计和模型优化，MLP能够与其他先进模型相抗衡，甚至在特定应用场景下表现更为出色[62]。

# 1.2.4 时间序列预测面临的挑战

尽管目前已有大量研究设计了多种时间序列预测算法，但这些算法仍面临以下几个关键问题：

（1）复杂性与非线性问题：时间序列数据通常表现出复杂的非线性特征，包含周期性、多尺度性和随机性特征。基于统计模型的算法在捕捉这些复杂特征时效果有限，而即便是深度学习模型，在面对高维度、非线性数据时，也难以全面建模并准确预测数据的动态变化。

（2）长短期依赖问题：时间序列数据通常同时包含短期波动和长期趋势。准确捕捉这些复杂的时间依赖关系是时间序列预测中的一大挑战。短期依赖涉及相邻数据点之间的关系，而长期依赖则涉及较远数据点之间的关联。许多现有模型在处理长时间跨度的依赖关系时存在困难，导致预测精度下降。

（3）多元时间序列的交互问题：在多元时间序列中，不同变量之间的相互依赖关系较为复杂。如何有效地建模这些变量之间的相互依赖关系对于提升预测精度至关重要。然而，复杂的交互关系往往导致模型的复杂度增加，同时也可能引入噪声，影响预测的稳定性。

（4）计算效率和模型部署问题：随着数据规模的不断增加，时间序列预测模型的训练和推理所需的计算资源显著增加。这不仅限制了模型的实时预测能力，还对模型在实际环境中的部署带来挑战，尤其是在计算资源有限的情况下。

综上所述，尽管时间序列预测技术在理论和实践中已取得显著进展，但仍有许多挑战亟待解决。未来研究需要在提高模型的复杂性处理能力、长短期依赖的捕捉能力、多变量交互建模能力以及提升计算效率方面做出更多的努力。

# 1.3本文的主要内容及创新点

基于时间序列预测领域的研究现状以及当前存在的问题，本文以深度学习模型为基础，综合运用时间序列分解、多尺度学习等前沿技术，重点研究时间序列的趋势和季节性波动，以及多尺度的时空依赖关系，提出四种基于深度学习的时间序列预测模型。本

文的主要创新点和贡献如下：

（1）基于季节-趋势分解的二维时间卷积密集网络模型：针对长期多元时间序列预测任务中，长期的趋势和季节性特征难以准确捕捉的问题，本文提出了一种基于季节-趋势分解的二维时间卷积密集网络预测模型。该模型首先采用基于Loess的季节-趋势分解方法，将原始时间序列分解为季节性、趋势和残差三个子序列，从而提取出时间序列中的季节性和趋势特征。然后设计了一个结合多层感知机和二维时间卷积网络的密集网络，有效提取多元时间序列的时间和空间依赖关系。实验结果表明，该模型在多个公开数据集上能够提升长期多元时间序列的预测精度。

（2）基于预训练的多头接收加权键值与多尺度图卷积网络模型：多元时间序列在不同的时间尺度上展现出不同的相关性，而来自不相关序列的潜在噪声可能会削弱模型对序列内部时间依赖关系的捕捉能力。为解决上述问题，本文提出了一种基于预训练的多头接收加权键值与多尺度图卷积网络预测模型。该模型首先在预训练阶段采用通道独立策略，确保每个时间序列的特征可以被独立学习，从而避免序列之间的噪声干扰。随后，设计了一个多头时间混合模块并结合多尺度图卷积网络，捕捉序列内和序列间跨时间尺度的时空依赖性。实验结果表明，该模型在多个真实数据集上的预测性能优于现有的先进模型，尤其在长期预测任务中表现出色。

（3）基于多尺度接收加权键值与二维时间卷积网络的模型：针对短期光伏功率预测中，光伏发电受天气条件影响显著，发电功率与外部气象因素之间存在复杂的周期性和非线性依赖关系，本文提出了一种基于多尺度接收加权键值与二维时间卷积网络的预测模型。首先，该模型通过快速傅里叶变换识别光伏发电功率及气象历史数据中的周期性特征，并且通过改进的接收加权键值时间混合模块来提取多种时间尺度的依赖关系。然后，利用多尺度二维时间卷积网络进一步捕捉数据中的复杂相互依赖特征。实验结果显示，该方法在多个光伏发电数据集上的表现优于现有的模型，取得较低的预测误差，并具有较强的鲁棒性，验证了其在短期光伏功率预测任务中的有效性。

（4）基于分解聚合注意力的Transformer模型：针对长期光伏功率预测任务，为准确捕捉历史数据中长期季节性和趋势特征，并降低注意力机制的计算复杂度，本文提出了一种基于分解聚合注意力机制的Transformer预测模型。为更好地捕捉光伏发电数据中的

长期依赖性，该模型将目标时间序列分解为趋势、季节性和残差三部分。随后，通过分解聚合注意力机制学习各子序列与外部变量间的时空依赖关系。此外，模型引入了基于多层感知机的残差块进行信息蒸馏，并结合稀疏注意力机制进一步降低注意力机制的计算复杂度。同时采用残差连接，保留输入数据的原始特征。实验结果表明，该方法在处理在长期光伏功率预测任务中表现出色，其预测结果优于现有的基于Transformer的模型。

# 1.4 论文组织结构

本文的研究围绕时间序列预测中的关键问题展开，基于深度学习方法提出多种创新性的解决方案。论文总体框架如图1-1所示，全文共分为七章，具体内容安排如下：

第一章：绪论。对本文的研究背景与意义进行阐述，分析时间序列预测的研究现状和现有的预测方法。具体包括统计模型、机器学习模型和深度学习模型，并总结时间序列预测面临的主要挑战。最后，介绍本文的主要研究内容与贡献。

第二章：首先简介时间序列预测的基本理论，然后重点介绍本文所依赖的深度学习理论，包括时间卷积网络、多层感知机、接收加权键值模型、图卷积网络、Transformer模型和预训练策略。此外，还详细介绍了时间序列处理方法，包括基于Loess的季节-趋势分解、快速傅里叶变换，并简述通道独立策略在时间序列预测任务中的应用。

第三章：基于季节-趋势分解的二维时间卷积密集网络预测模型。本章介绍该方法的研究动机，详细说明模型的设计与实现。然后介绍实验所用的数据集、对比方法和实验设置，并分析实验结果，验证该模型在长期多元时间序列预测任务中的有效性。

第四章：基于预训练的多头接收加权键值与多尺度图卷积网络预测模型。本章介绍该方法的研究动机，详细说明模型所采用的分解方法、预训练策略以及通道独立方法。然后介绍实验数据集、对比方法和实验设置，并通过消融实验验证所提出模块的有效性。

第五章：基于多尺度接收加权键值与二维时间卷积网络的预测模型。本章深入研究光伏功率预测任务及其挑战，介绍模型的关键模块。随后，详细说明实验使用的光伏功率数据集，并通过实验验证该模型在短期光伏功率预测中的有效性。

第六章：基于分解聚合注意力的Transformer预测模型。本章阐述长期光伏功率预测

任务的重要性，重点介绍模型中的分解聚合注意力机制、残差块及残差连接。详细分析实验数据集，通过实验验证该模型在长期光伏功率预测中的有效性。

第七章：总结与展望。回顾本文的核心贡献和研究成果，并展望未来的研究方向。

![](images/31b14a0dd4dfb8e68b96c2b94fcf7b0f18ccfaf0d2ea67cf8ef68666505ccc8e.jpg)



图1-1论文总体框架


# 第二章 相关理论和方法

本章首先简介时间序列预测基本理论，然后重点介绍与本文研究内容密切相关的深度学习理论和时间序列处理方法，涵盖时间卷积网络、图卷积网络、Transformer、多层感知机、接收加权键值模型、预训练，以及基于Loess的季节-趋势分解、快速傅里叶变换和通道独立策略。

# 2.1时间序列预测基本理论

时间序列预测是一种通过分析历史数据中的时间模式，预测未来趋势的技术。时间序列数据按时间顺序记录，广泛存在于经济[17]、能源[30]、金融[33]、气象[36]、交通[38]等诸多领域。时间序列预测的核心在于利用历史数据，预测未来一个或多个时间点的数值。根据输入数据的特性和预测的时间跨度等，时间序列预测模型可以细分为不同类别。

时间序列预测模型的一般公式表示为：

$$
\hat {x} _ {T + 1} = F \left(X _ {t = 1} ^ {t = T}\right) \tag {2-1}
$$

其中，函数  $F$  表示预测模型， $X_{t=1}^{t=T}$  表示历史数据， $\hat{x}_{T+1}$  是未来时间点  $T+1$  的预测值。

（1）根据输入数据的类型，预测任务可以分为单变量时间序列预测和多元时间序列预测两类。

单变量时间序列预测只使用单条时间序列的历史数据进行预测，其目标是预测该序列未来的值。公式表示为：

$$
\hat {x} _ {T + 1} = F \left(x _ {1}, x _ {2}, \dots , x _ {T}\right) \tag {2-2}
$$

其中， $(x_{1}, x_{2}, \ldots, x_{T})$  是单条时间序列的历史数据，函数  $F$  基于该单一序列的历史数据进行预测。

多元时间序列预测利用多个相互关联的时间序列的历史数据进行预测，目标是同时预测所有序列或者目标序列的未来值。公式表示为：

$$
\hat {Y} _ {T + 1} = F \left(X _ {t = 1} ^ {t = T}\right) \tag {2-3}
$$

其中，  $X = (c_{1},c_{2},\dots,c_{n})_{t = 1}^{t = T}$  表示历史数据，  $n$  表示变量的数量，  $\hat{Y}_{T + 1}$  是  $T + 1$  时刻单个或多个

目标序列的预测值。

(2) 根据预测的时间跨度, 预测任务可分为单步预测和多步预测。

单步预测的目标是预测下一个时间点的值。公式表示为：

$$
\hat {x} _ {T + 1} = F \left(x _ {1}, x _ {2}, \dots , x _ {T}\right) \tag {2-4}
$$

单步预测直接输出下一个时间点的预测值  $\hat{x}_{T + 1}$ 。

多步预测的目标是预测未来多个时间点的值，可以通过以下两种方式实现：

直接多步预测（Direct Multi-step, DMS）：每一时间步独立预测，例如预测  $T + 2$  时刻的预测值  $\hat{x}_{T + 2}$  不依赖于  $T + 1$  时刻的预测结果  $\hat{x}_{T + 1}$ ，而是直接基于历史数据预测。公式为：

$$
\hat {x} _ {T + k} = F _ {k} \left(x _ {1}, x _ {2}, \dots , x _ {T}\right) \tag {2-5}
$$

其中  $k$  是预测的步数。

迭代多步预测（Iterated Multi-step, IMS）：利用前一步的预测结果作为输入来预测下一个时间点的值。公式为：

$$
\hat {x} _ {T + k} = F _ {k} \left(x _ {1}, x _ {2}, \dots , x _ {T}, \dots , \hat {x} _ {T + k - 1}\right) \tag {2-6}
$$

迭代多步预测会导致误差随着预测长度的增加而累积。

在实际应用中，通常面临多元时间序列预测问题，并且变量内和变量间存在复杂的时空依赖关系，这使得多元时间序列预测相较于单变量预测更加具有挑战性。随着预测时间跨度的增加，预测的不确定性也随之上升，对模型的稳定性和鲁棒性提出更高的要求。

# 2.2 基于深度学习的相关理论

# 2.2.1 时间卷积网络

时间卷积网络（TCN）[31]的核心组件包括因果卷积（Causal Convolution）、扩展卷积（Dilated Convolution）和残差连接（Residual Connection）。这些模块共同作用，使得TCN能够适应复杂的时间序列预测任务，并在不同的应用场景中表现出色[33,36,40]。TCN的扩展因果卷积过程如图2-1所示。

![](images/f888670edf2b2d42a0fee5144de2365252e7152214a01016202bc0cdc5d43ca5.jpg)



图2-1两个堆叠的扩展因果卷积层，卷积核大小为2，扩展率为[1,2,4]


# （1）因果卷积

因果卷积是 TCN 的重要组成部分，它确保时间序列数据中的每个输出时间步  $y_{t}$  只依赖于当前及之前的输入值  $x_{1}, x_{2}, \dots, x_{t}$ ，而不会使用未来的信息。这种设计有效防止了预测过程中的“信息泄露”问题，能够确保模型不会在预测当前时间步时访问未来的数据，这是时间序列预测中的关键特性。公式表示为：

$$
y _ {t} = \sum_ {i = 0} ^ {k - 1} w _ {i} \cdot x _ {t - i} \tag {2-7}
$$

其中， $k$  是卷积核的大小， $w_{i}$  是卷积核的权重， $x_{t - i}$  是当前时间步及之前的输入值。

因果卷积的特性使得 TCN 能够像 RNN 一样逐步处理序列数据，同时有效减轻 RNN 中的梯度消失和梯度爆炸问题。在实际应用中，这使得 TCN 特别适用于处理具有严格因果关系的时间序列数据，如金融预测、气象预测和电力负载预测等场景。

# (2) 扩展卷积

扩展卷积通过在卷积核元素之间引入间隔，进一步扩大模型的感受野，使其能够捕捉更长时间跨度的依赖关系。其主要优势在于，在计算复杂度仅有轻微增加的情况下，通过更大的感受野有效处理长期依赖问题。随着扩展率的增大，TCN 可以在更广的时间

窗口内学习数据之间的关系，从而提升对长期依赖的捕捉能力。扩展卷积的公式为：

$$
y _ {t} = \sum_ {i = 0} ^ {k - 1} w _ {i} \cdot x _ {t - d \cdot i} \tag {2-8}
$$

其中， $d$  表示扩展率，通常随着网络层数的增加而呈指数增长。这意味着，较深层的网络可以利用扩展卷积处理超长时间依赖的序列数据，而无需显著增加计算开销。扩展卷积在处理诸如天气预报、股票价格预测等复杂序列数据时，尤其擅长捕捉长期依赖关系。凭借这种结构，TCN相比于RNN类网络具有明显的优势，能够同时高效处理短期与长期依赖数据，并保持较高的计算效率。

另外，尽管扩展卷积能够大幅度增加感受野，但设计合理的扩展率仍然是模型设计中需要关注的一个重要问题。如果扩展率过大，可能导致模型在捕捉短期依赖时出现不足；而过小的扩展率则可能限制模型学习长时间依赖关系的能力。

# （3）残差连接

随着网络层数的增加，模型可能会面临梯度消失的问题，进而影响模型的训练效果。为应对此挑战，TCN引入了残差连接。残差连接将输入直接传递到输出层，确保梯度在网络中顺利传播，降低梯度消失的风险。残差连接的公式为：

$$
y _ {t} = F \left(x _ {t}\right) + x _ {t} \tag {2-9}
$$

其中， $F(x_{t})$  表示通过卷积层的输出值。

残差连接使得 TCN 能够更有效地训练更深的网络层次，并且减轻梯度消失或梯度爆炸问题的影响，提升模型在处理长序列数据时的稳定性。这一特性使得 TCN 在复杂序列数据的应用中表现优异。

# 2.2.2 图卷积网络

图卷积网络（GCN）是一种针对图结构数据设计的深度学习模型，广泛应用于处理非欧几里得空间结构的数据[64,65]。与其它的时间序列预测方法相比，GCN通过引入图结构，能够同时处理空间和时间上的依赖，提升预测的准确性。其核心思想是将卷积操作应用于图结构，通过聚合邻居节点的信息更新每个节点的表示。GCN的节点更新包括以下步骤：

（1）邻居节点聚合。对于每个节点，通过聚合该节点及其邻居节点的特征信息来更新节点的表示。节点表示不仅依赖于其自身特征，还能够融入邻居节点的信息。基于邻域内特征的聚合方式，使模型能够捕捉局部结构关系。公式为：

$$
h _ {v} ^ {(l + 1)} = \sigma \left(\sum_ {u \in N _ {(v)} \cup \{v \}} \frac {1}{c _ {v u}} W ^ {(l)} h _ {u} ^ {(l)}\right) \tag {2-10}
$$

其中， $h_{\nu}^{(l+1)}$  是第  $l+1$  层中节点  $\nu$  的特征表示， $N_{(\nu)}$  表示节点的邻居节点集合， $W^{(l)}$  是第  $l$  层的权重矩阵， $c_{\nu u}$  是归一化系数，通常与节点度数相关， $h_{u}^{(l)}$  是第  $l$  层中节点  $u$  的特征表示  $\sigma$  是非线性激活函数（如ReLU）。

(2) 邻接矩阵表示。GCN 的计算可以用图的邻接矩阵来简化。邻接矩阵  $A$  是一个  $N \times N$  的矩阵，表示图中节点之间的连接关系。邻接矩阵中的元素  $A_{ij}$  为 1 时表示节点  $i$  和节点  $j$  之间存在连接，否则为 0 。公式表示为：

$$
H ^ {(l + 1)} = \sigma \left(\tilde {D} ^ {- 1 / 2} \tilde {A} \tilde {D} ^ {- 1 / 2} H ^ {(l)} W ^ {(l)}\right) \tag {2-11}
$$

其中， $H^{(l+1)}$  是第  $l$  层的节点特征矩阵， $\tilde{A} = A + I$  是加入自连接的邻接矩阵， $\tilde{D}$  是  $\tilde{A}$  的度矩阵， $W^{(l)}$  是权重矩阵。

（3）多层堆叠。通过多层 GCN 的堆叠，节点的表示可以逐层聚合更多范围的邻居信息。这种多层结构允许模型捕捉图中的全局结构特征。随着层数的增加，GCN 可以从局部邻域的信息扩展到更大范围，甚至是整个图结构的信息，从而增强对节点表示的表达能力。

# 2.2.3 Transformer

Transformer[10]模型由于其独特的自注意力机制，能够捕捉全局的依赖关系，在处理长时依赖问题上具有明显的优势，特别是在时间序列预测任务中，它能够捕捉到时间序列中跨时间步长的复杂模式和长期趋势[15,16]。Transformer的架构主要包括以下几个关键部分：

（1）自注意力机制（Self-attention Mechanism）。自注意力机制是Transformer的核心模块，用于计算输入序列所有位置之间的相关性，从而捕捉序列中任意两个位置之间的

依赖关系。公式表示为：

$$
\operatorname {A t t e n t i o n} (Q, K, V) = \operatorname {S o f t m a x} \left(\frac {Q K ^ {T}}{\sqrt {d _ {k}}}\right) V \tag {2-12}
$$

其中， $Q$ （Query）、 $K$ （Key）和  $V$ （Value）分别是通过输入序列的线性变换得到的矩阵， $d_k$  是 Key 的维度。公式（2-12）计算  $Q$  和  $K$  的相似度，并根据相似度对  $V$  进行加权求和，从而得到输出。

（2）多头注意力机制（Multi-head Attention）。多头注意力机制通过将输入序列映射到多个子空间，每个子空间分别应用独立的注意力机制，然后将结果拼接在一起。这种方法允许模型在不同的子空间中学习到不同的关系，从而捕捉到更多层次的信息。公式表示为：

$$
\operatorname {M u l t i H e a d} (Q, K, V) = \operatorname {C o n c a t} \left(\text {h e a d} _ {1}, \dots , \text {h e a d} _ {h}\right) W ^ {O} \tag {2-13}
$$

其中，  $head_{i} = Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}),W_{i}^{Q},W_{i}^{K},W_{i}^{V}$  和  $W^0$  是可训练的权重矩阵。

（3）位置编码（Positional Encoding）。由于Transformer模型不保留序列的位置信息，位置编码使得模型能够感知输入数据的顺序。公式表示为：

$$
P E (p o s, 2 i) = \sin \left(\frac {p o s}{1 0 0 0 0 ^ {2 i / d _ {\text {m o d e l}}}}\right) \tag {2-14}
$$

$$
P E (p o s, 2 i + 1) = \cos \left(\frac {p o s}{1 0 0 0 0 ^ {2 i / d _ {\text {m o d e l}}}}\right) \tag {2-15}
$$

其中，pos是位置，i是维度索引， $d_{model}$  是模型的维度。

（4）前馈神经网络（Feed-forward Neural Network, FFN）。在每个注意力层之后，Transformer 还包含一个前馈神经网络，用于对每个位置的输出进行进一步的非线性变换。公式表示为：

$$
F F N (x) = \max  \left(0, x W _ {1} + b _ {1}\right) W _ {2} + b _ {2} \tag {2-16}
$$

其中， $W_{1}$  和  $W_{2}$  是权重矩阵， $b_{1}$  和  $b_{2}$  是偏置项。

（5）残差连接与层归一化（Residual Connection and Layer Normalization）。为促进梯度传播并防止模型退化，每个子层（如自注意力机制和前馈神经网络）之后都使用残差连接和层归一化。公式表示为：

$$
L a y e r N o r m (x + S u b l a y e r (x)) \tag {2-17}
$$

其中  $\operatorname{Sublayer}(x)$  是子层的输出。

# 2.2.4 多层感知机

多层感知机（MLP）是一种前馈人工神经网络，由输入层、一个或多个隐藏层以及输出层组成。每一层的神经元通过加权求和与激活函数连接，且采用反向传播算法进行训练。作为神经网络中最基础的结构之一，MLP 通过简单的层叠结构和激活函数，能够有效捕捉输入数据中的复杂模式。近期的研究表明，在时间序列预测领域，基于 MLP 的方法在预测效果上已超越了许多基于 RNN 和 Transformer 的模型[61,63]。

MLP 的基本结构包括：（1）输入层：负责接收原始数据输入。在时间序列预测中，输入通常是过去时间点的数据，这些数据通常经过标准化或归一化处理，以确保模型的训练效果。（2）隐藏层：由多个神经元组成。每个神经元接受前一层输出的加权结果，并通过激活函数（如ReLU、Sigmoid等）产生输出。隐藏层使得MLP具备强大的非线性表达能力，能够捕捉数据中的复杂模式。堆叠多个隐藏层可以提高模型的预测精度，但也增加了计算复杂度。（3）输出层：生成最终的预测结果。在时间序列预测任务中，输出层的节点数量通常与预测目标的数量一致。例如，在单步预测任务中，输出层仅包含一个节点；而在多步预测任务中，输出层则可能包含多个节点。

MLP的前向传播过程可以用以下公式表示：

（1）输入到隐藏层的计算：

$$
h _ {j} = \sigma \left(\sum_ {i = 1} ^ {n} w _ {i j} x _ {i} + b _ {j}\right) \tag {2-18}
$$

其中， $h_j$  是隐藏层第  $j$  个神经元的输出， $x_i$  是输入层第  $i$  个节点的输入值， $w_{ij}$  是输入层到隐藏层的权重， $b_j$  是隐藏层第  $j$  个神经元的偏置项， $\sigma$  是激活函数，如ReLU或Sigmoid。

(2) 隐藏层到输出层的计算:

$$
y _ {k} = \sigma \left(\sum_ {j = 1} ^ {m} w _ {j k} h _ {i} + b _ {k}\right) \tag {2-19}
$$

其中， $y_{k}$  是输出层第  $k$  个节点的输出值， $w_{jk}$  是隐藏层到输出层的权重， $b_{k}$  是输出层第  $k$  个节点的偏置项。

# 2.2.5 接收加权键值模型

接收加权键值模型（Receptance Weighted Key Value, RWKV）[66]是一种新型神经网络架构，结合了 RNN 的顺序处理能力和 Transformer 的注意力机制，尤其在长序列数据处理和时间序列预测任务中表现出色。RWKV 通过引入类似于 Transformer 的注意力机制，同时保留 RNN 的顺序信息处理优势，为处理复杂的时间序列和顺序数据提供一种高效的算法框架。

RWKV 的设计目标是解决 RNN 模型在处理长时间依赖性问题时的不足，同时结合 Transformer 的全局信息捕捉能力，使其能够同时应对短期和长期的依赖问题。与传统的 RNN 通过递归计算捕捉时序信息不同，RWKV 引入了权重加权机制和注意力机制，使其具备更强的全局依赖捕捉能力，同时避免了点积注意力机制的二次计算复杂度。RWKV 模型的架构由多个时间混合模块（Time-mixing Block）和通道混合模块（Channel-mixing Block）组成，这些模块通过残差连接的方式逐层堆叠，形成一个深层的网络结构。时间混合模块和通道混合模块分别学习时间序列数据的时间特征和空间特征，残差连接将每一层的输入直接传递到后面的层，解决深层网络中的梯度消失问题，通过这种方式，模型能够有效地学习时间和特征维度上的信息，从而实现更为精确的序列建模。RWKV 的网络结构如图 2-2 所示。

![](images/c7bc74ed4129dce68a1e4037d7a502ac6f4ede157e88664541a681391b936446.jpg)



图2-2RWKV的架构


时间混合模块的核心功能是捕捉时间序列中的时间依赖性。通过时间维度的线性组合，该模块能够有效处理序列数据的顺序信息。时间混合模块具备两大功能：（1）捕捉短期与长期依赖：通过时间混合操作，模型根据不同时间步长之间的关系灵活调整当前输入与前一时间步输入的权重，更好地捕捉序列中的短期和长期依赖关系，能够帮助模型理解历史数据对未来的影响。（2）平滑时间步之间的过渡：时间混合模块通过对当前和前一时间步输入的加权求和，能够确保时间步之间的过渡更加平滑，减少突发变化对模型性能的负面影响。公式表示如下：

$$
r _ {t} = W _ {r} \cdot \left(\mu_ {r} x _ {t} + \left(1 - \mu_ {r}\right) x _ {t - 1}\right) \tag {2-20}
$$

$$
k _ {t} = W _ {k} \cdot \left(\mu_ {k} x _ {t} + \left(1 - \mu_ {k}\right) x _ {t - 1}\right) \tag {2-21}
$$

$$
v _ {t} = W _ {v} \cdot \left(\mu_ {v} x _ {t} + \left(1 - \mu_ {v}\right) x _ {t - 1}\right) \tag {2-22}
$$

RWKV 通过 WKV（Weight-key-value）操作实现信息的选择性记忆和处理，该机制具有线性时间和空间复杂度：

$$
w k v _ {t} = \frac {\sum_ {i = 1} ^ {t - 1} e ^ {- (t - 1 - i) w + k _ {i}} v _ {i} + e ^ {u + k _ {t}} v _ {t}}{\sum_ {i = 1} ^ {t - 1} e ^ {- (t - 1 - i) w + k _ {i}} + e ^ {u + k _ {t}}} \tag {2-23}
$$

$$
o _ {t} = W _ {o} \cdot (\sigma \left(r _ {t}\right) \odot w k v _ {t}) \tag {2-24}
$$

其中， $r_t$ 、 $k_t$  和  $\nu_t$  分别是当前时间步的接收向量 (Receptance)、键向量 (Key) 和值向量 (Value)， $W_r$ 、 $W_k$ 、 $W_\nu$  和  $W_o$  是权重矩阵， $u_r$ 、 $u_k$  和  $u_\nu$  是可训练的权重参数。 $wk\nu_t$  在 RWKV 中代替 Transformer 中自注意力机制 (Attention(Q,K,V)) 的作用，但通过在标量之间进行交互，避免计算上的二次复杂度。随着时间步长  $t$  的增加，向量  $o_t$  对逐步增加的时间步进行加权求和，能够整合更长的历史信息。在目标位置  $t$  处，RWKV 在位置区间  $[1,t]$  内执行加权求和，并与接收函数的输出相乘。

通道混合模块的作用是处理时间序列数据中的不同特征（或通道）之间的相互依赖关系。时间序列数据通常包含多个特征（如多元时间序列），这些特征之间可能存在复杂的相互依赖关系。通道混合模块通过对这些特征进行非线性变换和混合，帮助模型捕捉不同特征之间的相互作用。用公式表示如下：

$$
r _ {t} = W _ {r} \cdot \left(\mu_ {r} x _ {t} + \left(1 - \mu_ {r}\right) x _ {t - 1}\right) \tag {2-25}
$$

$$
k _ {t} = W _ {k} \cdot \left(\mu_ {k} x _ {t} + \left(1 - \mu_ {k}\right) x _ {t - 1}\right) \tag {2-26}
$$

$$
o _ {t} = \sigma \left(r _ {t}\right) \odot \left(W _ {v} \cdot R e L U \left(k _ {t}\right) ^ {2}\right) \tag {2-27}
$$

通道混合模块采用平方ReLU激活函数。接收向量  $r_t$  经过Sigmoid激活后充当“遗忘门”，过滤掉不必要的历史信息，使模型能够选择性保留对未来预测起到关键作用的信息，忽略与任务无关的过去信息。通过这种非线性激活，通道混合模块能进一步增强特征表达能力，使模型能够从复杂的多元数据中提取更深层次的模式与规律。

# 2.2.6 预训练

预训练（Pre-training）是一种通过在大规模数据集上进行初步训练，以学习通用特征表示的方法，然后通过微调（Fine-tuning）使模型适应特定任务[67,68,69]。近年来，预训练模型在时间序列预测领域中得到广泛的应用[70,71]，通过在大量的时间序列数据上进行预训练，模型可以学习到通用的时空依赖模式和特征表示，从而在特定的时间序列预测任务中表现优异。

采用预训练策略的模型，其训练过程通常分为两个阶段：

（1）预训练阶段（Pre-training Stage）。在该阶段，模型在大规模的无标签数据集上进行训练，目标是学习通用特征，如图2-3所示。预训练可以通过自监督学习、无监督学习，或者一些特定任务来实现。例如，在NLP中，BERT模型通过遮盖语言模型（Masked Language Model, MLM）任务进行预训练，即随机遮盖输入句子中的词语，模型的任务是预测这些遮盖的词。

![](images/c8533ffd08ad55bf20a423b4ba933ed6059d8ffc53bfb1f74a5aa7c3c88ff8a3.jpg)



图2-3预训练过程


（2）微调阶段（Fine-tuning Stage）。在预训练之后，模型被转移到特定的下游任务中进行微调。通过使用小规模有标签数据，模型可以适应特定任务的需求，从而提高在该任务上的表现。例如，在BERT中，经过预训练的模型可以通过添加分类层进行情感分析、文本分类等任务的微调；在图像处理任务中，预训练后的模型可以通过微调，适应特定的目标检测或图像分割任务。在预训练阶段，模型已经在大量无标签数据上学习到通用特征表示，微调的过程只需利用少量有标签数据即可实现模型在特定任务上的优化。这种方法不仅能够节省大量标注成本，还能提高模型的泛化能力，特别是在一些数据匮乏的任务中。

时间序列数据往往具有复杂的时间依赖性，通过在大量历史时间序列数据上进行预训练，模型能够捕捉这种长时间依赖关系。并且模型可以通过在多元时间序列数据上进行预训练，学习不同变量之间的交互作用，从而提高对复杂系统的建模能力[71]。例如，在金融预测领域，预训练方法可以用于股票价格预测、市场风险评估等任务，通过历史金融数据的预训练，模型能够更好地理解市场行为和趋势[72]；在气象预测领域，通过在全球历史气象数据上进行预训练，模型能够捕捉到天气变化的全球模式，并在特定地区的气象预测中表现出色。

# 2.3 时间序列处理方法

# 2.3.1 基于Loess的季节-趋势分解

基于Loess的季节-趋势分解（Seasonal-trend Decomposition using Loess，STL）[73]是一种时间序列分解方法，能够将原始序列分解为三个主要成分：趋势（Trend）、季节性（Seasonal）和残差（Residual）。STL的核心思想是通过局部多项式回归（Loess）平滑技术，分别提取出时间序列中的长期趋势、周期性变化和不可解释的噪声部分。与传统的分解方法相比，STL具有更大的灵活性和鲁棒性，适用于各种类型的时间序列数据。

STL 由两个迭代过程组成，分别称为“内循环”和“外循环”。季节性平滑和趋势平滑在内循环中进行，每次迭代中更新季节性成分和趋势成分。假设第  $k$  次内循环结束时的趋势和季节性成分分别为  $T_{t}^{k}$  和  $S_{t}^{k}$  。计算第  $k + 1$  次内循环中的  $T_{t}^{k + 1}$  和  $S_{t}^{k + 1}$  的步骤如下：

（1）去趋势。计算去趋势序列  $Y_{t}^{detrend} = Y_{t} - T_{t}^{k}$  。如果有缺失的时间步，那么此时间步上的  $Y_{t}^{detrend}$  也会缺失。

(2) 季节性平滑。使用 Loess 滤波器对  $Y_{t}^{detrend}$  进行平滑, 得到初步的季节性成分  $\hat{S}_{t}^{k+1}$  。

（3）低通滤波。使用低通滤波器处理  $\hat{S}_t^{k + 1}$  ，然后使用Loess滤波器进一步平滑残余的趋势成分  $\hat{T}_t^{k + 1}$  。

(4) 去除平滑后的季节性趋势。计算第  $k + 1$  次内循环中的季节性成分  $\hat{S}_{t}^{k + 1} - \hat{T}_{t}^{k + 1}$  。

（5）去季节性。将原始序列中的季节性成分去除，得到去季节性的时间序列  $Y_{t}^{\text{detrend}} = Y_{t} - S_{t}^{k + 1}$ 。

（6）趋势平滑。使用Loess滤波器进行平滑处理  $Y_{t}^{detrend}$  得到趋势成分  $T_{t}^{k + 1}$  。

在完成内循环后，初始序列被分解为趋势成分和季节性成分，残差部分则通过外循环计算得到：  $R_{t}^{k + 1} = Y_{t} - T_{t}^{k + 1} - S_{t}^{k + 1}$  。

STL分解广泛应用于时间序列预测，尤其适合存在显著趋势和季节性特征的数据。通过STL分离时间序列数据的季节性成分，能够帮助使模型更准确地识别时间序列的趋势和周期性变化。经过STL分解后的时间序列更易于传统统计模型（如ARIMA）或深度学习模型（如LSTM、Transformer）建模，可以进一步提高预测性能。例如，在电力负荷预测中，季节性和趋势性变化非常明显。STL通过分解负荷数据，提取出日间和季节性波动，以及长期增长趋势，从而提高对未来负荷的预测精度[74]；在股票市场和外汇市场中，价格和交易量通常受到季节性因素（如节假日效应、季度财报等）和长期趋势的影响，STL能够帮助分析师分离这些因素，更好地理解市场行为并进行预测[75]；交通流量数据受到日间、周末、假期等季节性因素的影响，同时还可能存在随着城市发展而产生出长期趋势，通过STL分解，交通管理部门可以更好地理解流量模式，从而优化交通控制和基础设施规划[76]。

# 2.3.2 快速傅里叶变换

快速傅里叶变换（Fast Fourier Transform, FFT）是一种高效的算法，用于计算傅里叶变换，将时间域信号转换为频域信号，从而便于分析其频率成分。傅里叶变换是信号分析与处理中的关键工具，FFT能够极大优化计算过程，大幅度提高计算速度，成为信号处

理、数据分析以及图像处理等多个领域广泛应用的核心算法。

傅里叶变换的核心思想是将一个时间序列或信号分解为不同频率的正弦波的叠加，从而分析信号的频率成分。对于一个长度为  $N$  的离散时间序列  $x[n]$ ，其离散傅里叶变换（Discrete Fourier Transform, DFT）的公式为：

$$
X [ k ] = \sum_ {n = 0} ^ {N - 1} x [ n ] \cdot e ^ {- j \frac {2 \pi}{N} k n}, k = 0, 1, 2, \dots , N - 1 \tag {2-28}
$$

其中， $X[k]$  是频域中的第  $k$  个频率分量， $x[n]$  是时间域中的第  $n$  个数据点， $e^{-j\frac{2\pi}{N} kn}$  是复指数函数，代表频率为  $k$  的正弦波。

直接计算DFT的复杂度为  $O(N^2)$ ，而FFT能够将计算复杂度降低到  $O(N\log N)$ ，大幅降低处理大规模数据的计算成本。FFT的核心思想基于“分治法”，通过递归地将原始序列分解为较小的子序列，再分别对其进行傅里叶变换并合并结果。Cooley-Tukey算法是最常见的FFT实现方式之一，它通过将序列拆分为偶数和奇数点两部分，分别计算傅里叶变换，最终结合两部分结果得到整体的傅里叶变换。其公式为：

$$
X [ k ] = X _ {\text {e v e n}} [ k ] + e ^ {- j \frac {2 \pi k}{N}} X _ {\text {o d d}} [ k ], k = 0, 1, 2, \dots , \frac {N}{2} - 1 \tag {2-29}
$$

$$
X [ k + \frac {N}{2} ] = X _ {\text {e v e n}} [ k ] - e ^ {- j \frac {2 \pi k}{N}} X _ {\text {o d d}} [ k ], k = 0, 1, 2, \dots , \frac {N}{2} - 1 \tag {2-30}
$$

其中， $X_{even}[k]$  和  $X_{odd}[k]$  分别是偶数点和奇数点的傅里叶变换。

在时间序列预测中，FFT的核心应用是通过频域分析识别周期性成分，从而提升预测与数据分析的准确性。时间序列中通常隐藏着周期性特征，FFT将这些序列转换至频域后，能清晰地揭示这些周期性信息。尤其是在周期性较强的时间序列（如气象数据[77]、金融数据[78]）中，这些信息对于构建准确的预测模型至关重要。此外，时间序列中的噪声通常表现为高频成分，FFT可帮助识别并去除这些高频噪声，使数据更为平滑，进一步增强预测的准确性。通过FFT分解，时间序列可以提取出一系列频域特征，这些特征可用于作为机器学习模型的输入，提高预测精度。

# 2.3.3 通道独立策略

通道独立（Channel-independent, CI）策略是近年来在时间序列预测领域中受到广泛

关注的一种技术方法[70,72]。多元时间序列预测模型，通常面临如何有效利用多个通道（即多个变量或特征）之间信息的挑战。通道独立策略旨在通过独立处理每个通道的特征，使得模型能够更好地捕捉各个通道自身的动态特征，避免不相关序列带来的噪声干扰，从而提升预测的性能。

![](images/cf835efbe470be9f22347584836586e8d42e79b1f510d5ab62b1b2bd2b9c4377.jpg)



图2-4通道独立策略


如图2-4所示，多元时间序列  $x \in R^{M \times L}$  包含多个子序列，普通的通道混合方式直接将多元时间序列投影到嵌入空间中进行信息混合。而通道独立策略将多元时间序列中的每个子序列单独输入到预测模型进行处理，侧重于独立提取每个子序列内的特征，然后在模型后期或者输出阶段进行通道混合。通道独立策略分为两个阶段：（1）通道独立建模：在该阶段中，每个通道的时间序列数据由独立的子模型处理，这些子模型可以是相同的模型结构，也可以根据通道的特性进行选择，采用不同的模型结构。通过这种方式，能够减少通道间的噪声干扰，使模型能够专注于学习每个通道的时间依赖关系，更加精准地捕捉各通道的特性。（2）通道特征融合：通道独立策略并未完全忽略通道之间的关系，而是在模型的输出层或中间层实现特征融合。这种方式允许模型在更高层次上捕捉通道间的交互作用，同时确保每个通道的独立特征提取不受影响。

在时间序列预测任务中，尤其是处理多元时间序列时，通道独立策略展现出明显的优势。例如，（1）在金融市场预测领域，不同的资产（如股票、债券、商品等）各自具有独特的时间动态变换，通道独立策略可以通过对每种资产的时间序列进行单独建模，捕捉其独特的波动模式。然后经过特征融合，将独立的特征进行整合，用于对整体市场的预测。（2）工业传感器数据预测，工业生产系统通常涉及大量的传感器，每个传感器记录的数据反映出生产系统不同部分的工作状态。通道独立策略可以帮助模型更准确地理解每个传感器的时间动态特征，进而通过后续融合提高对整体系统状态的预测能力。

(3) 在气象预测领域, 气象数据通常由多个变量构成, 如温度、湿度、气压等。采用通道独立策略, 将每个变量的时间序列单独建模, 以捕捉其独特的季节性和趋势特征, 有

利于进一步提高气象预测精度。

# 2.4 本章小结

本章首先介绍时间序列预测基本理论，并根据输入数据类型和预测时间跨度对时间序列预测任务进行分类。然后，介绍与本文相关的深度学习理论，包括TCN、GCN、Transformer、MLP、RWKV以及预训练，同时还介绍了时间序列处理方法，如STL、FFT和通道独立策略。

在后续章节中，第三章采用STL方法分析时间序列数据中的长期趋势和波动，并使用MLP和TCN建模时间序列的时空依赖关系。第四章通过FFT分析时间序列的多周期特征，结合RWKV和GCN学习时间序列的多尺度时空依赖，并在预训练阶段应用通道独立策略减轻序列间的噪声干扰。第五章针对短期光伏功率预测任务，利用FFT分析光伏发电功率及其外部气象条件的多周期性，结合RWKV和TCN，捕捉数据中的多尺度时空依赖关系。第六章针对长期光伏功率预测任务，基于Transformer框架，设计了分解聚合注意力机制，应用STL方法对光伏功率数据进行分解，捕捉其长期趋势和季节性波动，并设计了基于MLP的残差块用于蒸馏操作，以降低注意力机制的计算复杂度。

# 第三章 基于季节-趋势分解的二维时间卷积密集网络预测模型

近年来，基于Transformer的方法在时间序列预测中得到广泛应用，但最近的一些研究表明，许多复杂的Transformer模型在长期多元时间序列预测中的表现不如简单的基于MLP的模型。并且现有的许多方法忽视了数据中的长期特征，如长期的趋势和季节性波动，而这些特征对长期多元时间序列预测的准确性至关重要。为应对这些挑战，进一步提升预测效果，本章提出了一种基于季节-趋势分解的二维时间卷积密集网络（Seasonal-trend Decomposition-based 2-dimensional Temporal Convolutional Dense Network，STL-2DTCDN）。该方法结合了基于Loess的季节-趋势分解（STL），以识别数据中的长期趋势和季节性特征，并结合MLP和TCN，设计了二维时间卷积密集网络（2DTCDN）以捕捉分解后的序列中复杂的相互依赖关系。为评估该方法，我们在六个数据集上进行了实验，结果表明，STL-2DTCDN在长期多元时间序列预测任务中表现优于现有方法，显著提升了预测精度。

# 3.1 引言

长期多元时间序列预测在交通[79]、气象[80]、能源管理[81]、金融活动[82]和环境保护[83]等多个实际领域中发挥着重要作用。通过利用大量历史数据预测未来趋势，这些预测能为决策制定和规划提供强有力的支持。根据变量的数量，时间序列预测任务可以分为多元和单变量预测。相比单变量预测，长期多元时间序列预测更具挑战，但其在实际应用中更加广泛。因此，开发能够提升长期多元时间序列预测性能的算法尤为关键。

研究者们已经提出多种时间序列预测方法。传统的基于统计学的模型，如AR[4]、ARIMA[84]和ES[85]，主要应用于单变量时间序列的预测。然而，传统方法在处理长期多元时间序列的复杂非线性依赖关系时常常表现出不足。近年来，深度学习方法在时间序列预测领域取得飞速进展。RNN作为序列建模的重要工具，广泛应用于自然语言处理等领域[86]。鉴于时间序列数据的顺序特性，许多基于RNN的模型及其变体被应用于时间序列预测[87,88,89,90]。此外，CNN和TCN也被广泛应用于时间序列预测，以增强模型捕捉局部

时间特征的能力[91,92]。为提高预测结果的稳健性和可靠性，一些研究还提出通过集成多种模型的预测方法[93,94,95]。

近年来，Transformer[10]在图像处理[96]和自然语言处理[97]领域已取得显著成果。与RNN相比，基于Transformer的模型在捕捉长期依赖关系方面更具优势。因此，许多研究基于Transformer架构设计出多种预测方法。然而，Transformer的自注意力机制在时间和内存复杂度上的二次复杂性限制其在长期时间序列预测中的广泛应用。为解决这些问题，一些研究为Transformer设计出更为高效的模块。例如，Logtran[98]通过因果卷积与Transformer相结合，引入LogSparse自注意力机制。Reformer[99]采用局部敏感哈希（Locality-sensitive Hashing, LSH）替代传统自注意力，并使用可逆残差来优化训练。Informer[100]通过ProbSparse自注意力机制，专注于提取重要的信息。Autoformer[101]则设计出自相关机制，结合序列分解来优化Transformer架构。Fedformer[102]使用傅里叶变换提升性能，而PatchTST[70]则通过将时间序列切分为小块进行输入，进一步改进模型的时空表示。CNformer[103]提出基于卷积神经网络的编码器-解码器结构，以替代传统的注意力机制。

尽管基于Transformer的预测算法已在长期时间序列预测中取得重要成果，但一些最新的研究表明，在许多时间序列预测任务中，简单的线性模型表现得同样出色，甚至优于复杂的Transformer模型。例如，Zeng等人[61]提出的线性模型在多个基准测试中超越了Transformer的表现。另外，Das等人[62]设计了一种新的时间序列密集编码器（Time-series Dense Encoder, TiDE）模型，能够捕捉非线性依赖，同时保持线性模型的高效性。

虽然Transformer和线性模型在时间序列预测中都做出了重要贡献，但它们在捕捉长期复杂依赖关系和利用数据中的长期趋势和季节性特征方面仍然存在不足。为解决这些问题，本章提出一种基于季节-趋势分解的二维时间卷积密集网络（STL-2DTCDN）。STL-2DTCDN通过  $\mathsf{STL}^{[73]}$  方法将原始时间序列分解为季节性、趋势和残差三个部分，并使用2DTCDN捕捉多元时间序列中的复杂依赖关系。本章节主要研究内容如下：

（1）提出一种适用于长期多元时间序列预测的STL-2DTCDN模型，该模型结合线性模型和TCN的混合架构，并融入STL方法。在六个多元时间序列数据集上，STL-2DTCDN展现出先进的预测性能，大幅提升预测的准确性，验证其在处理复杂长期依赖

关系中的有效性。

(2) 提出二维时间卷积密集网络（2DTCDN）。2DTCDN 结合二维卷积核、因果卷积、扩张卷积和多层感知机，能够有效捕捉多元时间序列中复杂相互依赖关系。

(3) 引入 STL 方法以识别时间序列中的长期趋势和波动。通过 STL 分解, 原始时间序列被分解为季节性、趋势和残差三个子序列, 这些子序列有效反映出数据的周期变化和趋势, 为预测模型提供更精准的特征支持。

本章其余部分的工作如下：在3.2节中，首先定义多元时间序列预测问题并详细介绍STL-2DTCDN模型框架；在3.3节中，介绍实验数据集、实验设置，然后分析实验结果；在3.4节中总结本章的主要工作。

# 3.2基于季节-趋势分解的二维时间卷积密集网络

# 3.2.1 问题定义

多元时间序列预测问题公式表达如下：给定一段历史时间数据，记作  $Y_{1:L} = \{y_1^t, y_2^t, \dots, y_c^t\}_{t=1}^L$ ，其中  $L$  是固定的回溯窗口大小， $c(c > 1)$  是变量的数量， $y_i^t$  表示第  $t$  个时间步第  $i$  个变量的值。多元时间序列预测任务的目标是预测序列  $\hat{Y}_{L+1:L+H} = \{\hat{y}_1^t, \hat{y}_2^t, \dots, \hat{y}_c^t\}_{t=L+1}^{L+H}$ ，其中  $\hat{y}_i^t$  是第  $t$  个时间步第  $i$  个变量的预测结果， $H(H > 1)$  表示预测的时间步数量。时间段从  $L + 1$  到  $L + H$  的实际数据记作  $Y_{L+1:L+H} = \{y_1^t, y_2^t, \dots, y_c^t\}_{t=L+1}^{L+H}$ 。长期多元时间序列预测的目标是预测  $\hat{Y}$ ，并且  $H$  值较大（ $H >> 1$ ）。

# 3.2.2 模型框架

STL-2DTCDN 框架的整体结构如图 3-1 所示。首先，我们采用 STL 方法，将原始时间序列分解为趋势、季节性和残差成分，这一分解有助于模型理解不同的时间模式。然后，将趋势、季节性和残差成分与时间特征结合，每个子序列分别通过包含 2DTCDN 模块的编码器进行处理。编码器生成的特征表示随后被拼接，作为解码器的输入，进一步提取时间序列的深层特征。最终，解码器的输出与时间特征再次结合，经过密集层

(Dense Layer), 生成最终预测结果。

![](images/97ba6c9d56db91787482e9e461da6580ffde81069ed95115c1104dff8901fc2a.jpg)



图3-1 STL-2DTCDN模型框架


# 3.2.3 季节-趋势分解

STL 是一种广泛应用于时间序列分析的方法，它能够将原始时间序列分解为趋势、季节性和残差三个不同的组成部分，其公式如下：

$$
Y _ {t} = T _ {t} + S _ {t} + R _ {t} \tag {3-1}
$$

其中， $t = 1,2,\dots,n$  表示时间步， $Y_{t}$  表示原始时间序列数据， $T_{t}$ ， $S_{t}$  和  $R_{t}$  分别表示趋势、季节性和残差部分。与传统的分解方法相比，STL 能够提供更加稳健的时间序列分解结果，特别是在时间序列中包含异常值的情况下，STL 通过平滑和分解过程，能够提供更加稳健的时间序列趋势和季节性成分，从而提高模型的鲁棒性。

STL 的参数设置在一些研究中已被深入探讨，因此本章基于以往研究的成果[104]，参考文献中推荐的参数值，设定 STL 的参数，以确保分解过程的稳定性和准确性。图 3-2 展示了使用推荐参数进行 STL 分解后的时间序列结果，该图清晰展示出分解后的趋势、季节性和残差子序列。

![](images/ca622a0933b9c46f1e1879219fc2f7f9d9dcef97c2db025f7a8b3be68e8c8b83.jpg)


![](images/e0d2a7af9bac7cd3109d6ed88ecdcefd2c7735b4ef509dba07d604040c59536c.jpg)



图3-2 STL的分解结果：(a)原始数据(b)原始数据分解结果


# 3.2.4 二维时间卷积密集网络

TCN是一种适用于长序列建模的有效方法。与传统的RNN相比，TCN采用卷积的方法来挖掘时间序列中的复杂依赖关系。TCN的架构如图3-3所示，其由多个层组成，并包含一个可选的  $1\times 1$  卷积。TCN引入扩展因果卷积，扩大感受野，从而能够捕捉时间序列中不同时间尺度的特征。对于一维序列  $X\in R^{M}$  和扩展率为  $d$  的卷积核  $K_{d}$ ，扩展因果卷积的操作定义如下：

$$
\hat {X} (t) = \sum_ {\tau = 1} ^ {l} X (t - (d \cdot \tau)) \cdot K _ {d} (\tau) \tag {3-2}
$$

其中， $\hat{X}(t)$  是通过扩展因果卷积处理后的第  $t$  个输出元素， $X(t - (d \cdot \tau))$  表示输入序列  $X$  的第  $t - (d \cdot \tau)$  个元素， $K_d(\tau)$  表示卷积核的第  $\tau$  个元素， $l$  是卷积核的长度。

![](images/d0d426b590f708d7e2d9bb6487e07b419e596d36510846cabbb71661a9ed0b81.jpg)



图3-3TCN模型图


标准TCN的卷积核是一维的，只能沿着时间序列的时间维度进行卷积。因此，传统的TCN在捕捉多元时间序列数据中各个时间序列之间的相互依赖关系方面存在一定的局限性。为更好地使TCN适应复杂的多元时间序列预测任务，我们对传统的TCN进行改进，提出2DTCDN架构。2DTCDN的架构如图3-4所示。

![](images/72e91b567e049f479c287ec0dc84d486b1249a8fc4fdffa781a644e6f2357b52.jpg)



图3-42DTCDN模型图


因果卷积是一个重要概念，它的作用是限制时间  $t$  的输出仅受不晚于  $t$  的元素影响，确保未来不会影响过去。这一概念被称为信息泄漏，在时间序列预测中至关重要，最早由Waibel等人[105]在30多年前提出。为保持输入层与卷积层的一致维度，通常在隐藏层中使用零值进行填充。然而，由于在所提出的2DTCDN中使用的是二维卷积核，其填充方法和卷积过程与一维扩展因果卷积不同。对于一个二维序列  $X \in R^{M \times N}$  和扩展率为  $d$  的二维卷积核  $K_{d}$ ，二维扩展因果卷积的操作公式如下：

$$
\hat {X} (i, j) = \sum_ {m = 1} ^ {h} \sum_ {n = 1} ^ {w} X (i - (m - 1) \cdot d, j - (n - 1) \cdot d) \cdot K _ {d} (m, n) \tag {3-3}
$$

其中， $\hat{X}(i,j)$  是通过二维扩展因果卷积处理后的第  $(i,j)$  个输出元素， $X(i - (m - 1)\cdot d, j - (n - 1)\cdot d)$  表示输入二维矩阵  $X$  中的第  $(i - (m - 1)\cdot d, j - (n - 1)\cdot d)$  个元素， $K_d(m,n)$  表示二维卷积核第  $(m,n)$  个元素， $h$  和  $w$  分别表示二维卷积核的高度和宽度。

![](images/fc365313de78fed18480e02f96532586fae06ec5d91b235d1aa1c721ff8850d2.jpg)



图3-52DTCDN的填充方法


2DTCDN的填充方法如图3-5所示，其中卷积核大小设置为  $3\times 3$  ，扩展率设置为1。在时间维度上的填充方式与一维因果卷积类似，填充长度计算为(kernel_size-1)×dilation。在特征维度上的填充，是通过将前(kernel_size-1)×dilation的特征进行复制，并将其放置在最后一个特征之后作为填充数据。

扩展卷积是传统卷积操作的一种变体。在标准卷积中，卷积核以固定步幅滑动，每个卷积核的权重与相邻输入数据交互。而扩展卷积通过在卷积核的权重之间引入间隔，不改变分辨率的情况下扩大感受野，从而捕捉更广泛的信息。二维扩展因果卷积的过程如图3-6所示，卷积核大小设置为  $3 \times 3$  ，扩展率和步幅均设置为1。

![](images/0557bbb39e0c8f1ee4c21a435e3f70baca7748b7cc27c820669ddb3133a6463d.jpg)



图3-62DTCDN的扩展因果卷积过程


残差连接是深度神经网络中的一种重要组件，最早由He等人[106]提出。残差连接的主要目的是在不同层之间添加跳跃连接，减轻梯度消失问题，从而为反向传播过程中的梯度传递提供捷径。在提出的2DTCDN中，我们将密集层与残差连接相结合，进一步提

升模型性能。

Zeng 等人[61]和 Das 等人[62]的最新研究表明，简单的线性模型在时间序列预测任务中也表现出了良好的效果。单层的线性模型同样能够有效地捕捉序列数据中的复杂相互依赖关系，使神经网络能够深入挖掘对任务至关重要的特征。在提出的 2DTCDN 中，我们将 TiDE 的残差块与二维扩展因果卷积进行整合，以进一步提升模型对长时间依赖关系的建模能力。

# 3.3 实验

# 3.3.1 实验数据集

本章提出的STL-2DTCDN模型在六个数据集上进行了实验。所有数据集按照时间顺序分为三个部分：训练集、验证集和测试集。对于Traffic和Electricity数据集，划分比例为7:1:2。而对于ETT数据集，根据Informer[100]和Autoformer[101]的建议，以6:2:2的比例进行划分。六个数据集的统计信息如表3-1所示。

- ETT（电力变压器温度）：包括两个以1小时为间隔的数据集（ETTh1, ETTh2）和两个以15分钟为间隔的数据集（ETTm1, ETTm2）。每个数据集包含七个电力变压器的属性。

- Traffic: 收集来自加利福尼亚的交通数据。

- Electricity: 描述 321 个客户的电力消耗情况（单位为千瓦时）。


表 3-1 六个数据集的统计信息


<table><tr><td>数据集</td><td>Electricity</td><td>ETTh1</td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td><td>Traffic</td></tr><tr><td>特征数</td><td>321</td><td>7</td><td>7</td><td>7</td><td>7</td><td>862</td></tr><tr><td>时间步</td><td>26304</td><td>17420</td><td>17420</td><td>69680</td><td>69680</td><td>17544</td></tr><tr><td>频率</td><td>1小时</td><td>1小时</td><td>1小时</td><td>15分钟</td><td>15分钟</td><td>1小时</td></tr></table>

# 3.3.2 实验设置

目前，基于深度学习的模型已成为时间序列预测中的主要方法。我们选择了六种基于深度学习的基线方法与本章提出的STL-2DTCDN进行对比。选定的基线方法包括三类： $\mathrm{TCN}^{[31]}$ 、基于Transformer的方法（Informer[100]、Autoformer[101]、PatchTST/64[70]、

FEDformer[102]和线性模型（DLinear[61]和TiDE[62]）。TCN利用因果卷积，进一步增强其处理长期依赖关系的能力。近年来，Informer和Autoformer等基于Transformer的方法在时间序列预测任务中表现出显著优势。此外，研究表明，DLinear和TiDE等线性模型在各种预测任务中也能取得令人满意的效果。

TiDE在所有预测长度的实验中都使用固定的720长度的回溯窗口。其他对比模型则按照推荐的回溯窗口设置。对比方法的结果来自  $\mathrm{TiDE}^{[62]}$  和  $\mathrm{PatchTST}^{[70]}$  。

为评估模型的性能，本章采用平均绝对误差（Mean Absolute Error, MAE）和均方误差（Mean Square Error, MSE）作为评价指标。它们的定义如下：

$$
M A E = \frac {1}{c \times H} \sum_ {j = 1} ^ {c} \sum_ {i = 1} ^ {H} \left| y _ {t + i} ^ {j} - \hat {y} _ {t + i} ^ {j} \right| \tag {3-4}
$$

$$
M S E = \frac {1}{c \times H} \sum_ {j = 1} ^ {c} \sum_ {i = 1} ^ {H} \left(y _ {t + i} ^ {j} - \hat {y} _ {t + i} ^ {j}\right) ^ {2} \tag {3-5}
$$

其中， $t$  表示时间步长， $H$  表示预测的时间范围， $c$  表示子序列的数量。

本章提出的STL-2DTCDN模型使用L2损失函数进行训练，并采用ADAM优化器，初始学习率为  $10^{-4}$  。在所有的预测长度{96,192,336,720}中，根据TiDE的设置，回溯窗口均设置为720。批次大小设置为32，实验重复进行五次。实验在两张NVIDIA GeForce RTX 2080 Ti显卡上进行，并在PyTorch深度学习框架下实现。STL-2DTCDN模型所涉及的超参数范围如表3-2所示。我们通过在验证数据集上进行滚动验证，利用验证误差来调整这些超参数。


表 3-2 超参数的范围


<table><tr><td>超参数</td><td>取值范围</td></tr><tr><td>编码器层数</td><td>[1,2,3]</td></tr><tr><td>解码器层数</td><td>[1,2,3]</td></tr><tr><td>卷积核（时间维度）</td><td>[6,12,24,48]</td></tr><tr><td>卷积核（特征维度）</td><td>[1,2,3,4]或[4,8,12,16]</td></tr><tr><td>隐藏层维度</td><td>[256,512,1024]</td></tr><tr><td>时间特征维度</td><td>[16,32,64,128]</td></tr></table>

由于在所有的六个数据集中，回溯窗口的大小与数据集子序列的数量差异较大，因此卷积核的两个维度（时间维度和特征维度）是分别设计的。考虑到不同数据集中子序列数量的差异，例如 Electricity 和 Traffic 数据集中的特征数量较多，我们在特征维度上使

用较大的参数[4, 8, 12, 16]；而对于像 ETT 特征较少的数据集，则在特征维度上使用较小的参数[1, 2, 3, 4]。六个数据集的超参数设置如表 3-3 所示。


表 3-3 超参数的设置


<table><tr><td>超参数</td><td>ETTh1</td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td><td>Electricity</td><td>Traffic</td></tr><tr><td>编码器层数</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>解码器层数</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>卷积核（时间维度）</td><td>24</td><td>24</td><td>6</td><td>6</td><td>24</td><td>24</td></tr><tr><td>卷积核（特征维度）</td><td>3</td><td>3</td><td>3</td><td>3</td><td>12</td><td>16</td></tr><tr><td>隐藏层维度</td><td>512</td><td>512</td><td>512</td><td>512</td><td>512</td><td>512</td></tr><tr><td>时间特征维度</td><td>64</td><td>32</td><td>64</td><td>128</td><td>128</td><td>64</td></tr></table>

# 3.3.3 实验结果分析

实验结果如表3-4所示。表中的每一行对应于特定预测长度下的结果对比，而每一列代表某个模型在所有预测长度下的结果。用粗体标出的数值表示最优结果。

从表3-4可以观察到：（1）STL-2DTCDN在大多数情况下都能取得最优结果（从最后一行的最优结果数可以看出)。（2）STL-2DTCDN的性能优于TiDE，MSE分别平均下降了  $3.2\%$  （在预测长度为96时），  $5.7\%$  （在预测长度为192时），  $5.8\%$  （在预测长度为336时），以及  $6.9\%$  （在预测长度为720时）。预测步长越长，STL-2DTCDN的表现越好。这表明STL-2DTCDN更适合长期预测。（3）STL-2DTCDN在大型数据集的长期预测中明显优于TiDE。对于Traffic数据集和Electricity数据集，MSE分别减少了  $10.1\%$  （在预测长度为720时）和  $13.8\%$  （在预测长度为720时）。然而，对于四个ETT数据集，在预测长度为720时，STL-2DTCDN的MSE平均仅比TiDE减少  $4.4\%$  ，这是因为Traffic和Electricity数据集的特征数量显著多于四个ETT数据集，2DTCDN可以在特征维度上利用更大尺寸的卷积核，以更好地探索不同时间序列之间的相互依赖关系。

STL-2DTCDN与TiDE在Traffic和Electricity数据集上的预测结果与真实值的对比如图3-7和图3-8所示。从这些图中可以清晰看到，STL-2DTCDN在捕捉时间序列中的周期性模式和整体趋势拟合方面表现出色。这表明，STL-2DTCDN能够有效识别数据中的时间特征，包括季节性波动和长期趋势，提高了预测的准确性。


表 3-4 六个数据集的预测结果


<table><tr><td colspan="2">模型</td><td colspan="2">Proposed</td><td>TiDE</td><td>PatchTST64</td><td>DLinear</td><td>FEDformer</td><td>Autoformer</td><td>Informer</td><td>TCN</td></tr><tr><td colspan="2">评价指标</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td></tr><tr><td rowspan="4">Traffic</td><td>96</td><td>0.305 0.242</td><td>0.336 0.253</td><td>0.360 0.249</td><td>0.410 0.282</td><td>0.576 0.359</td><td>0.597 0.371</td><td>0.733 0.410</td><td>1.532 0.821</td><td></td></tr><tr><td>192</td><td>0.317 0.256</td><td>0.346 0.257</td><td>0.379 0.256</td><td>0.423 0.287</td><td>0.610 0.380</td><td>0.607 0.382</td><td>0.777 0.435</td><td>1.550 0.826</td><td></td></tr><tr><td>336</td><td>0.324 0.246</td><td>0.355 0.260</td><td>0.392 0.264</td><td>0.436 0.296</td><td>0.608 0.375</td><td>0.623 0.387</td><td>0.776 0.434</td><td>1.556 0.834</td><td></td></tr><tr><td>720</td><td>0.347 0.267</td><td>0.386 0.273</td><td>0.432 0.286</td><td>0.466 0.315</td><td>0.621 0.375</td><td>0.639 0.395</td><td>0.827 0.466</td><td>1.589 0.840</td><td></td></tr><tr><td rowspan="4">Electricity</td><td>96</td><td>0.129 0.230</td><td>0.132 0.229</td><td>0.129 0.222</td><td>0.140 0.237</td><td>0.186 0.302</td><td>0.196 0.313</td><td>0.304 0.393</td><td>1.103 0.902</td><td></td></tr><tr><td>192</td><td>0.141 0.238</td><td>0.147 0.243</td><td>0.147 0.240</td><td>0.153 0.249</td><td>0.197 0.311</td><td>0.211 0.324</td><td>0.327 0.417</td><td>1.113 0.898</td><td></td></tr><tr><td>336</td><td>0.152 0.256</td><td>0.161 0.261</td><td>0.163 0.259</td><td>0.169 0.267</td><td>0.213 0.328</td><td>0.214 0.327</td><td>0.333 0.422</td><td>1.167 0.915</td><td></td></tr><tr><td>720</td><td>0.169 0.287</td><td>0.196 0.294</td><td>0.197 0.290</td><td>0.203 0.301</td><td>0.233 0.344</td><td>0.236 0.342</td><td>0.351 0.427</td><td>1.334 0.935</td><td></td></tr><tr><td rowspan="4">ETTh1</td><td>96</td><td>0.376 0.404</td><td>0.375 0.398</td><td>0.379 0.401</td><td>0.375 0.399</td><td>0.376 0.415</td><td>0.435 0.446</td><td>0.941 0.769</td><td>1.534 1.079</td><td></td></tr><tr><td>192</td><td>0.401 0.427</td><td>0.412 0.422</td><td>0.413 0.429</td><td>0.412 0.420</td><td>0.423 0.446</td><td>0.456 0.457</td><td>1.007 0.786</td><td>1.566 1.087</td><td></td></tr><tr><td>336</td><td>0.412 0.426</td><td>0.435 0.433</td><td>0.435 0.436</td><td>0.439 0.443</td><td>0.444 0.462</td><td>0.486 0.487</td><td>1.038 0.784</td><td>1.614 1.077</td><td></td></tr><tr><td>720</td><td>0.426 0.477</td><td>0.454 0.465</td><td>0.446 0.464</td><td>0.472 0.490</td><td>0.469 0.492</td><td>0.515 0.517</td><td>1.144 0.857</td><td>1.657 1.102</td><td></td></tr><tr><td rowspan="4">ETTh2</td><td>96</td><td>0.260 0.331</td><td>0.270 0.336</td><td>0.274 0.337</td><td>0.289 0.353</td><td>0.332 0.374</td><td>0.332 0.386</td><td>1.549 0.952</td><td>1.876 1.345</td><td></td></tr><tr><td>192</td><td>0.314 0.371</td><td>0.332 0.380</td><td>0.338 0.376</td><td>0.383 0.418</td><td>0.407 0.446</td><td>0.426 0.434</td><td>3.792 1.542</td><td>1.547 1.752</td><td></td></tr><tr><td>336</td><td>0.337 0.395</td><td>0.360 0.407</td><td>0.363 0.397</td><td>0.448 0.465</td><td>0.400 0.447</td><td>0.477 0.479</td><td>4.215 1.642</td><td>1.791 1.749</td><td></td></tr><tr><td>720</td><td>0.390 0.426</td><td>0.419 0.451</td><td>0.393 0.430</td><td>0.605 0.551</td><td>0.412 0.469</td><td>0.453 0.490</td><td>3.656 1.619</td><td>2.501 3.622</td><td></td></tr><tr><td rowspan="4">ETTm1</td><td>96</td><td>0.287 0.325</td><td>0.306 0.349</td><td>0.293 0.346</td><td>0.299 0.343</td><td>0.326 0.390</td><td>0.510 0.492</td><td>0.626 0.560</td><td>1.869 1.125</td><td></td></tr><tr><td>192</td><td>0.315 0.346</td><td>0.335 0.366</td><td>0.333 0.370</td><td>0.335 0.365</td><td>0.365 0.415</td><td>0.514 0.495</td><td>0.725 0.619</td><td>1.453 1.790</td><td></td></tr><tr><td>336</td><td>0.347 0.368</td><td>0.364 0.384</td><td>0.369 0.392</td><td>0.369 0.386</td><td>0.392 0.425</td><td>0.510 0.492</td><td>1.005 0.741</td><td>1.647 1.676</td><td></td></tr><tr><td>720</td><td>0.408 0.416</td><td>0.413 0.413</td><td>0.416 0.420</td><td>0.425 0.421</td><td>0.446 0.458</td><td>0.527 0.493</td><td>1.133 0.854</td><td>1.827 2.807</td><td></td></tr><tr><td rowspan="4">ETTm2</td><td>96</td><td>0.164 0.253</td><td>0.161 0.251</td><td>0.166 0.256</td><td>0.167 0.260</td><td>0.180 0.271</td><td>0.205 0.293</td><td>0.355 0.462</td><td>1.633 1.258</td><td></td></tr><tr><td>192</td><td>0.198 0.270</td><td>0.215 0.289</td><td>0.223 0.296</td><td>0.224 0.303</td><td>0.252 0.318</td><td>0.278 0.336</td><td>0.595 0.586</td><td>1.996 1.385</td><td></td></tr><tr><td>336</td><td>0.256 0.311</td><td>0.267 0.326</td><td>0.274 0.329</td><td>0.281 0.342</td><td>0.324 0.364</td><td>0.343 0.379</td><td>1.270 0.871</td><td>1.872 1.221</td><td></td></tr><tr><td>720</td><td>0.341 0.379</td><td>0.352 0.383</td><td>0.362 0.385</td><td>0.397 0.421</td><td>0.410 0.420</td><td>0.414 0.419</td><td>3.001 1.267</td><td>2.003 1.438</td><td></td></tr><tr><td colspan="2">Count</td><td>38</td><td>5</td><td>4</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td></td></tr></table>

![](images/2ae992544d9680ed0df7297ba0fe6944eec5acbeb8d91e3229fddee169ee83f0.jpg)


![](images/a0a7e1c748eb09f4f5acb429a73c6a81ef295c8da0f4880eae2ff02ddcc988ac.jpg)



图3-7STL-2DTCDN和TiDE在Traffic数据集上的比较(预测步长720)：(a)STL-2DTCDN(b)TiDE


![](images/f0c529828120ef451be379f974a7225e14e1403c62cf2c20b1f583df08b6204a.jpg)


![](images/c53dd08e1e706f2c1ab3e0c8adfaa04cc9a3125ff011bfecf7c90b5ecba6f207.jpg)



图3-8STL-2DTCDN和TiDE在Electricity数据集上的比较(预测步长720)：(a)STL-2DTCDN(b)TiDE


# 3.3.4消融实验

通过消融实验，我们进一步分析 STL-2DTCDN 中每个模块的贡献。依次从 STL-2DTCDN 中移除每个模块，并评估剩余模块组成的子框架的性能。每个子框架的详细定义如下：

- wo/ STL: 从 STL-2DTCDN 中移除 STL。

- wo/2DTCDN:从STL-2DTCDN中移除2DTCDN。

- wo/ Time features: 从 STL-2DTCDN 中移除时间特征。

- STL-2DTCDN  $\rightarrow$  STL-TCN: 用普通的 TCN 替换 2DTCDN。


表 3-5 不同模块的消融实验结果（预测步长为 720）


<table><tr><td colspan="2">数据集</td><td>Traffic</td><td>Electricity</td><td>ETTh2</td></tr><tr><td rowspan="2">STL-2DTCDN</td><td>MSE</td><td>0.347</td><td>0.169</td><td>0.390</td></tr><tr><td>MAE</td><td>0.267</td><td>0.287</td><td>0.426</td></tr><tr><td rowspan="2">wo/ STL</td><td>MSE</td><td>0.461</td><td>0.235</td><td>0.511</td></tr><tr><td>MAE</td><td>0.379</td><td>0.336</td><td>0.576</td></tr><tr><td rowspan="2">wo/ 2DTCDN</td><td>MSE</td><td>0.895</td><td>0.369</td><td>1.534</td></tr><tr><td>MAE</td><td>0.571</td><td>0.487</td><td>1.361</td></tr><tr><td rowspan="2">wo/ Time 
features</td><td>MSE</td><td>0.492</td><td>0.234</td><td>0.453</td></tr><tr><td>MAE</td><td>0.387</td><td>0.358</td><td>0.497</td></tr><tr><td rowspan="2">STL-2DTCDN 
→ STL-TCN</td><td>MSE</td><td>0.615</td><td>0.254</td><td>0.934</td></tr><tr><td>MAE</td><td>0.402</td><td>0.399</td><td>0.762</td></tr></table>

STL-2DTCDN 以及移除每个模块后子框架的实验结果如表 3-5 所示。可以从表中观察到，STL、2DTCDN 和时间特征的组合在不同的数据集中提供了最精确的预测结果，而移除任何一个模块都会导致预测性能下降。此外，我们还将 2DTCDN 替换为普通的 TCN 模块，实验结果表明，2DTCDN 在长期多元预测任务中表现更好。

# 3.4 本章小结

本章设计了一个用于长期多元时间序列预测的STL-2DTCDN模型。该模型通过STL将原始时间序列分解为趋势、季节性和残差三个子序列，并利用时间特征为模型添加额外的协变量。我们对传统的TCN进行改进，结合多层感知机提出了2DTCDN，该模块能够有效捕捉多维时间序列之间的复杂依赖关系。实验结果表明，STL-2DTCDN在多个数据集上的预测性能优于现有的Transformer模型和线性模型。此外，通过消融实验，我们验证了STL、2DTCDN和时间特征的组合对模型性能的关键作用。本章的研究表明，STL-2DTCDN在捕捉时间序列中的多种模式及复杂的依赖关系方面具有显著优势，在长期多元时间序列预测任务中取得优秀效果。

# 第四章 基于预训练的多头接收加权键值与多尺度图卷积网络预测模型

多元时间序列预测的主要挑战在于捕捉序列内的时间依赖关系和序列间的空间依赖关系。然而，多元时间序列在不同时间尺度上可能表现出多样化的相关性，另外来自不相关序列的潜在噪声可能会干扰模型学习序列内的时间依赖关系。为应对这些挑战，我们提出了一种基于预训练的多头接收加权键值与多尺度图卷积网络（Pre-trained Multi-head Receptance Weighted Key Value and Multi-scale Graph Convolutional Network, PMRWKV-GCN）预测模型。该模型旨在学习不同时间尺度下的序列内和序列间的时空依赖关系，并减轻无关序列间的噪声干扰。首先，我们利用FFT将时间序列转换到频域，将时间序列的波动分解为不同频率的周期成分，从而识别出多尺度的周期性特征。然后，将识别出的周期性特征与RWKV的时间混合模块相融合，设计了一个多头时间混合模块，并结合通道独立策略，在自监督预训练阶段捕捉多尺度的时间依赖关系。同时，我们将FFT识别出的周期性特征与GCN相融合，设计了多尺度GCN模块，用于学习多尺度的序列间依赖关系。我们在八个真实数据集上进行了实验，实验结果验证所提出的PMRWKV-GCN模型优于当前最先进的时间序列预测算法，展现出较大的性能提升。

# 4.1 引言

第三章提出的多元时间序列预测方法利用STL和2DTCDN模块学习时间序列中的趋势、季节性和序列间的相互依赖关系。然而，序列间的相互依赖关系是动态变化的，在不同时间尺度上表现出多种相关性，并且不相关序列带来的噪声会干扰模型学习单个序列内的时间依赖关系。为解决这些问题，本章引入基于RWKV[66]与GCN[51]的多尺度学习框架，设计了PMRWKV-GCN模型，以增强模型对多尺度时间和空间依赖关系的捕捉能力。

在多元时间序列预测任务中，准确捕捉单序列内的时间模式和多序列间的空间依赖关系至关重要。近年来，深度学习方法已被广泛应用于多元时间序列预测，并在捕捉复

杂时间和空间依赖方面表现优越。RNN及其变体常用于时间依赖建模，但在处理长序列时，RNN模型容易受到梯度消失和梯度爆炸问题的限制，影响其有效性。CNN和TCN的引入进一步增强对时间序列中长程依赖性的捕捉，特别是TCN的因果卷积结构和扩展卷积大大提升其对长期依赖关系的建模能力。然而，CNN和TCN在建模序列间复杂关系方面仍有局限性。

Transformer[10]架构因其在自然语言处理中的成功而备受关注，得益于其自注意力机制和并行处理能力，Transformer能够捕捉序列中复杂的长期依赖关系，并通过多头自注意力机制从多个角度学习特征。许多研究已将Transformer应用于时间序列预测任务，如Reformer[99]、Informer[100]、Autoformer[101]、Crossformer[107]和Transformer[108]。然而，Transformer架构在处理长期时间序列数据时会遇到空间占用过大和计算复杂度高的问题。因此，Peng等人[66]提出了具有线性注意力机制的RWKV架构，该架构结合RNN的高效推理和Transformer的可并行化训练的优势。RWKV架构已经在一些领域中表现出良好的性能[109,110,111,112]。尽管RWKV模型在处理长序列方面效果优异，但由于其仅关注当前状态和前一步的状态，难以捕捉输入序列的长期依赖性。相比之下，Transformer利用点积注意力机制来考虑整个序列，从而更全面地理解长期关系。此外，图卷积网络（GCN）作为多元时间序列预测的有力工具，特别是在捕捉不同序列之间的空间依赖性方面展现出巨大潜力[113,114,115,116]。GCN将不同时间序列之间的关系建模为图结构，使模型能够捕捉序列间复杂的相互依赖关系。

尽管现有深度学习方法在整合时间和空间依赖信息上表现出色，但在多种时间尺度上精准捕捉这些依赖关系仍面临重大挑战。此外，在多元时间序列预测中，模型可能学习到不相关序列的噪声，进而影响预测性能。为此，我们提出 PMRWKV-GCN 模型，通过多尺度学习捕捉序列内的时间依赖性和序列间的空间依赖性，同时在预训练阶段应用通道独立策略避免序列间的噪声干扰。我们在八个公开数据集上进行了充分实验，结果表明 PMRWKV-GCN 模型优于当前最先进的方法。本章主要贡献如下：

（1）提出了一种新颖的 PMRWKV-GCN 模型，在多元时间序列预测中有效学习多尺度时间和空间依赖信息。该框架利用 FFT 识别时间序列的多种周期模式，并在预训练阶段与微调阶段捕捉多尺度的时空依赖关系。

（2）在预训练阶段，我们采用通道独立策略，使模型能够分别从每个时间序列通道中捕捉独特的时间依赖关系。这一方法有助于避免来自不相关通道的噪声干扰，增强模型对每个通道特征的理解。

（3）在微调阶段，模型结合多头时间混合模块和多尺度 GCN，以捕捉多元时间序列中不同通道之间的时空依赖关系，能够提高模型预测的准确性和泛化能力。

本章其余部分的结构如下：在4.2节详细介绍PMRWKV-GCN模型架构；在4.3节介绍实验数据集、实验设置，然后分析实验结果；在4.4节总结本章主要工作。

# 4.2基于预训练的多头接收加权键值与多尺度图卷积网络

# 4.2.1 基于 FFT 的多尺度周期性特征识别

在交通流量预测和气象预测等应用领域中，时间序列数据通常展现出多种周期性模式，这些周期性可能以日、月或年为时间尺度影响系统行为，增加了预测的复杂性。识别并分析这些周期性对于构建准确的预测模型至关重要，因为它们对数据的整体趋势有重要影响。为识别这些周期性模式，我们应用FFT，将时间序列从时间域转换到频率域，从而检测出数据中难以在时间域中识别的重要周期成分。

我们应用FFT分析多元时间序列数据  $\mathbf{X} \in R^{L \times N}$ ，其中  $L$  是序列长度， $N$  是维度的数量。这个过程用公式表示为：

$$
\mathbf {A} = A \operatorname {v g} \left(\operatorname {A m p} \left(\operatorname {F F T} (\mathbf {X})\right)\right) \tag {4-1}
$$

其中， $FFT(\mathbf{X})$  表示快速傅里叶变换过程， $Amp()$  表示振幅函数， $Avg()$  表示对所有  $N$  个维度进行平均操作。为使模型能聚焦于最重要的频率并减少不相关高频率带来的噪声，我们选择振幅最高的  $k$  个频率。由于频率域是共轭的，我们只考虑  $\{1, \dots, [L/2]\}$  范围内的频率：

$$
\left\{f _ {1}, f _ {2}, \dots , f _ {k} \right\} = T o p k (\mathbf {A}), f _ {*} \in \left\{1, \dots , \left[ \frac {L}{2} \right] \right\} \tag {4-2}
$$

其中， $k$  是一个超参数，决定了需要保留的频率的数量。选择  $k$  个重要的频率有助于减轻频率域表示的稀疏性，并减少那些可能对预测模型无意义的高频成分的噪声影响。这些

选定的频率对应于数据中最具影响力的周期性模式，这对于准确的建模和预测至关重要。

每个选定的频率  $f_{i}$  接下来通过以下公式转换为周期：

$$
p _ {i} = \frac {L}{f _ {i}}, i \in \{1, \dots , k \} \tag {4-3}
$$

这种周期识别使得后续的建模阶段能够专注于识别出的关键周期性特征，从而提高模型预测结果的准确性。

# 4.2.2 多头时间混合模块

RWKV 的时间混合模块采用线性注意力机制，该机制与 Transformer 中的点积注意力机制不同，但同样能够实现并行训练，使得 RWKV 在推理过程中更高效。然而，由于时间混合模块仅考虑前一步和当前状态，在捕捉长期复杂依赖关系方面面临挑战。为增强模型捕捉长期依赖关系的能力，我们提出了一个多头时间混合模块，该模块集成重要的周期信息，扩展模型的感受野，并在多个尺度上保留重要的周期特征。所提出的多头时间混合模块的详细过程如下：

频率提取：我们使用FFT来识别时间序列中的显著周期性成分。基于它们的振幅，选择出前  $k$  个频率  $\{f_1,f_2,\dots ,f_k\}$  ，这些被选择出的频率对应的振幅  $\{a_{1},a_{2},\dots ,a_{k}\}$  经过Softmax函数进行归一化：

$$
\left\{a _ {1} ^ {\prime}, a _ {2} ^ {\prime}, \dots , a _ {k} ^ {\prime} \right\} = \operatorname {S o f t m a x} \left(a _ {1}, a _ {2}, \dots , a _ {k}\right) \tag {4-4}
$$

这些归一化后的振幅作为每个周期尺度的权重，在后续计算中实现多个周期特征的聚合。

多尺度特征聚合：对于每个时间步长，键（Key）、值（Value）和接收向量（Receptance vector）通过聚合来自多个周期的信息进行计算。计算表达式如下：

$$
r _ {t} ^ {\prime} = W _ {r} \cdot \left(\mu_ {r} x _ {t} + \left(1 - \mu_ {r}\right) \left(x _ {t - 1} + \sum_ {i = 1} ^ {k} a _ {i} ^ {\prime} x _ {t - p _ {i}}\right)\right) \tag {4-5}
$$

$$
k _ {t} ^ {\prime} = W _ {k} \cdot \left(\mu_ {k} x _ {t} + \left(1 - \mu_ {k}\right) \left(x _ {t - 1} + \sum_ {i = 1} ^ {k} a _ {i} ^ {\prime} x _ {t - p _ {i}}\right)\right) \tag {4-6}
$$

$$
v _ {t} ^ {\prime} = W _ {v} \cdot \left(\mu_ {v} x _ {t} + \left(1 - \mu_ {v}\right) \left(x _ {t - 1} + \sum_ {i = 1} ^ {k} a _ {i} ^ {\prime} x _ {t - p _ {i}}\right)\right) \tag {4-7}
$$

其中， $a_{i}^{\prime}$  和  $p_{i}$  分别对应于从选定频率中得到的权重和周期，参数  $\mu_{r}$  、 $\mu_{k}$  和  $\mu_{v}$  表示时间步长偏移策略的插值权重。

多头计算：为进一步增强模型捕捉多尺度时间依赖关系的能力，我们引入多头计算机制。该机制包含多个头，每个头对应于通过 FFT 识别出的不同周期。这些头的输出按如下方式计算：

$$
w k v _ {t} ^ {0} = \frac {\sum_ {i = 1} ^ {t - 1} e ^ {- (t - 1 - i) w + k _ {i}} v _ {i} + e ^ {u + k _ {t}} v _ {t}}{\sum_ {i = 1} ^ {t - 1} e ^ {- (t - 1 - i) w + k _ {i}} + e ^ {u + k _ {t}}} \tag {4-8}
$$

$$
w k v _ {t} ^ {j} = \frac {\sum_ {i = 1} ^ {t - 1} e ^ {- (t - 1 - p _ {j} i) w + k _ {i}} v _ {i} + e ^ {u + k _ {t}} v _ {t}}{\sum_ {i = 1} ^ {t - 1} e ^ {- (t - 1 - p _ {j} i) w + k _ {i}} + e ^ {u + k _ {t}}}, j \in [ 1, \dots , k ] \tag {4-9}
$$

其中， $w$  和  $u$  是标量， $w k v_{t}^{0}$  代表原始的 RWKV 模型对时间序列中的每个时间步长进行信息聚合，计算集中在前一步和当前步的信息聚合，进而捕捉序列中的时间依赖性。 $w k v_{t}^{1}, \ldots, w k v_{t}^{k}$  代表对时间序列每个周期长度的步长进行信息聚合，它们集成了重要的周期信息，在多个尺度上保留重要的周期特征。然后这些聚合后的信息被拼接以形成最终的输出：

$$
\operatorname {M u l t i H e a d} \left(w k v _ {t} ^ {\prime}\right) = \operatorname {C o n c a t} \left(w k v _ {t} ^ {0}, w k v _ {t} ^ {1}, \dots , w k v _ {t} ^ {k}\right) \tag {4-10}
$$

多头时间混合模块的最终输出计算如下：

$$
o _ {t} ^ {\prime} = W _ {o} \cdot \left(\sigma \left(r _ {t} ^ {\prime}\right) \odot \text {M u l t i H e a d} \left(w k v _ {t} ^ {\prime}\right)\right) \tag {4-11}
$$

其中， $\sigma$  表示 Sigmoid 函数。该模块不仅考虑当前时间步和前一步的状态，还在不同的尺度下整合重要的周期信息，增强模型捕捉复杂时间依赖关系的能力。

# 4.2.3 多尺度 GCN

多元时间序列中的序列间依赖关系可能在不同时间尺度上有所变化，这使得模型在捕捉这些复杂的依赖性时变得更加困难。为解决这一问题，我们引入多尺度 GCN（Multi-scale GCN）模块以学习不同尺度下的空间依赖关系。通过引入图结构，该模块能够有效捕捉多时间尺度下不同通道间的相互依赖关系，实现对复杂系统的精准预测。多尺度图卷积框架如图 4-1 所示。

首先根据快速傅里叶变换分析中识别出的多个周期尺度  $\{p_1, p_2, \dots, p_k\}$  (其中  $p_i < L$  )，将输入的时间序列分割成不同部分。时间窗口  $L$  被分割成  $\lceil L / p_i \rceil$  个部分，每个部分的长度等于对应的周期长度。这种分割确保数据可以在不同的时间尺度上表示，方便捕获特定尺度的依赖性。对于每个尺度  $p_i$  ，输入序列会被重新调整形状，并在必要时进行填充以适应分割：

$$
X ^ {i} = \operatorname {R e s h a p e} (\operatorname {P a d d i n g} (\mathbf {X})) , i \in \{1, \dots , k \} \tag {4-12}
$$

然后对于每个与特定尺度  $p_i$  对应的时间片段，应用 GCN 来处理并提取特征。GCN 的初始邻接矩阵均匀初始化。GCN 具体操作过程如下：

节点特征更新：对于每个片段，节点特征通过图卷积网络更新，图卷积网络从邻域模式中聚合信息。更新的计算公式如下：

$$
H _ {i} ^ {(l + 1)} = \sigma \left(\tilde {A} _ {i} H _ {i} ^ {(l)} W _ {i} ^ {(l)}\right) \tag {4-13}
$$

其中， $H_{i}^{(l + 1)}$  表示第  $l + 1$  层的更新特征矩阵， $\tilde{A}_i$  是针对尺度  $p_i$  的归一化矩阵， $W_{i}^{(l)}$  是可训练的权重矩阵， $\sigma$  是激活函数。

![](images/cb9d6a7e634f32915e9de8aa20a49ee8aa952ff340036f83ca46dbec1e001f34.jpg)



图4-1多尺度GCN模块的过程


在处理完所有片段后，针对给定尺度  $p_i$ ，所有片段的输出沿时间维度进行拼接，形成该周期的输出特征矩阵。这个拼接过程能够聚合所有片段的特征，提供该特定周期的

完整时空特征表示。

$$
H _ {i} ^ {\text {n e w}} = \operatorname {C o n c a t} \left(H _ {1} ^ {(l + 1)}, H _ {2} ^ {(l + 1)}, \dots , H _ {\lceil L / p _ {i} \rceil} ^ {(l + 1)}\right) \tag {4-14}
$$

动态邻接矩阵更新：对于对应尺度  $p_i$  的邻接矩阵  $A_i$  通过自注意力机制进行动态更新，以反映各通道间不断演变的依赖关系。自注意力机制的 Query、Key 和 Value 的计算公式如下：

$$
Q _ {i} = H _ {i} W _ {Q}, K _ {i} = H _ {i} W _ {K}, V _ {i} = H _ {i} W _ {V} \tag {4-15}
$$

注意力得分的计算方式为：

$$
a t t e n t i o n \_ s c o r e = L e a k y R e L U (Q K / \sqrt {d}) \tag {4-16}
$$

归一化后的注意力权重 attention_score 计算出来后被用于更新尺度  $p_i$  对应的邻接矩阵:

$$
A _ {i} ^ {\text {n e w}} = \text {a t t e n t i o n} \_ \text {s c o r e} \cdot V _ {i} \tag {4-17}
$$

通过图卷积网络层处理后，所有尺度  $\{p_1, p_2, \dots, p_k\}$  的信息被聚合和更新。

节点特征聚合：来自所有尺度的特征矩阵  $H_{i}^{(l + 1)}$  使用从快速傅里叶变换分析中得出的权重  $\{a_1', a_2', \dots, a_k'\}$  进行聚合，这些权重能够反映每个尺度的重要性。最终聚合后的节点特征表示为：

$$
H ^ {\text {n e w}} = \sum_ {i = 1} ^ {k} a _ {i} ^ {\prime} H _ {i} ^ {\text {n e w}} \tag {4-18}
$$

邻接矩阵聚合：来自不同尺度的更新邻接矩阵也使用相同的权重进行聚合：

$$
A ^ {\text {n e w}} = \sum_ {i = 1} ^ {k} a _ {i} ^ {\prime} A _ {i} ^ {\text {n e w}} \tag {4-19}
$$

加权聚合能够确保不同尺度的重要特征和依赖关系被充分捕捉和利用。通过整合多尺度特征，模型能够同时捕捉全局与局部模式，提升预测性能。

本节提出的多尺度 GCN 框架充分利用 GCN 在建模空间依赖关系方面的优势，同时捕捉多尺度下的时空动态关系。通过动态更新邻接矩阵，模型能够适应时间序列通道间关系的变化，特别适用于处理复杂且不断演变的数据集。

# 4.2.4 基于通道独立策略的预训练

预训练近年来在自然语言处理和计算机视觉领域获得显著的发展。最近的研究成果也验证了预训练方法在时间序列预测任务中的有效性[70,71]。本节设计了一种融合通道独立策略的预训练方法。已有的研究表明，通道独立策略能够有效提升多元时间序列预测任务的准确性[117]。该方法的核心是将每个时间序列分别处理，而不考虑序列间的关系。这一策略的主要优点是防止模型过早地学习到通道间的依赖性，避免来自不相关序列的噪声干扰单个通道中的时间依赖关系的识别。因此，我们在预训练阶段采用通道独立策略，确保模型能够独立学习每个时间序列通道的独特特征。PMRWKV-GCN的预训练阶段和微调阶段如图4-2所示。

![](images/d8ac0f8849ba2481e44aa6ded1c1326e30f823c1f38ee319c0c74d06b8087307.jpg)



图4-2 PMRWKV-GCN的框架


该自监督预训练学习框架包括对数据片段进行掩码，并训练模型预测这些被掩码的部分。具体步骤如下：

数据掩码：对于每个通道，多个固定长度的片段会在输入的时间窗口内随机被掩码。根据PatchTST[70]的研究，我们将片段的长度和输入时间窗口分别设置为64和512。这一掩码方式独立应用于每个通道，符合通道独立策略。掩码迫使模型根据上下文推断出缺失的信息。

模型预训练：在该阶段，模型使用改进的多头时间混合模块，该模块是基于RWKV

架构的增强版本。此模块旨在独立学习每个通道的时间依赖关系。通过使用同一通道中未被掩码的数据，多头时间混合模块预测被掩码片段的值。在此阶段，多尺度 GCN 模块未被训练，因为输入数据是在通道独立的情况下处理的，因此不需要进行通道间的依赖建模。

损失计算：通过预测值与被遮掩数据的真实值之间的均方误差（MSE）来评估预测准确性。公式如下所示：

$$
M S E = \frac {1}{m} \sum_ {j = 1} ^ {m} \frac {1}{n _ {j}} \sum_ {i = 1} ^ {n _ {j}} \left(y _ {i j} - \hat {y} _ {i j}\right) ^ {2} \tag {4-20}
$$

其中， $y_{ij}$  和  $\hat{y}_{ij}$  分别是第  $j$  掩码段的真实值和预测值， $n_j$  是该段的长度， $m$  是被遮掩段的总数。

# 4.3 实验

在本节中，我们通过实验验证 PMRWKV-GCN 模型在不同数据集和预测长度上的表现，并将其与多种最先进的时间序列预测模型进行对比。评估指标使用均方误差（MSE）和平均绝对误差（MAE）。

# 4.3.1 实验数据集

PMRWKV-GCN 模型在八个不同的数据集上进行了实验。每个数据集按时间顺序划分为三个部分：训练集、验证集和测试集。对于 Weather、Traffic 和 Electricity 数据集，分割比例为 7:1:2，而对于 ETT 数据集，根据已有研究的建议[100]，分割比例设定为 6:2:2。实验数据集的统计信息如所表 4-1 所示。

- ETT（电力变压器温度）：该数据集包含四个子集：两个1小时级数据集（ETTh1, ETTh2）和两个15分钟级数据集（ETTm1, ETTm2）。

- Exchange: 该数据集收集了八个国家的汇率数据，包括新西兰、英国、中国、日本、瑞士、加拿大、新加坡和澳大利亚。

- Traffic：该数据集收集加了利福尼亚州高速公路上的道路占用情况。

- Electricity: 该数据集描述 321 个客户的电力消耗情况（单位：千瓦时）。

- Weather: 该数据集包含 21 项气象指标, 例如气温和湿度。数据记录于 2020 年,每 10 分钟采集一次, 地点为德国。


表 4-1 数据集的统计信息


<table><tr><td>数据集</td><td>Electricity</td><td>Weather</td><td>ETTh1</td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td><td>Exchange</td><td>Traffic</td></tr><tr><td>特征数</td><td>321</td><td>21</td><td>7</td><td>7</td><td>7</td><td>7</td><td>8</td><td>862</td></tr><tr><td>时间步</td><td>26304</td><td>52696</td><td>17420</td><td>17420</td><td>69680</td><td>69680</td><td>7588</td><td>17544</td></tr><tr><td>频率</td><td>1小时</td><td>10小时</td><td>1小时</td><td>1小时</td><td>15分钟</td><td>15分钟</td><td>1天</td><td>1小时</td></tr></table>

# 4.3.2 实验设置

为验证所提出模型的性能，我们将其与六种最先进的时间序列预测模型进行比较。这些模型分别代表不同类别的方法，包括基于Transformer的架构：Autoformer[101]、Informer[100]和ETSformer[118]；图卷积网络：MSGNet[119]；线性模型：DLinear[61]；以及CNN模型：TimesNet[120]。

在预训练阶段，根据Nie等人[70]的工作，我们对每个数据集进行100轮的自监督预训练，回溯窗口和分段大小分别设置为512和64。预训练阶段遮掩比例设置为  $50\%$  ，并用零值填充。根据TimesNet[120]的工作，频率数量  $k$  设置为5。


表 4-2 超参数取值范围


<table><tr><td>超参数</td><td>Range</td></tr><tr><td>N1的层数</td><td>[1,2,3,4]</td></tr><tr><td>N2的层数</td><td>[1,2,3,4]</td></tr><tr><td>k的取值</td><td>[5]</td></tr><tr><td>模型维度</td><td>[32,64,128,256]</td></tr><tr><td>节点的嵌入维度</td><td>[24,48,96,128]</td></tr></table>

所有实验均在配备两块 NVIDIA GeForce RTX 2080 Ti GPU 的工作站上并使用 PyTorch 深度学习框架进行。为了公平比较，所有实验的回溯窗口  $L$  设置为 96，对应的预测长度为  $\{96, 192, 336, 720\}$  。批量大小和初始学习率分别设置为 32 和  $10^{-4}$  。训练使用 MSE 作为损失函数，并进行五次实验以确保可靠性。训练轮数为 10。PMRWKV-GCN 模型的超参数范围如表 4-2 所示，这些超参数是基于验证集上的滚动验证误差进行调整的。每个数据集的超参数设置如表 4-3 所示。对比模型的结果取自 MSGNet[119]和 TimesNet[120]的研究报告。


表 4-3 八个数据集的超参数设置


<table><tr><td>超参数</td><td>Weather</td><td>ETTh1</td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td><td>Electricity Exchange</td><td>Traffic</td></tr><tr><td>N1的层数</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>N2的层数</td><td>2</td><td>2</td><td>2</td><td>2</td><td>3</td><td>2</td><td>3</td></tr><tr><td>k的取值</td><td>5</td><td>5</td><td>5</td><td>5</td><td>5</td><td>5</td><td>5</td></tr><tr><td>模型维度</td><td>32</td><td>32</td><td>32</td><td>32</td><td>32</td><td>256</td><td>64</td></tr><tr><td>节点的嵌入维度</td><td>24</td><td>24</td><td>24</td><td>24</td><td>96</td><td>24</td><td>96</td></tr></table>

# 4.3.3 实验结果分析

表4-4详细比较了不同预测长度{96,192,336,720}下各模型在八个数据集上的表现。每个数据集和预测长度中最优结果用加粗标出，次优结果用下划线标注。实验结果清晰地展示出所提出的PMRWKV-GCN模型的有效性。“w/o Pretrain”代表的是不经过预训练阶段的PMRWKV-GCN模型。从表4-4中可以观察到几个关键点：

（1）所提出的 PMRWKV-GCN 模型在不同的数据集和预测长度始终优于其他模型。在大多数情况下，它取得最低的平均 MSE 和 MAE，验证了该模型在捕捉复杂时间和空间依赖关系方面的能力。

（2）即使没有预训练阶段（w/o Pretrain），该模型在多个数据集的表现仍然优于其他模型，显示出模型的鲁棒性。并且预训练阶段能够提升模型预测性能，验证了预训练策略的有效性。

（3）在Weather数据集上，PMRWKV-GCN的表现超越当前的SOTA模型MSGNet，将平均MSE和MAE分别降低  $6.83\%$  （从0.249降至0.232）和  $8.63\%$  （从0.278降至0.254）。

（4）在Electricity数据集上，PMRWKV-GCN表现尤为突出，将平均MSE和MAE分别降低了  $11.86\%$  （从0.194降至0.171）和  $10.33\%$  （从0.300降至0.269）。

(5) 在 Exchange 数据集上, PMRWKV-GCN 的平均表现 (MSE 0.365, MAE 0.398)仍具有竞争力, 反映出该模型对各种数据模式的适应性。

实验结果验证了所提出的 PMRWKV-GCN 模型的有效性。通过结合多头时间混合模块和多尺度 GCN 模块的优势，该模型在多个数据集和预测长度上均取得出优秀的表现。预训练的加入进一步增强其预测能力，特别是在长期预测中，使 PMRWKV-GCN 成为一

个有效的多元时间序列预测算法。


表 4-4 八个数据集的预测结果


<table><tr><td rowspan="2" colspan="2">模型 评价指标</td><td colspan="2">Proposed</td><td colspan="2">w/oPretrain</td><td colspan="2">MSGNet</td><td colspan="2">TimesNet</td><td colspan="2">DLinear</td><td colspan="2">ETSformer</td><td colspan="2">Autoformer</td><td colspan="2">Informer</td></tr><tr><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">Weather</td><td>96</td><td>0.155</td><td>0.187</td><td>0.159</td><td>0.192</td><td>0.163</td><td>0.212</td><td>0.172</td><td>0.220</td><td>0.196</td><td>0.255</td><td>0.197</td><td>0.281</td><td>0.266</td><td>0.336</td><td>0.300</td><td>0.384</td></tr><tr><td>192</td><td>0.201</td><td>0.242</td><td>0.207</td><td>0.250</td><td>0.212</td><td>0.254</td><td>0.219</td><td>0.261</td><td>0.237</td><td>0.296</td><td>0.237</td><td>0.312</td><td>0.307</td><td>0.367</td><td>0.598</td><td>0.544</td></tr><tr><td>336</td><td>0.253</td><td>0.267</td><td>0.265</td><td>0.273</td><td>0.272</td><td>0.299</td><td>0.280</td><td>0.306</td><td>0.283</td><td>0.335</td><td>0.298</td><td>0.353</td><td>0.359</td><td>0.395</td><td>0.578</td><td>0.523</td></tr><tr><td>720</td><td>0.320</td><td>0.318</td><td>0.322</td><td>0.330</td><td>0.350</td><td>0.348</td><td>0.365</td><td>0.359</td><td>0.345</td><td>0.381</td><td>0.352</td><td>0.288</td><td>0.419</td><td>0.428</td><td>1.059</td><td>0.741</td></tr><tr><td>Avg</td><td>0.232</td><td>0.254</td><td>0.238</td><td>0.261</td><td>0.249</td><td>0.278</td><td>0.259</td><td>0.287</td><td>0.265</td><td>0.317</td><td>0.271</td><td>0.309</td><td>0.338</td><td>0.382</td><td>0.634</td><td>0.548</td></tr><tr><td rowspan="5">ETTm1</td><td>96</td><td>0.293</td><td>0.330</td><td>0.306</td><td>0.342</td><td>0.319</td><td>0.366</td><td>0.338</td><td>0.375</td><td>0.345</td><td>0.372</td><td>0.375</td><td>0.398</td><td>0.505</td><td>0.475</td><td>0.672</td><td>0.571</td></tr><tr><td>192</td><td>0.343</td><td>0.357</td><td>0.351</td><td>0.371</td><td>0.376</td><td>0.397</td><td>0.374</td><td>0.387</td><td>0.380</td><td>0.389</td><td>0.408</td><td>0.410</td><td>0.553</td><td>0.496</td><td>0.795</td><td>0.669</td></tr><tr><td>336</td><td>0.377</td><td>0.398</td><td>0.398</td><td>0.405</td><td>0.417</td><td>0.422</td><td>0.410</td><td>0.411</td><td>0.413</td><td>0.413</td><td>0.435</td><td>0.428</td><td>0.621</td><td>0.537</td><td>1.212</td><td>0.871</td></tr><tr><td>720</td><td>0.452</td><td>0.429</td><td>0.461</td><td>0.432</td><td>0.481</td><td>0.458</td><td>0.478</td><td>0.450</td><td>0.474</td><td>0.453</td><td>0.499</td><td>0.462</td><td>0.671</td><td>0.561</td><td>1.166</td><td>0.823</td></tr><tr><td>Avg</td><td>0.366</td><td>0.379</td><td>0.379</td><td>0.388</td><td>0.398</td><td>0.411</td><td>0.400</td><td>0.406</td><td>0.403</td><td>0.407</td><td>0.429</td><td>0.425</td><td>0.588</td><td>0.517</td><td>0.961</td><td>0.734</td></tr><tr><td rowspan="5">ETTm2</td><td>96</td><td>0.162</td><td>0.247</td><td>0.171</td><td>0.260</td><td>0.177</td><td>0.262</td><td>0.187</td><td>0.267</td><td>0.193</td><td>0.292</td><td>0.189</td><td>0.280</td><td>0.255</td><td>0.339</td><td>0.365</td><td>0.453</td></tr><tr><td>192</td><td>0.223</td><td>0.286</td><td>0.235</td><td>0.293</td><td>0.247</td><td>0.307</td><td>0.249</td><td>0.309</td><td>0.284</td><td>0.362</td><td>0.253</td><td>0.319</td><td>0.281</td><td>0.340</td><td>0.533</td><td>0.563</td></tr><tr><td>336</td><td>0.275</td><td>0.306</td><td>0.284</td><td>0.312</td><td>0.312</td><td>0.346</td><td>0.321</td><td>0.351</td><td>0.369</td><td>0.427</td><td>0.314</td><td>0.357</td><td>0.339</td><td>0.372</td><td>1.363</td><td>0.887</td></tr><tr><td>720</td><td>0.401</td><td>0.371</td><td>0.407</td><td>0.384</td><td>0.414</td><td>0.403</td><td>0.408</td><td>0.403</td><td>0.554</td><td>0.522</td><td>0.414</td><td>0.413</td><td>0.433</td><td>0.432</td><td>3.379</td><td>1.338</td></tr><tr><td>Avg</td><td>0.265</td><td>0.303</td><td>0.274</td><td>0.312</td><td>0.288</td><td>0.330</td><td>0.291</td><td>0.333</td><td>0.350</td><td>0.401</td><td>0.293</td><td>0.342</td><td>0.327</td><td>0.371</td><td>1.410</td><td>0.810</td></tr><tr><td rowspan="5">ETTh1</td><td>96</td><td>0.354</td><td>0.366</td><td>0.367</td><td>0.388</td><td>0.390</td><td>0.411</td><td>0.384</td><td>0.402</td><td>0.386</td><td>0.400</td><td>0.494</td><td>0.479</td><td>0.449</td><td>0.459</td><td>0.865</td><td>0.713</td></tr><tr><td>192</td><td>0.419</td><td>0.392</td><td>0.433</td><td>0.417</td><td>0.442</td><td>0.442</td><td>0.436</td><td>0.429</td><td>0.437</td><td>0.432</td><td>0.538</td><td>0.504</td><td>0.500</td><td>0.482</td><td>1.008</td><td>0.792</td></tr><tr><td>336</td><td>0.438</td><td>0.425</td><td>0.453</td><td>0.441</td><td>0.480</td><td>0.468</td><td>0.491</td><td>0.469</td><td>0.481</td><td>0.459</td><td>0.574</td><td>0.521</td><td>0.521</td><td>0.496</td><td>1.107</td><td>0.809</td></tr><tr><td>720</td><td>0.460</td><td>0.463</td><td>0.475</td><td>0.470</td><td>0.494</td><td>0.488</td><td>0.521</td><td>0.500</td><td>0.519</td><td>0.516</td><td>0.562</td><td>0.535</td><td>0.514</td><td>0.512</td><td>1.181</td><td>0.865</td></tr><tr><td>Avg</td><td>0.418</td><td>0.412</td><td>0.432</td><td>0.429</td><td>0.452</td><td>0.452</td><td>0.458</td><td>0.450</td><td>0.456</td><td>0.452</td><td>0.542</td><td>0.510</td><td>0.496</td><td>0.487</td><td>1.040</td><td>0.795</td></tr><tr><td rowspan="5">ETTh2</td><td>96</td><td>0.282</td><td>0.34</td><td>0.297</td><td>0.352</td><td>0.328</td><td>0.371</td><td>0.340</td><td>0.374</td><td>0.333</td><td>0.387</td><td>0.340</td><td>0.391</td><td>0.346</td><td>0.388</td><td>3.755</td><td>1.525</td></tr><tr><td>192</td><td>0.369</td><td>0.362</td><td>0.388</td><td>0.385</td><td>0.402</td><td>0.414</td><td>0.402</td><td>0.414</td><td>0.477</td><td>0.476</td><td>0.430</td><td>0.439</td><td>0.456</td><td>0.452</td><td>5.602</td><td>1.931</td></tr><tr><td>336</td><td>0.416</td><td>0.411</td><td>0.423</td><td>0.421</td><td>0.435</td><td>0.443</td><td>0.452</td><td>0.452</td><td>0.594</td><td>0.541</td><td>0.485</td><td>0.479</td><td>0.482</td><td>0.486</td><td>4.721</td><td>1.835</td></tr><tr><td>720</td><td>0.375</td><td>0.423</td><td>0.391</td><td>0.430</td><td>0.417</td><td>0.441</td><td>0.462</td><td>0.468</td><td>0.831</td><td>0.657</td><td>0.500</td><td>0.497</td><td>0.515</td><td>0.511</td><td>3.647</td><td>1.625</td></tr><tr><td>Avg</td><td>0.361</td><td>0.384</td><td>0.375</td><td>0.397</td><td>0.396</td><td>0.417</td><td>0.414</td><td>0.427</td><td>0.559</td><td>0.515</td><td>0.439</td><td>0.452</td><td>0.450</td><td>0.459</td><td>4.431</td><td>1.729</td></tr><tr><td rowspan="5">Electricity</td><td>96</td><td>0.158</td><td>0.246</td><td>0.160</td><td>0.254</td><td>0.165</td><td>0.274</td><td>0.168</td><td>0.272</td><td>0.197</td><td>0.282</td><td>0.187</td><td>0.304</td><td>0.201</td><td>0.317</td><td>0.274</td><td>0.368</td></tr><tr><td>192</td><td>0.161</td><td>0.265</td><td>0.165</td><td>0.268</td><td>0.184</td><td>0.292</td><td>0.184</td><td>0.289</td><td>0.196</td><td>0.285</td><td>0.199</td><td>0.315</td><td>0.222</td><td>0.334</td><td>0.296</td><td>0.386</td></tr><tr><td>336</td><td>0.174</td><td>0.273</td><td>0.180</td><td>0.281</td><td>0.195</td><td>0.302</td><td>0.198</td><td>0.300</td><td>0.209</td><td>0.301</td><td>0.212</td><td>0.329</td><td>0.231</td><td>0.338</td><td>0.300</td><td>0.394</td></tr><tr><td>720</td><td>0.192</td><td>0.29</td><td>0.196</td><td>0.297</td><td>0.231</td><td>0.332</td><td>0.220</td><td>0.320</td><td>0.245</td><td>0.333</td><td>0.233</td><td>0.345</td><td>0.254</td><td>0.361</td><td>0.373</td><td>0.439</td></tr><tr><td>Avg</td><td>0.171</td><td>0.269</td><td>0.175</td><td>0.275</td><td>0.194</td><td>0.300</td><td>0.193</td><td>0.295</td><td>0.212</td><td>0.300</td><td>0.208</td><td>0.323</td><td>0.227</td><td>0.338</td><td>0.311</td><td>0.397</td></tr><tr><td rowspan="5">Traffic</td><td>96</td><td>0.543</td><td>0.288</td><td>0.552</td><td>0.291</td><td>0.591</td><td>0.320</td><td>0.593</td><td>0.321</td><td>0.650</td><td>0.396</td><td>0.607</td><td>0.392</td><td>0.613</td><td>0.388</td><td>0.719</td><td>0.391</td></tr><tr><td>192</td><td>0.549</td><td>0.296</td><td>0.566</td><td>0.304</td><td>0.615</td><td>0.330</td><td>0.617</td><td>0.336</td><td>0.598</td><td>0.370</td><td>0.621</td><td>0.399</td><td>0.616</td><td>0.382</td><td>0.696</td><td>0.379</td></tr><tr><td>336</td><td>0.572</td><td>0.298</td><td>0.581</td><td>0.316</td><td>0.622</td><td>0.333</td><td>0.629</td><td>0.336</td><td>0.605</td><td>0.373</td><td>0.622</td><td>0.396</td><td>0.622</td><td>0.337</td><td>0.777</td><td>0.420</td></tr><tr><td>720</td><td>0.582</td><td>0.313</td><td>0.593</td><td>0.320</td><td>0.634</td><td>0.346</td><td>0.640</td><td>0.350</td><td>0.645</td><td>0.394</td><td>0.632</td><td>0.396</td><td>0.660</td><td>0.408</td><td>0.864</td><td>0.472</td></tr><tr><td>Avg</td><td>0.562</td><td>0.299</td><td>0.573</td><td>0.308</td><td>0.616</td><td>0.332</td><td>0.620</td><td>0.336</td><td>0.625</td><td>0.383</td><td>0.621</td><td>0.396</td><td>0.628</td><td>0.379</td><td>0.764</td><td>0.416</td></tr><tr><td rowspan="5">Exchange</td><td>96</td><td>0.097</td><td>0.206</td><td>0.103</td><td>0.222</td><td>0.102</td><td>0.230</td><td>0.107</td><td>0.234</td><td>0.088</td><td>0.218</td><td>0.085</td><td>0.204</td><td>0.197</td><td>0.323</td><td>0.847</td><td>0.752</td></tr><tr><td>192</td><td>0.172</td><td>0.295</td><td>0.178</td><td>0.302</td><td>0.195</td><td>0.317</td><td>0.226</td><td>0.344</td><td>0.176</td><td>0.315</td><td>0.182</td><td>0.303</td><td>0.300</td><td>0.369</td><td>1.204</td><td>0.895</td></tr><tr><td>336</td><td>0.340</td><td>0.403</td><td>0.347</td><td>0.410</td><td>0.359</td><td>0.436</td><td>0.367</td><td>0.448</td><td>0.313</td><td>0.427</td><td>0.348</td><td>0.428</td><td>0.509</td><td>0.524</td><td>1.672</td><td>1.036</td></tr><tr><td>720</td><td>0.851</td><td>0.689</td><td>0.886</td><td>0.711</td><td>0.940</td><td>0.738</td><td>0.964</td><td>0.746</td><td>0.839</td><td>0.695</td><td>1.025</td><td>0.774</td><td>1.447</td><td>0.941</td><td>2.478</td><td>1.310</td></tr><tr><td>Avg</td><td>0.365</td><td>0.398</td><td>0.379</td><td>0.411</td><td>0.399</td><td>0.430</td><td>0.416</td><td>0.443</td><td>0.354</td><td>0.414</td><td>0.410</td><td>0.427</td><td>0.613</td><td>0.539</td><td>1.550</td><td>0.998</td></tr></table>

PMRWKV-GCN模型与MSGNet模型在Electricity和Traffic数据集上的预测结果如图

4-3 和图 4-4 所示。图中展示了预测值与真实值在预测步长为 336 时的对比。显而易见，PMRWKV-GCN 模型的预测结果更紧密地跟随真实数据的波动，特别是在数据剧烈波动时期，如红色方框所示区域。这表明，PMSRWKV-GCN 在捕捉和预测潜在的时间动态方面具有更强的能力，这对于 Electricity 和 Traffic 等具有复杂波动模式的数据集尤为重要。

![](images/37a2a47e1a159868dd83d104a13e054bcbab82e113e9d6d02e1b3cfa073d1516.jpg)


![](images/67f15b6fd73f21a7844a9a2ed9143e40588714ed23b4d241ce624eeb7ef71a7e.jpg)



图4-3 PMRWKV-GCN与MSGNet在Electricity数据集上的比较（预测步长336）


![](images/0540a1fc9f614fdb2e26f5258032fabde80a33a43405294b295569a18b0c94e0.jpg)


![](images/5569d1602a2323657a5b56df0b77f81e51d3237d029dc2d693805b03b17aca77.jpg)



图4-4 PMRWKV-GCN与MSGNet在Traffic数据集上的比较（预测步长336）


![](images/e2ab9d6e1d5b902b631868d91ccd34b229678cb08cbb2ea727e3ed4eb38133af.jpg)



(a) Electricity


![](images/df360ba2e82bbbef5c0dd1ab4a516fd9b77e60a4b5e6a6fcb452890fe1f6d725.jpg)



(b) Traffic



图4-5 Electricity和Traffic数据集预测结果的小提琴图（预测步长336）


PMRWKV-GCN和MSGNet在Electricity和Traffic数据集上的误差分布如图4-5所示。PMSRWKV-GCN的误差分布更集中在零点附近，表明与MSGNet相比，PMSRWKV-GCN的预测更稳定、更准确。PMSRWKV-GCN更紧密的误差分布还表明，该模型的极端预测误差更少，这对于提高预测结果的可靠性至关重要。

# 4.3.4 消融实验

# 预训练的效果

预训练是提升 PMRWKV-GCN 模型预测性能的关键步骤。关于验证预训练阶段有效性的消融实验结果如表 4-5 所示。表中的 MSE 和 MAE 值是基于每个数据集的四种不同预测长度 \{96, 192, 336, 720\} 的平均值。表中最后一行展示了应用预训练策略后模型性能提升的百分比。


表 4-5 预训练阶段的消融实验


<table><tr><td>数据集</td><td colspan="2">Weather</td><td colspan="2">ETTm1</td><td colspan="2">ETTm2</td><td colspan="2">ETTh1</td><td colspan="2">ETTh2</td><td colspan="2">Electricity</td><td colspan="2">Traffic</td><td colspan="2">Exchange</td></tr><tr><td>评价指标</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>完整模型</td><td>0.232</td><td>0.254</td><td>0.366</td><td>0.379</td><td>0.265</td><td>0.303</td><td>0.418</td><td>0.412</td><td>0.361</td><td>0.384</td><td>0.171</td><td>0.269</td><td>0.562</td><td>0.299</td><td>0.365</td><td>0.398</td></tr><tr><td>wo/预训练阶段</td><td>0.238</td><td>0.261</td><td>0.379</td><td>0.388</td><td>0.274</td><td>0.312</td><td>0.432</td><td>0.429</td><td>0.375</td><td>0.397</td><td>0.175</td><td>0.275</td><td>0.573</td><td>0.308</td><td>0.379</td><td>0.411</td></tr><tr><td>提升(%)</td><td>2.52</td><td>2.68</td><td>3.43</td><td>2.32</td><td>3.28</td><td>2.88</td><td>3.24</td><td>3.96</td><td>3.73</td><td>3.27</td><td>2.29</td><td>2.18</td><td>1.92</td><td>2.92</td><td>3.69</td><td>3.16</td></tr></table>

结果清楚地表明，预训练策略能够提升模型在所有数据集上的预测性能。MSE平均提高了  $1.92\%$  到  $3.73\%$  ，这表明预训练可以有效帮助模型在微调之前更好地捕捉时间依赖关系。这种提升在不同类型的数据集上保持一致，无论是电力消耗（ETT、Electricity）、交通数据（Traffic）还是货币汇率（Exchange）。

# 模块消融实验（无预训练）


表 4-6 消融实验结果 (预测步长 720)


<table><tr><td colspan="2">数据集</td><td>Weather</td><td>ETTm1</td><td>ECL</td><td>Traffic</td><td>Exchange</td></tr><tr><td rowspan="2">w/o 预训练</td><td>MSE</td><td>0.322</td><td>0.461</td><td>0.196</td><td>0.593</td><td>0.886</td></tr><tr><td>MAE</td><td>0.330</td><td>0.432</td><td>0.297</td><td>0.320</td><td>0.711</td></tr><tr><td>wo/ 多头时间</td><td>MSE</td><td>0.355</td><td>0.480</td><td>0.241</td><td>0.644</td><td>0.932</td></tr><tr><td>混合模块</td><td>MAE</td><td>0.350</td><td>0.462</td><td>0.353</td><td>0.347</td><td>0.735</td></tr><tr><td>wo/ 多尺度</td><td>MSE</td><td>0.407</td><td>0.622</td><td>0.324</td><td>0.853</td><td>1.261</td></tr><tr><td>GCN</td><td>MAE</td><td>0.411</td><td>0.539</td><td>0.420</td><td>0.433</td><td>0.967</td></tr></table>

为进一步分析 PMRWKV-GCN 模型中每个模块的贡献，我们在去除预训练阶段的情

况下进行了模块消融实验。此分析有助于理解多头时间混合模块和多尺度 GCN 模块的重要性。依次移除各个模块后的实验结果如表 4-6 所示。

实验结果表明，多头时间混合模块和多尺度 GCN 模块都对模型性能有重要贡献，移除其中任何一个模块都会导致误差指标显著上升。

wo/ 多头时间混合模块：移除多头时间混合模块会导致性能下降，在不同的数据集上，MSE 会增加约  $3.96\%$  至  $18.67\%$  。这验证了该模块在捕捉时间依赖性和提高预测准确性方面具有重要作用。

wo/ 多尺度 GCN：多尺度 GCN 模块在处理如 Traffic 和 Electricity 等具有复杂空间依赖关系的数据集时尤为重要。移除该模块后，模型性能显著下降，尤其是 MSE 增幅明显，范围从  $15.65\%$  到  $39.51\%$  。这表明，多尺度 GCN 模块在捕捉数据中的空间依赖关系方面起着关键作用。

消融实验表明这两个模块都对模型的性能有重要影响。尤其是多尺度 GCN 模块在处理跨时间尺度的空间依赖性方面起到关键作用，移除该模块后会导致模型性能显著下降。

# 4.4 本章小结

本章提出了一个用于多元时间序列预测的 PMRWKV-GCN 模型，旨在通过捕捉多尺度的时间和空间依赖关系来提升预测性能。该模型结合了受 RWKV 架构启发的多头时间混合模块与多尺度 GCN 模块，引入了预训练阶段，并利用了 FFT 提取的周期信息，帮助模型学习多元时间序列数据中复杂的依赖关系。实验结果表明，预训练阶段帮助模型在微调之前学习关键的时间依赖性，从而提高预测的准确性。消融实验进一步证明，多尺度 GCN 在捕捉空间依赖性方面至关重要，移除该模块会显著降低模型性能，尤其是在处理具有复杂序列关系的数据集时。该模型在多个数据集和不同预测范围内均优于现有的最先进基线模型（MSGNet）。即使在未进行预训练的情况下，PMRWKV-GCN 模型仍展现出强大的竞争力，验证其在多元时间序列预测中的鲁棒性和有效性。综上所述，PMRWKV-GCN 模型通过结合多头时间混合模块与多尺度 GCN 模块，在多元时间序列预测中表现出优秀的预测性能，为该领域提供了强有力的工具。

# 第五章 基于多尺度接收加权键值与二维时间卷积网络的预测模型

光伏功率预测作为时间序列预测的一个应用领域，在优化电力调度和维护电网稳定性方面起着重要作用。然而，由于光伏发电具有复杂的周期性，并且受到天气条件的显著影响，进一步增加了其不稳定性、间歇性和随机性，使得光伏发电功率预测更加具有挑战性。因此，本章提了出一种基于多尺度接收加权键值与二维时间卷积网络（Multi-scale Receptance Weighted Key Value with 2-dimensional Temporal Convolutional Network, MSRWKV-2DTCN）的短期光伏功率预测模型，该模型能够有效学习数据的周期性特征以及序列间的复杂依赖关系，从而提高预测的准确性。首先，MSRWKV-2DTCN 模型使用 FFT 识别光伏发电数据中的多种周期性。随后，我们将这些识别出的周期性特征与 RWKV 的时间混合模块相结合，设计了多尺度时间混合模块来学习历史数据的周期性。最后，为捕捉历史数据中复杂的空间依赖关系，我们用多尺度二维时间卷积网络（2-dimensional Temporal Convolutional Network, 2D TCN）替代 RWKV 中的通道混合模块。实验在澳大利亚尤拉拉太阳能系统收集的数据集上进行，以验证所提出模型的性能。对比实验和消融实验表明，MSRWKV-2DTCN 在短期光伏发电功率预测中的准确性和鲁棒性优于现有的先进模型。

# 5.1 引言

第三章和第四章提出了两种用于多元时间序列预测的方法，能够有效地学习时间序列中的趋势、季节性以及复杂的时空依赖关系。并且在多个领域的数据集上进行了实验，验证了所提出的时间序列预测算法的有效性。在本章中，我们针对短期光伏系统发电功率预测问题进行了深入研究，采集并处理了真实的光伏系统发电功率及相关气象条件的时间序列数据，并设计了先进的预测算法。实验结果表明，本章提出的MSRWKV-2DTCN模型在短期光伏功率预测任务中，在多个评价指标上的表现均优于现有的最先进模型，充分展示了MSRWKV-2DTCN在实际应用中的优势和潜力。

随着全球能源需求的持续增长，人们越来越倾向于使用替代性可再生能源，以减少

温室气体排放。太阳能作为一种可再生且环境友好的能源，正日益在能源系统中发挥着重要作用[121,122]。太阳能是人口稠密的城市地区理想的能源需求解决方案，是到2050年实现净零碳能源供应的关键组成部分[123]。然而，光伏电力系统的输出具有复杂的周期性，并受外部气象因素影响，从而导致了间歇性、不稳定性和随机性，给电网的稳定和平衡带来了挑战[124]。因此，提高光伏发电功率预测的精度对于电网的有效调度至关重要。光伏发电功率预测的框架如图5-1所示。

![](images/19827cc91a812dbeac4ca5f04ebe57f9c32df37a6a4ae6b84ce085a8e92269f5.jpg)



图5-1光伏发电功率预测框架


根据当前对光伏功率预测的众多研究成果[125,126,127]，我们将光伏功率预测模型划分为三类：物理模型、数据驱动模型和混合模型。分类方法如图5-2所示。

物理模型，通常被称为“白盒（White Box）”方法，利用数学方程和算法来模拟光伏发电系统将太阳能转化为电能的物理过程[128]。根据物理模型的输入，可分为三类：卫星成像、天空图像和数值天气预报（Numerical Weather Prediction, NWP）[125]。虽然物理模型能提供一定程度的解释性，但由于对外部数据（如卫星信息）依赖较强，当天气条件变化剧烈或外部数据更新不够及时，其预测性能会大幅下降，限制了其在复杂天气条件下的应用[129,130]。

数据驱动模型利用历史光伏功率数据和历史气象数据来预测光伏系统的未来输出。  
数据驱动模型包括持久模型、统计模型和深度学习模型。持久模型基于一个简单假设，

即当前天的光伏输出决定了第二天的输出。统计模型利用历史数据并试图找出历史数据的数学模式。常用的统计模型包括马尔可夫链（Markov Chain）[131]、AR[132]、ES[133]、ARMA[134]、模糊逻辑（Fuzzy Logic）[135]等。这些方法通过在训练阶段不断优化参数，能够实现可靠的预测性能[136]。然而，上述基于统计模型的预测算法在学习复杂的非线性关系时仍然存在不足[137]。

![](images/dd4a3c611c11a6c4fd781e4c91b7d32562f4b8a7fe71362c4ab48f8adc3202e1.jpg)



图5-2光伏发电功率预测方法分类


随着深度学习技术的发展，深度学习模型在捕捉光伏发电数据中的非线性特征和复杂模式方面展现出优势[138,139]。SVM被广泛应用于风能和太阳能预测中，由于其简单且具有一定的可解释性，取得良好的效果[140]。RNN在处理时间序列数据方面展现出强大的能力[141,142]，其变体如LSTM[143]和GRU[144]，通过缓解传统RNN中的梯度消失问题，在捕捉长期依赖关系上取得更好的表现，被广泛用于光伏功率预测任务。与此同时，CNN则在捕捉数据的局部特征和空间信息方面具有明显优势，在光伏功率预测领域取得显著效果[145,146]。然而，这些模型在处理长期依赖关系和复杂多变量之间的空间依赖时仍存在不足。

混合模型通过结合物理模型、统计模型与深度学习模型，整合多种方法的优势，进

一步提高预测精度[147]。在实际应用中，一些研究结合RNN和CNN的特点，利用RNN捕捉时间依赖性，利用CNN提取空间特征，来学习历史数据的时空信息[148,149,150]。常用的混合模型包括：LSTM-CNN[151]、GRU-CNN[152]、CNN-LSTM[153]、带LSTM的小波分解（Wavelet Packet Decomposition, WPD）[154]、ConLSTM-BDGRU[155]等。在光伏功率预测领域，这些混合模型取得了很好的预测效果。

深度学习方法如 RNN 及其变体，面临长期依赖性不足、训练无法并行化以及对序列长度不敏感等问题，导致信息丢失或冗余。为解决这些问题，Vaswani 等人[10]提出了 Transformer 模型，该模型通过注意力机制代替循环神经网络中的隐藏层更新，能够增强模型在处理长时间依赖关系时的表现。Transformer 已被广泛用于光伏功率预测领域。Sherozbek 等人[156]使用 Transformer 的编码器架构进行光伏功率预测，并实现较低的误差。Phan 等人[157]结合 Transformer 架构和 XGBoost，来提高光伏功率预测的精度。Wang[121]等人将 Transformer 架构与因果卷积网络结合用于光伏功率预测。然而，Transformer 在处理长序列时计算复杂度较高，特别是在于长序列场景下。为此，Peng 等人[66]提出 RWKV 模型，该模型利用线性注意力机制，避免 Transformer 的二次复杂度问题，同时实现并行化训练。

当前的光伏发电功率预测研究在同时捕捉历史数据的复杂周期性和非线性依赖关系方面存在局限性，这影响了光伏发电功率预测的准确性。此外，尽管RWKV在处理长序列依赖关系上已被证明是有效的[109,110]，但其在捕捉长期依赖性时仍存在不足，因为它仅关注上一时刻与当前的状态，而标准Transformer通过点积注意力机制获取整个序列的信息。此外，RWKV中的通道混合模块在提取多种特征之间的复杂依赖关系方面表现不足。因此，本章提出了一种名为MSRWKV-2DTCN的混合模型，该模型结合FFT和2D TCN与RWKV架构，旨在提高短期光伏功率预测的准确性。FFT将时间序列数据转换到频域进行分析，识别出序列中的多重周期模式，帮助RWKV模型的时间混合模块聚焦于关键的周期变化信息。通过FFT发现的多周期性，2D TCN能够捕捉不同序列之间的多尺度依赖关系。

本章主要研究内容如下：

（1）当前光伏发电功率预测在有效捕捉历史数据复杂周期性方面存在不足。为此，

本章提出MSRWKV-2DTCN的混合模型，该模型能够学习光伏功率历史数据和外部气象条件的多尺度依赖关系，提高短期光伏功率预测精度。

（2）设计了一种新的多尺度时间混合模块，将RWKV的时间混合模块与FFT识别出的周期特征结合起来。该模块能够跨越多个时间尺度从历史数据中提取时间依赖关系，并自适应地聚合这些依赖性信息。

（3）鉴于气象条件对光伏功率的显著影响，本章通过使用2D TCN替代RWKV模型的通道混合模块，增强了RWKV模型对复杂空间依赖关系的学习能力。

本章其余部分的结构如下：在5.2节中详细介绍MSRWKV-2DTCN框架；在5.3节中首先介绍实验数据集、数据预处理、实验设置，然后分析实验结果；在5.4节中总结本章的主要工作。

# 5.2基于多尺度接收加权键值与二维时间卷积网络

MSRWKV-2DTCN光伏功率预测模型的框架如图5-3所示。首先，使用FFT将历史时间序列数据转换到频域，以识别其中的各种周期模式。然后，将这些识别出的周期信息和历史数据输入到多尺度时间混合模块和2DTCN模块中，这两个模块协同作用，结合频域信息和时域数据进行特征提取与时空依赖关系的建模。每个模块的具体细节将在接下来的章节中详细介绍。

# 5.2.1 基于FFT的周期识别

在现实世界中，特别是在光伏发电领域，时间序列通常表现出多种周期性模式。光伏发电功率预测数据集通常包含历史光伏输出功率和气象信息，这两者都显示出明显的周期性波动。气象信息可能呈现出日、月、年的周期变化，这些变化直接影响光伏输出功率，增加了其复杂性和不稳定性。理解这些周期模式对于建立准确的预测模型至关重要，因为它们对整体数据趋势产生了显著影响。因此，识别和分析这些周期性，特别是关注气象条件对光伏系统输出的影响，对于提高预测精度至关重要。


模块1：数据预处理


![](images/3474ef3465507c098063dd4b764a350f9778e70f618afe1d7bfadc468b0af7c8.jpg)



模块2：模型训练


![](images/9ccb1c80ea3a9eec37c14fd948592eb7820688fc0e30836fad64186b78f47af8.jpg)



模块3：模型评估


![](images/ec3ee9b87c387323ca6e502a7ec5fc07e4ee26dbde544e734c366b3d0bbd7a26.jpg)



图5-3MSRWKV-2DTCN光伏功率预测框架


为此，我们采用FFT技术将时间序列数据转换为频域，从而能够识别历史光伏输出功率和气象信息中的各种周期模式，具体如下公式所示：

$$
\mathbf {A} = A \operatorname {v g} (A m p (F F T (\mathbf {X}))) \tag {5-1}
$$

$$
\left\{f _ {1}, f _ {2}, \dots , f _ {k} \right\} = T o p k (\mathbf {A}) \tag {5-2}
$$

$$
f _ {s} \in \left\{1, \dots , \left[ \frac {L}{2} \right] \right\}
$$

其中， $\mathbf{X} \in R^{L \times N}$  表示包含光伏发电功率和气象信息的多元时间序列数据， $FFT(\mathbf{X})$  表示对  $\mathbf{X} \in R^{L \times N}$  进行 FFT 的过程， $Amp()$  用于计算振幅值， $Avg()$  表示计算 N 个维度平均值的， $L$  是序列长度。由于频率域是共轭的，因此只考虑范围为  $\{1, \dots, [L/2]\}$  的频率。为减少频域稀疏性和减少来自无关高频的噪声，我们选择前  $k$  个值，其中  $k$  是一个超参数。通过这种选择方法，可以找到前  $k$  个重要频率  $\{f_1, f_2, \dots, f_k\}$  并获得  $k$  个对应的振幅  $\left\{A_{f_1}, A_{f_2}, \dots, A_{f_k}\right\}$ 。

选定的频率对应于  $k$  个重要的周期  $P = \{p_{1}, p_{2}, \dots, p_{k}\}$ , 这些周期通过以下公式计算:

$$
p _ {i} = \frac {L}{f _ {i}}, i \in \{1, \dots , k \} \tag {5-3}
$$

# 5.2.2 多尺度时间混合模块

RWKV 模型的标准时间混合模块采用线性注意力机制实现了并行训练，类似于 Transformer。然而，RWKV 模型在捕捉复杂依赖关系时遇到挑战，因为它只考虑前一个时间步和当前状态。光伏发电系统的输出受天气条件的显著影响，通常呈现出复杂的周期性模式。因此，捕捉这些周期信息对于建立准确的预测模型至关重要，尤其是在预测光伏发电功率这样具有不稳定性和间歇性的场景下。为了解决这一问题，我们设计了一个多尺度时间混合模块，该模块能够扩展模型的感受野，处理不同时间尺度的周期模式，保留关键的周期信息，学习多尺度下的时间依赖关系。该模块的计算过程如下：

$$
\left\{A _ {f _ {1}} ^ {\prime}, A _ {f _ {2}} ^ {\prime}, \dots , A _ {f _ {k}} ^ {\prime} \right\} = \operatorname {S o f t m a x} \left(A _ {f _ {1}}, A _ {f _ {2}}, \dots , A _ {f _ {k}}\right) \tag {5-4}
$$

$$
r _ {t} ^ {\prime} = W _ {r} \cdot \left(\mu_ {r} x _ {t} + \left(1 - \mu_ {r}\right) \left(x _ {t - 1} + \sum_ {i = 1} ^ {k} A _ {f _ {i}} ^ {\prime} x _ {t - p _ {i}}\right)\right) \tag {5-5}
$$

$$
k _ {t} ^ {\prime} = W _ {k} \cdot \left(\mu_ {k} x _ {t} + \left(1 - \mu_ {k}\right) \left(x _ {t - 1} + \sum_ {i = 1} ^ {k} A _ {f _ {i}} ^ {\prime} x _ {t - p _ {i}}\right)\right) \tag {5-6}
$$

$$
v _ {t} ^ {\prime} = W _ {v} \cdot \left(\mu_ {v} x _ {t} + \left(1 - \mu_ {v}\right) \left(x _ {t - 1} + \sum_ {i = 1} ^ {k} A _ {f _ {i}} ^ {\prime} x _ {t - p _ {i}}\right)\right) \tag {5-7}
$$

$$
w k v _ {t} ^ {\prime} = \frac {\sum_ {i = 1} ^ {t - 1} e ^ {- (t - 1 - i) w + k _ {i} ^ {\prime}} v _ {i} ^ {\prime} + e ^ {u + k _ {i} ^ {\prime}} v _ {i} ^ {\prime}}{\sum_ {i = 1} ^ {t - 1} e ^ {- (t - 1 - i) w + k _ {i} ^ {\prime}} + e ^ {u + k _ {i} ^ {\prime}}} \tag {5-8}
$$

$$
o _ {t} = W _ {o} \cdot \left(\sigma \left(r _ {t} ^ {\prime}\right) \odot w k v _ {t} ^ {\prime}\right) \tag {5-9}
$$

其中， $\left\{A_{f_1}, A_{f_2}, \dots, A_{f_k}\right\}$  是对应前  $k$  个重要频率  $\left\{f_1, f_2, \dots, f_k\right\}$  的振幅，参数  $u_r$ 、 $u_k$  和  $u_v$  表示时间步长偏移策略的插值权重， $w$  和  $u$  是标量，符号  $\sigma$  表示 Sigmoid 函数。受 TimesNet[120]和 Autoformer[101]等研究的启发，所选择的频率及其对应周期的重要性可以通过振幅值  $\left\{A_{f_1}, A_{f_2}, \dots, A_{f_k}\right\}$  来表示。在公式（5-4）中，选定的前  $k$  个振幅值经过 Softmax 函

数处理, 作为每个周期尺度的权重。这使得模型在计算  $r_{t}^{\prime} 、 k_{t}^{\prime}$  和  $\nu_{t}^{\prime}$  时能够自适应地聚合多种周期特征。因此, 所提出的多尺度时间混合模块不仅考虑到当前和前一步状态, 还能够整合不同尺度下的关键周期信息。

# 5.2.3 多尺度二维卷积网络

光伏发电系统的输出功率受气象因素的显著影响，因此捕捉数据中的复杂相互依赖关系对于提高的光伏发电功率预测的准确性至关重要。尽管RWKV模型在序列建模中表现优秀，但由于其通道混合模块的设计较为简单，它在捕捉不同时间序列特征之间的复杂依赖关系方面存在局限。为解决这一问题，我们将通道混合模块替换为多尺度2D TCN模块，以更好地捕捉数据中的多尺度依赖关系。

TCN 通常使用一维卷积来处理时间序列数据。一维卷积操作将卷积核应用于每个时间步，以捕捉局部模式和长期依赖关系。然而，一维卷积操作难以充分捕捉光伏发电功率与气象信息之间的复杂依赖关系，因为它们将卷积核独立应用于单个时间序列，无法直接建模多元时间序列中不同序列之间的相互关系。因此，我们引入了多尺度 2D TCN 模块，以更好地建模光伏发电功率与外部气象因素之间的复杂依赖关系。为避免信息泄露，我们在时间维度的左侧应用零值填充，填充长度为  $(K - 1) \times D$  ，其中  $K$  表示卷积核的大小， $D$  表示扩展率。此外，特征维度的前  $(K - 1) \times D$  个特征会被复制，并放置在最后一个特征之后，作为特征维度的填充数据。

在提出的多尺度2D TCN中，选择的  $k$  个周期的长度  $\{p_1,p_2,\dots ,p_k\}$  用作2D卷积的卷积核。这些由  $k$  个卷积核生成的不同特征表示，通过使用权重  $\left\{A_{f_1}',A_{f_2}'\dots ,A_{f_k}'\right\}$  自适应地进行聚合：

$$
X ^ {\prime M \times N} = X ^ {M \times N} + \sum_ {i = 1} ^ {k} A _ {f _ {i}} ^ {\prime} \times X _ {i} ^ {\prime M \times N} \tag {5-10}
$$

其中， $X^{M \times N}$  表示前一层的输出， $X_{i}^{\prime M \times N}$  表示使用大小为  $p_{i} \times p_{i}$  的卷积核进行卷积操作的结果， $A_{f_{i}}^{\prime}$  表示频率  $f_{i}$  的相对权重。2D TCN 的网络结构如图 5-4 所示。

![](images/5d091d83cf7fbbb78f353a3132671091f0c5d9bac26c864633410ae998d9c66c.jpg)



图5-42DTCN的网络结构


# 5.3 实验

# 5.3.1 实验数据集

本章的实验是在澳大利亚沙漠知识太阳能中心（Desert Knowledge Australia Solar Centre, DKASC）尤拉拉太阳能系统（Yulara Solar System）收集的数据集上进行[158]。为全面评估模型的性能，我们选取两个具有代表性的光伏阵列，包括沙漠花园阵列（Desert Gardens Array, DKASC-DG）和康奈兰机场阵列（Connellan Airport Array, DKASC-CA）。


表 5-1 两个光伏系统的配置


<table><tr><td>技术说明</td><td>DKASC-CA</td><td>DKASC-DG</td></tr><tr><td>发电能力</td><td>105.9kW</td><td>1058.4kW</td></tr><tr><td>阵列结构</td><td>固定:屋顶安装</td><td>固定:地面安装</td></tr><tr><td>面板数量</td><td>326</td><td>3360</td></tr><tr><td>光伏技术</td><td>mono-Si</td><td>poly-Si</td></tr><tr><td>阵列倾斜/方位</td><td>倾斜=10,方位=40(E of N)</td><td>倾斜=15,方位=0(Solar North)</td></tr><tr><td>安装时间</td><td>2016年3月</td><td>2016年3月</td></tr></table>

![](images/2727f40fc8a82fb38654682ffcc8c05492c5b7fb2a4b8ef59c87cf0d6137c558.jpg)



沙漠花园阵列


![](images/abecf6e182d1d8fda5af0e5420b7b528b2c1ae08705b553ace8ca87c7ee6d4ed.jpg)



康奈尔机场阵列


![](images/3af1cf414665dbb64de4728b856df7dac098d44b9459b80dcf8211563b179ca1.jpg)



尤拉拉光伏发电系统



图5-5尤拉拉光伏发电系统及两个选定的光伏阵列



表 5-2 两个光伏系统的数据特征和气象信息


<table><tr><td></td><td>特征</td><td>Min</td><td>25%</td><td>50%</td><td>75%</td><td>Max</td><td>Mean</td><td>STD</td><td>单位</td></tr><tr><td rowspan="2">DKASC-DG</td><td>AP</td><td>-0.25</td><td>-0.17</td><td>0.31</td><td>226.5</td><td>791.55</td><td>121.08</td><td>170.39</td><td>KW</td></tr><tr><td>CPA</td><td>0.00</td><td>4.43</td><td>7.30</td><td>326.69</td><td>1127.85</td><td>177.37</td><td>242.53</td><td>A</td></tr><tr><td rowspan="2">DKASC-CA</td><td>AP</td><td>-0.04</td><td>-0.04</td><td>0.01</td><td>45.45</td><td>102.13</td><td>22.00</td><td>30.54</td><td>KW</td></tr><tr><td>CPA</td><td>0.58</td><td>0.62</td><td>0.83</td><td>66.79</td><td>151.74</td><td>32.73</td><td>44.52</td><td>A</td></tr><tr><td rowspan="10">气象信息</td><td>WS</td><td>0.00</td><td>1.10</td><td>1.85</td><td>2.77</td><td>12.88</td><td>2.03</td><td>1.21</td><td>m/s</td></tr><tr><td>WTC</td><td>0.10</td><td>16.10</td><td>22.30</td><td>28.17</td><td>44.82</td><td>22.15</td><td>8.27</td><td>°C</td></tr><tr><td>GHR</td><td>-15.00</td><td>-3.37</td><td>2.25</td><td>510.69</td><td>1406.09</td><td>252.54</td><td>349.65</td><td>w/m²</td></tr><tr><td>WD</td><td>0.00</td><td>130.37</td><td>152.47</td><td>197.03</td><td>349.53</td><td>167.35</td><td>60.29</td><td>°</td></tr><tr><td>WDR</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.12</td><td>0.00</td><td>0.00</td><td>mm</td></tr><tr><td>MWS</td><td>0.00</td><td>2.10</td><td>3.50</td><td>5.40</td><td>24.30</td><td>3.86</td><td>2.27</td><td>m/s</td></tr><tr><td>APre</td><td>940.45</td><td>952.43</td><td>956.30</td><td>960.51</td><td>972.71</td><td>956.64</td><td>5.49</td><td>hPa</td></tr><tr><td>PM</td><td>-14.13</td><td>-3.17</td><td>2.26</td><td>549.70</td><td>1402.57</td><td>268.54</td><td>368.52</td><td>w/m²</td></tr><tr><td>TP1</td><td>30.39</td><td>35.54</td><td>39.96</td><td>49.14</td><td>82.16</td><td>43.33</td><td>10.34</td><td>°C</td></tr><tr><td>TP2</td><td>30.37</td><td>35.40</td><td>39.84</td><td>50.02</td><td>80.61</td><td>43.46</td><td>10.46</td><td>°C</td></tr></table>

选定的两个光伏阵列的配置细节和位置如表5-1和图5-5所示。数据集中包括由光伏发电系统记录的发电数据和由当地气象站记录的气象信息。每个数据集包含时间戳、有功功率（Active Power, AP）、当前相位平均值（Current Phase Average, CPA）、以及气象信息，如每日降雨量（Weather Daily Rainfall, WDR）、风速（Wind Speed, WS）、风向（Wind Direction, WD）、最大风速（Max Wind Speed, MWS）、气压（Air Pressure, APre）、全球水平辐射（Global Horizontal Radiation, GHR）、日射强度计（Pyranometer, PM）、气温（Weather Temperature Celsius, WTC）、温度探头1（Temperature Probe 1, TP1）和温度探头2（Temperature Probe 2, TP2）。这两个数据集的时间跨度为2021年1月1日00:00:00至2021年12月31日23:55:00，时间步间隔为5分钟。关于光伏系统和气象数据的统计信息如表5-2所示。DKASC-DG数据集在1月份的历史数据和气象信息如图5-6所示。

AP 表示光伏发电系统的有功功率。如表 5-2 所示，最小的有功功率值为负。这是由于光伏系统在夜间不发电时，太阳能发电设备仍有基本功耗。由于不需要预测设备的基本功耗，因此我们按照 Khan 等人[146]的方法，将负值转换为零。

![](images/cd6ac85adb30acac2796b1a48c9003bc22cfd319d2ca491fff7b09f38f9e3eca.jpg)



图5-6DKASC-DG数据集1月份历史数据和气象信息


实验中选择的这两个数据集存在两种情况下的缺失值：一种是随机分布的缺失值，另一种是连续分布的缺失值。缺失的数据会导致部分信息丢失，可能会影响模型的准确性和泛化能力。如果缺失数据的模式与其他数据相关联，模型可能会因这种偏差而得出错误的推论。特别是存在连续缺失数据点的情况下，这种影响可能更加明显。为应对这两个数据集中的缺失值问题，我们设计了一种分段填充策略：对于连续缺失值少于12个数据点（即缺失时间段在1小时内）的情况，使用线性插值法填充缺失的时间点；对于连续缺失值超过12个数据点的情况，使用缺失数据前后七天对应时间点的平均值来填充缺失值。线性插值方法公式如下：

$$
X ^ {\prime} = \frac {t _ {\text {m i s s}} - t _ {\text {p r e}}}{t _ {\text {n e x t}} - t _ {\text {p r e}}} \times X _ {\text {p r e}} + \frac {t _ {\text {n e x t}} - t _ {\text {m i s s}}}{t _ {\text {n e x t}} - t _ {\text {p r e}}} \times X _ {\text {n e x t}} \tag {5-11}
$$

其中， $X'$  是计算出的缺失点的值； $X_{pre}$  和  $X_{next}$  分别表示缺失点前、后的第一个可用点的

值；  $t_{miss}$  、  $t_{pre}$  和  $t_{next}$  分别表示缺失值的时间点、  $X_{pre}$  的时间点和  $X_{next}$  的时间点。

在本章的研究中，数据通过Min-Max归一化缩放至[0,1]的范围。由于光伏输出和气象因素的变化更接近周期性，而非正态分布，且不同特征的取值范围差异较大，Min-Max归一化方法更为适用。该方法不依赖于数据分布的假设，能够保持数据的相对大小关系，消除特征间的尺度差异。Min-Max归一化的公式如下所示：

$$
x _ {i} ^ {\prime n} = \frac {x _ {i} ^ {n} - x _ {\min} ^ {n}}{x _ {\max} ^ {n} - x _ {\min} ^ {n}} \tag {5-12}
$$

其中， $x_{i}^{n}$  表示第  $n$  个特征的第  $i$  个数据点； $x_{\min}^{n}$  和  $x_{\max}^{n}$  分别表示第  $n$  个特征的最小值和最大值； $x_{i}^{\prime n}$  是归一化后的值。

# 5.3.2 实验设置

本章通过与多种深度学习模型进行对比，评估所提出的MSRWKV-2DTCN的有效性。这些比较方法可以分为三类：单一模型（LSTM[143]和  $\mathrm{TCN}^{[31]}$  ）、混合模型（DSN[147]、CNN-LSTM[153]和GRU-CNN[152]），以及基于Transformer架构的模型（Informer[100]和Crossformer[107]）。LSTM和TCN是用于序列建模任务的传统单一模型。DSN是光伏功率预测领域目前最先进的方法，通过结合GRU和CNN来学习历史数据的时间特征和空间模式。Informer和Crossformer是两种用于时间序列预测的高效方法。Informer采用ProbSparse自注意力机制和知识蒸馏，以减轻Transformer的计算复杂度。Crossformer通过两阶段注意力机制建模跨时间和跨维度的依赖关系。

模型的性能通过以下指标进行评估：均方误差（MSE）、平均绝对误差（MAE）、均方根误差（Root Mean Square Error, RMSE）、归一化均方根误差（Normalized Root Mean Square Error, NRMSE）和平均偏移误差（Mean Bias Error, MBE）。

$$
M S E = \frac {1}{N} \sum_ {i = 1} ^ {N} \left(x _ {i} ^ {\prime} - x _ {i}\right) ^ {2} \tag {5-13}
$$

$$
M A E = \frac {1}{N} \sum_ {i = 1} ^ {N} \left| x _ {i} ^ {\prime} - x _ {i} \right| \tag {5-14}
$$

$$
R M S E = \sqrt {\frac {1}{N} \sum_ {i = 1} ^ {N} \left(x _ {i} ^ {\prime} - x _ {i}\right) ^ {2}} \tag {5-15}
$$

$$
N R M S E = \frac {\sqrt {\frac {1}{N} \sum_ {i = 1} ^ {N} \left(x _ {i} ^ {\prime} - x _ {i}\right) ^ {2}}}{X _ {\text {m e a n}}} \tag {5-16}
$$

$$
M B E = \frac {1}{N} \sum_ {i = 1} ^ {N} \left(x _ {i} ^ {\prime} - x _ {i}\right) \tag {5-17}
$$

其中， $N$  表示样本数量； $x_{i}$  和  $x_{i}^{\prime}$  分别表示实际值和预测值； $X_{mean}$  是实际值的平均值。

MSE、MBE和MAE用于评估预测值与实际值之间的差异。NRMSE是将RMSE归一化为某个范围、平均值或其他统计属性后的值。这使得它成为无量纲指标，能够进行跨数据集的比较。每个指标从不同的角度评估模型性能，以全面比较模型的预测效果和误差特性。


表 5-3 超参数取值范围


<table><tr><td>超参数</td><td>取值范围</td></tr><tr><td>叠加层数</td><td>[1,2,3,4]</td></tr><tr><td>k的取值</td><td>[3,5,7,9]</td></tr><tr><td>隐藏层</td><td>[128,256,512,1024]</td></tr></table>

训练过程中使用ADAM优化器和L2损失函数。对于MSRWKV-2DTCN和对比方法，回溯窗口设为288（1天)，预测时间跨度为12（1小时)。实验数据集按7:1:2的比例分为训练集、验证集和测试集。实验在配备有两个NVIDIA GeForce RTX 2080 Ti GPU的工作站，使用PyTorch深度学习框架进行。MSRWKV-2DTCN的超参数范围如表5-3所示，并根据验证集上的滚动验证误差进行调优。根据实验结果，模型的超参数设置如下：堆叠层数  $= 2$  ，  $\mathrm{k} = 5$  ，隐藏层大小  $= 128$  。

对于多尺度2D TCN，扩展率设为[1,2,4]。研究表明，这组扩展率在模型复杂性和性能之间提供良好的平衡[159,160]。此外，扩展率的选择还考虑了输入序列长度和卷积核尺寸，确保扩展后的卷积核尺寸不会超过输入数据的长度，这对于保持时间特征和计算效率至关重要。因此，我们在实验中直接采用了[1,2,4]作为扩展率。

# 5.3.3 实验结果分析

MSRWKV-2DTCN与对比模型的实验结果如表5-4所示。最佳结果用加粗字体显示。实验结果表明，混合模型和基于Transformer的模型相比单一模型具有较低的误差，因为

单一模型在学习空间模式和时间依赖关系上存在局限性。MSRWKV-2DTCN 在 DKASC-DG 数据集上相比其他预测模型的 MSE、MAE、MBE、RMSE 和 NRMSE 分别表现出了 0.0043、0.0277、-0.0021、0.0656 和 0.2806 的较低误差。与 DSN 在这些指标上的结果相比，MSRWKV-2DTCN 分别在 MSE、MAE、MBE、RMSE 和 NRMSE 上减少了  $24.56\%$  、 $32.60\%$  、 $61.11\%$  、 $13.11\%$  和  $13.13\%$  。MSRWKV-2DTCN 在 DKASC-CA 数据集上的表现同样优于对比方法。MSRWKV-2DTCN 及对比方法的性能比较如图 5-7 所示。由于 NRMSE 的值明显大于其它指标，为了更好的视觉效果，图中未展示 NRMSE。从图 5-7 中可以观察到，MSRWKV-2DTCN、DSN 和 Crossformer 表现出较强的性能，且 MSRWKV-2DTCN 表现最佳。


表 5-4 MSRWKV-2DTCN 和对比模型的实验结果


<table><tr><td>数据集</td><td>模型</td><td>MSE</td><td>MAE</td><td>MBE</td><td>RMSE</td><td>NRMSE</td></tr><tr><td rowspan="8">DKASC-DG</td><td>MSRWKV-2DTCN</td><td>0.0043</td><td>0.0277</td><td>-0.0021</td><td>0.0656</td><td>0.2806</td></tr><tr><td>DSN</td><td>0.0057</td><td>0.0411</td><td>-0.0054</td><td>0.0755</td><td>0.3230</td></tr><tr><td>GRU-CNN</td><td>0.0142</td><td>0.0928</td><td>-0.0227</td><td>0.1192</td><td>0.5098</td></tr><tr><td>CNN-LSTM</td><td>0.0167</td><td>0.0972</td><td>-0.0244</td><td>0.1292</td><td>0.5529</td></tr><tr><td>Crossformer</td><td>0.0074</td><td>0.0534</td><td>-0.0190</td><td>0.0860</td><td>0.3681</td></tr><tr><td>Informer</td><td>0.0114</td><td>0.0685</td><td>-0.0245</td><td>0.1068</td><td>0.4568</td></tr><tr><td>LSTM</td><td>0.0325</td><td>0.1157</td><td>-0.0345</td><td>0.1803</td><td>0.7713</td></tr><tr><td>TCN</td><td>0.0291</td><td>0.1012</td><td>-0.0311</td><td>0.1706</td><td>0.7299</td></tr><tr><td rowspan="8">DKASC-CA</td><td>MSRWKV-2DTCN</td><td>0.0047</td><td>0.0252</td><td>-0.0022</td><td>0.0686</td><td>0.2720</td></tr><tr><td>DSN</td><td>0.0061</td><td>0.0393</td><td>-0.0051</td><td>0.0781</td><td>0.3099</td></tr><tr><td>GRU-CNN</td><td>0.0155</td><td>0.0873</td><td>-0.0144</td><td>0.1245</td><td>0.4940</td></tr><tr><td>CNN-LSTM</td><td>0.0153</td><td>0.0920</td><td>-0.0152</td><td>0.1237</td><td>0.4908</td></tr><tr><td>Crossformer</td><td>0.0079</td><td>0.0501</td><td>-0.0045</td><td>0.0889</td><td>0.3526</td></tr><tr><td>Informer</td><td>0.0117</td><td>0.0637</td><td>-0.0092</td><td>0.1082</td><td>0.4292</td></tr><tr><td>LSTM</td><td>0.0303</td><td>0.1032</td><td>-0.0324</td><td>0.1741</td><td>0.6906</td></tr><tr><td>TCN</td><td>0.0270</td><td>0.0955</td><td>-0.0302</td><td>0.1643</td><td>0.6519</td></tr></table>

另外，光伏系统的输出功率极易受到天气变化的影响，尤其是多云、下雨等天气波动，这些变化对电网的稳定性构成挑战。因此，预测模型在降雨和多云天气下的表现对于电网规划和调度至关重要。MSRWKV-2DTCN 在晴天、多云和降雨条件下的预测结果如图 5-8 所示。在晴天，该模型能够精准预测未来功率输出，而在降雨和多云天气下，MSRWKV-2DTCN 也能够有效捕捉光伏功率的变化趋势。验证了该模型能够在不同气象条件下具有较强的鲁棒性和适应性，能够提高光伏功率预测的稳定性和准确性，进一步提高电力调度和系统运行的效率

![](images/e0f7458fb9e5c8670b51178c4c4537952277f955ee55b15afe18eeaf14912e7f.jpg)


![](images/89ea5131821ff08b07cc94dbee4c942aaad44bde6f6f9393ea29acd1359d3e58.jpg)



图5-7MSRWKV-2DTCN与其它模型的性能比较


![](images/d8b9154950920ea1c07bad22380f02315904fab034c930b54ca67b405b71b303.jpg)


![](images/bcf93bae5cb18cc3bac3278d8dd46c9fca1ad03610300c4fd0ad09630ae3ee4d.jpg)


![](images/8883a5aaa052b18b220c510bd217fd3f440c248ed3dfd9c6cba467f28eab9327.jpg)


![](images/bf5f9a03ebc08a4d15545874784c74c94c21ddf89a1ab74a651222b3b7bf44d6.jpg)


![](images/a1bcecb308ff64994f96626bd3df7931eb51342ad2e9232aef78c044bbbaf538.jpg)


![](images/a09f2e8da39b7d6bf6ed2efa532f436375321ad851015aba3c80a341780c7e27.jpg)



图 5-8 MSRWKV-2DTCN 在不同天气条件下的预测值与实际值对比


# 5.3.4 消融实验

为进一步分析MSRWKV-2DTCN中每个主要模块的贡献，我们在DKASC-DG和DKASC-CA数据集上进行了消融实验。MSRWKV-2DTCN模型的每个模块依次被替换或移除，以形成不同的子框架：

- w/o MSRWKV：将多尺度时间混合模块替换为RWKV的标准时间混合模块。

- w/o 2DTCN：将多尺度2D TCN替换为标准TCN。

RWKV：使用标准RWKV架构。

MSRWKV-2DTCN 模型及其子框架在 DKASC-DG 和 DKASC-CA 数据集上的对比结果如表 5-5 所示。结果显示，完整的 MSRWKV-2DTCN 模型在所有指标上均优于其它子框架。值得注意的是，w/o 2DTCN 所产生的误差要比 w/o MSRWKV 产生的误差更高，这验证了多尺度 2D TCN 在捕捉光伏功率和外部因素之间关系方面的有效性。标准 RWKV 架构在两个数据集中得到了最高的误差，进一步验证多尺度时间混合模块和多尺度 2D TCN 在模型中的重要贡献。综上所述，消融实验证实了所提出的模块能够显著提高 MSRWKV-2DTCN 模型在光伏功率预测方面的性能。


表 5-5 消融实验结果


<table><tr><td>数据集</td><td>评价指标</td><td>DKASC-DG</td><td>DKASC-CA</td></tr><tr><td rowspan="3">MSRWKV-2DTCN</td><td>MSE</td><td>0.0043</td><td>0.0047</td></tr><tr><td>MAE</td><td>0.0277</td><td>0.0252</td></tr><tr><td>MBE</td><td>-0.0021</td><td>-0.0022</td></tr><tr><td rowspan="3">w/o MSRWKV</td><td>MSE</td><td>0.0059</td><td>0.0052</td></tr><tr><td>MAE</td><td>0.0437</td><td>0.0458</td></tr><tr><td>MBE</td><td>-0.0040</td><td>-0.0037</td></tr><tr><td rowspan="3">w/o 2DTCN</td><td>MSE</td><td>0.0075</td><td>0.0071</td></tr><tr><td>MAE</td><td>0.0518</td><td>0.0503</td></tr><tr><td>MBE</td><td>-0.0082</td><td>-0.0065</td></tr><tr><td rowspan="3">RWKV</td><td>MSE</td><td>0.0092</td><td>0.0085</td></tr><tr><td>MAE</td><td>0.0622</td><td>0.0641</td></tr><tr><td>MBE</td><td>-0.0102</td><td>-0.0080</td></tr></table>

# 5.3.5 不同季节预测效果分析

由于季节因素会影响光伏发电系统的输出，我们对DKASC-DG和DKASC-CA数据集每个季节的数据进行实验，以评估MSRWKV-2DTCN在不同季节的性能。相比之下，混合模型和基于Transformer的模型在捕捉时间特征和空间依赖性方面优于单一模型。因

此，单一模型在进一步的季节性分析中被排除。每个数据集被分为四个季节。DKASC-DG和DKASC-CA数据集在每个季节中的平均实验结果如表5-6和图5-9所示。在每个季节中，MSRWKV-2DTCN在所有评估指标上均优于其它混合模型和基于Transformer的模型。与DSN相比，所提出的模型在MSE、MAE、MBE、RMSE和NRMSE方面的平均降幅分别为  $31.06\%$  、  $34.81\%$  、  $43.29\%$  、  $16.99\%$  和  $16.99\%$  。DKASC-DG数据集四个季节的预测结果如图5-10所示，从图中可以看出，MSRWKV-2DTCN其它对比算法相比，取得了最优的预测效果。图5-10的子图(b)中展示了MSRWKV-2DTCN在秋季多云天气条件下的预测结果，进一步验证了MSRWKV-2DTCN能够在复杂的天气条件下有效捕捉光伏系统输出功率的趋势。


表 5-6 各季节的比较分析


<table><tr><td>模型</td><td>季节</td><td>MSE</td><td>MAE</td><td>MBE</td><td>RMSE</td><td>NRMSE</td></tr><tr><td rowspan="4">MSRWKV-2DTCN</td><td>春季</td><td>0.0031</td><td>0.0283</td><td>-0.0022</td><td>0.0557</td><td>0.2063</td></tr><tr><td>夏季</td><td>0.0034</td><td>0.0306</td><td>-0.0063</td><td>0.0583</td><td>0.2270</td></tr><tr><td>秋季</td><td>0.0042</td><td>0.0340</td><td>-0.0061</td><td>0.0648</td><td>0.3480</td></tr><tr><td>冬季</td><td>0.0032</td><td>0.0297</td><td>-0.0071</td><td>0.0566</td><td>0.2630</td></tr><tr><td rowspan="4">DSN</td><td>春季</td><td>0.0048</td><td>0.0497</td><td>-0.0086</td><td>0.0693</td><td>0.2567</td></tr><tr><td>夏季</td><td>0.0047</td><td>0.0412</td><td>-0.0102</td><td>0.0686</td><td>0.2669</td></tr><tr><td>秋季</td><td>0.0062</td><td>0.0496</td><td>-0.0091</td><td>0.0787</td><td>0.4228</td></tr><tr><td>冬季</td><td>0.0045</td><td>0.0487</td><td>-0.0098</td><td>0.0671</td><td>0.3119</td></tr><tr><td rowspan="4">CNN-LSTM</td><td>春季</td><td>0.0067</td><td>0.0552</td><td>-0.0073</td><td>0.0819</td><td>0.3033</td></tr><tr><td>夏季</td><td>0.0058</td><td>0.0609</td><td>-0.0088</td><td>0.0762</td><td>0.2965</td></tr><tr><td>秋季</td><td>0.0086</td><td>0.0735</td><td>-0.0103</td><td>0.0927</td><td>0.4979</td></tr><tr><td>冬季</td><td>0.0062</td><td>0.0611</td><td>-0.0075</td><td>0.0787</td><td>0.3661</td></tr><tr><td rowspan="4">GRU-CNN</td><td>春季</td><td>0.0124</td><td>0.0921</td><td>-0.0058</td><td>0.1114</td><td>0.4127</td></tr><tr><td>夏季</td><td>0.0137</td><td>0.0769</td><td>-0.0065</td><td>0.1170</td><td>0.4556</td></tr><tr><td>秋季</td><td>0.0154</td><td>0.0947</td><td>-0.0098</td><td>0.1241</td><td>0.6663</td></tr><tr><td>冬季</td><td>0.0118</td><td>0.0936</td><td>-0.0067</td><td>0.1086</td><td>0.5051</td></tr><tr><td rowspan="4">Crossformer</td><td>春季</td><td>0.0151</td><td>0.1030</td><td>-0.0074</td><td>0.1229</td><td>0.4554</td></tr><tr><td>夏季</td><td>0.0158</td><td>0.0982</td><td>-0.0055</td><td>0.1257</td><td>0.4893</td></tr><tr><td>秋季</td><td>0.0173</td><td>0.1064</td><td>-0.0114</td><td>0.1315</td><td>0.7062</td></tr><tr><td>冬季</td><td>0.0160</td><td>0.0969</td><td>-0.0085</td><td>0.1265</td><td>0.5881</td></tr><tr><td rowspan="4">Informer</td><td>春季</td><td>0.0089</td><td>0.0623</td><td>-0.0082</td><td>0.0943</td><td>0.3496</td></tr><tr><td>夏季</td><td>0.0092</td><td>0.0716</td><td>-0.0083</td><td>0.0959</td><td>0.3734</td></tr><tr><td>秋季</td><td>0.0107</td><td>0.0813</td><td>-0.0127</td><td>0.1034</td><td>0.5554</td></tr><tr><td>冬季</td><td>0.0085</td><td>0.0722</td><td>-0.0079</td><td>0.0922</td><td>0.4287</td></tr></table>

![](images/bcc9ef188ae26f664a4536a922d9f9aae130f324177315b0a80d56b6cd179ee4.jpg)


![](images/79b46ed00152d9d98164b26be39a9ed86206bbc34c58d1fea551c38e492d6439.jpg)


![](images/1d92ce948aadc6bde89386c8ba758f671366f4be6d3d6f1eb9002f9026efabe5.jpg)


![](images/cca7686a8d04168e6531fd62071a2596a7f337103398e811d2f635c92632ebf8.jpg)



图5-9模型在各个季节的表现


![](images/8ca9cdd8fca3fc55d1a8c337b75074f4a77e67ae4513751dbb7cf1d464b6c3a4.jpg)


![](images/d2defd08c17bab8bd46033d07e582ac35d62188959329dc67acb08846f26a3da.jpg)


![](images/480fbfdba4c00920d97305eab7fee76b46d94e8d2153b88751888e5a01a79fe2.jpg)


![](images/f2162feaa8e130ecda987b94ec60ce3789f509845139f9a69f950119c85a041f.jpg)



图5-10DKASC-DG数据集各季节预测结果


# 5.3.6 鲁棒性分析

在本节中，我们使用皮尔逊相关系数法（Pearson Correlation Coefficient）对特征进行选择，来评估MSRWKV-2DTCN模型的鲁棒性。实验目标是评估当与目标变量相关性较弱的特征被排除时，模型的性能是否受到影响，从而验证模型的鲁棒性。

特征选择是机器学习中常见的预处理步骤，通过消除冗余或相关性较弱的特征来提高模型性能，降低模型的复杂性，进一步提高预测精度，并防止过拟合[161,162]。根据Liu等人[161]在光伏功率预测中采用的方法，我们应用了皮尔逊相关系数来排除对光伏发电输出影响较小的特征。

每个特征与目标变量“有功功率（AP）”的皮尔逊相关系数如图5-11所示。根据这两组数据集的相关矩阵热图和该领域之前的研究[163,164,165]，设定的相关系数阈值为0.2。绝对相关值低于0.2的特征被认为与目标变量之间的线性关系可以忽略不计，被排除在进一步分析之外。因此，用于模型训练的数据中排除了天气日降雨量（WDR）、风向（WD）和气压（APre）这三个特征。去除这些冗余特征有助于减少训练过程中的潜在噪声。

![](images/eb8a55e0d6069122a10fd94e6ad9c308c1db222c6cffb62ba9746ef9361ab104.jpg)



图5-11两个数据集的相关矩阵热力图


在特征选择过程之后，我们进行了实验，比较MSRWKV-2DTCN使用完整特征数据集和简化特征数据集（去除低相关性特征）的性能。在DKASC-DG和DKASC-CA数据集上的预测结果如表5-7所示。实验结果表明，使用简化特征数据集时，MSE和MAE指标表现出微小变化。DKASC-DG在使用简化特征集时，MSE从0.0043降低至0.0042，

MAE从0.0277降低至0.0275。同样地，在DKASC-CA数据集中，MSE从0.0047降低至0.0046，而MAE保持在0.0252。误差的这种轻微减少表明，即使加入了不相关的噪声数据，MSRWKV-2DTCN仍然能够保持高性能，这验证了该模型具有很强的鲁棒性。


表 5-7 完整特征数据集与简化特征数据集的实验结果


<table><tr><td>数据集</td><td>方法</td><td>MSE</td><td>MAE</td><td>MBE</td><td>RMSE</td><td>NRMSE</td></tr><tr><td rowspan="2">DKASC-DG</td><td>完整特征</td><td>0.0043</td><td>0.0277</td><td>-0.0021</td><td>0.0656</td><td>0.2806</td></tr><tr><td>简化特征</td><td>0.0042</td><td>0.0275</td><td>-0.0021</td><td>0.0648</td><td>0.2773</td></tr><tr><td rowspan="2">DKASC-CA</td><td>完整特征</td><td>0.0047</td><td>0.0252</td><td>-0.0022</td><td>0.0686</td><td>0.2720</td></tr><tr><td>简化特征</td><td>0.0046</td><td>0.0252</td><td>-0.0021</td><td>0.0678</td><td>0.2691</td></tr></table>

# 5.3.7 在其他光伏发电数据集的性能评估

在前面的实验中，我们使用从DKASC的尤拉拉光伏系统公开数据集进行了实验。为进一步验证MSRWKV-2DTCN的有效性，我们还对另外两个来自DKASC的公开数据集进行实验，这些数据集也被  $\mathrm{DSN}^{[147]}$  和其它光伏发电研究广泛使用。实验结果如表5-8所示。其中对比模型的结果来自于DSN的实验研究。符号“-”表示原始参考文献中没有提供这些特定的评价指标。MSRWKV-2DTCN在对比方法中取得了最优的指标值。


表 5-8 在其他 DKASC 数据集上的实验结果


<table><tr><td>数据集</td><td>模型</td><td>MSE</td><td>MBE</td><td>RMSE</td><td>MAE</td></tr><tr><td rowspan="8">DKASC-ASA-1A</td><td>CNN-LSTM[153]</td><td>-</td><td>-</td><td>0.3430</td><td>0.1260</td></tr><tr><td>CNN[166]</td><td>-</td><td>-</td><td>0.3090</td><td>0.1750</td></tr><tr><td>GRU-CNN[152]</td><td>0.0216</td><td>0.0171</td><td>0.1468</td><td>0.0742</td></tr><tr><td>LSTM-CNN[151]</td><td>-</td><td>-</td><td>0.6210</td><td>0.2210</td></tr><tr><td>ESNBCNN[146]</td><td>0.0259</td><td>-0.0017</td><td>0.1610</td><td>0.0845</td></tr><tr><td>CNNGRU[146]</td><td>0.0771</td><td>0.0172</td><td>0.2776</td><td>0.1531</td></tr><tr><td>DSN[147]</td><td>0.0052</td><td>0.0019</td><td>0.0721</td><td>0.0383</td></tr><tr><td>MSRWKV-2DTCN</td><td>0.0041</td><td>-0.0014</td><td>0.0640</td><td>0.0320</td></tr><tr><td rowspan="7">DKASC-ASA-1B</td><td>GRU-CNN[152]</td><td>0.0298</td><td>0.0235</td><td>0.1727</td><td>0.0923</td></tr><tr><td>ESNBCNN[146]</td><td>0.0463</td><td>0.0029</td><td>0.2153</td><td>0.1466</td></tr><tr><td>CNNGRU[146]</td><td>0.1605</td><td>-0.0300</td><td>0.4006</td><td>0.3247</td></tr><tr><td>LSTMCNN[146]</td><td>0.1249</td><td>0.0948</td><td>0.3535</td><td>0.2116</td></tr><tr><td>CNNLSTM[146]</td><td>0.1573</td><td>0.0113</td><td>0.3967</td><td>0.2408</td></tr><tr><td>DSN[147]</td><td>0.0086</td><td>-0.0013</td><td>0.0928</td><td>0.0550</td></tr><tr><td>MSRWKV-2DTCN</td><td>0.0063</td><td>-0.0009</td><td>0.0794</td><td>0.0439</td></tr></table>

此外，为进一步评估MSRWKV-2DTCN的性能，我们在表5-9中提供一个综合对比。这些综合对比模型使用了来自DKASC的光伏功率数据集，包括与智能优化算法结合的SVM模型[167]、基于极限学习机（Extreme Learning Machine, ELM）的模型[168]、 $\mathrm{GCN}^{[170]}$ 、基于CNN[169,171]的模型以及DSN[147]。这些对比方法的信息和实验结果如表5-9所示。相

比之下，MSRWKV-2DTCN在降低误差率方面表现出色，验证了其在光伏发电功率预测领域的有效性。


表 5-9 MSRWKV-2DTCN 与其他先进模型的比较


<table><tr><td>年份</td><td>作者</td><td>模型</td><td>时间间隔</td><td>功率</td><td>MAE</td><td>MSE</td><td>RMSE</td></tr><tr><td rowspan="2">2019</td><td rowspan="2">Li等人 [167]</td><td>HIMVO-SVM</td><td rowspan="2">30分钟</td><td rowspan="2">-</td><td>0.2805</td><td>-</td><td>-</td></tr><tr><td>MOV-LSTM</td><td>0.3705</td><td>-</td><td>-</td></tr><tr><td rowspan="2">2020</td><td rowspan="2">Zhou等 人[168]</td><td>SDA-GA-ELM</td><td rowspan="2">5分钟</td><td rowspan="2">4.95 KW</td><td>0.2367</td><td>-</td><td>0.2107</td></tr><tr><td>SDA-ELM</td><td>0.2717</td><td>-</td><td>0.2478</td></tr><tr><td rowspan="2">2020</td><td rowspan="2">Zang等 人[169]</td><td>DenseNet</td><td rowspan="2">1小时</td><td rowspan="2">5.00 KW</td><td>0.1520</td><td>0.0810</td><td>-</td></tr><tr><td>ResNet</td><td>0.1800</td><td>0.1280</td><td>-</td></tr><tr><td>2021</td><td>Cheng等 人[170]</td><td>GCN</td><td>5分钟</td><td>-</td><td>0.1770</td><td>-</td><td>0.3360</td></tr><tr><td>2022</td><td>Khan等 人[171]</td><td>CNNESN</td><td>5分钟</td><td>-</td><td>0.0449</td><td>0.0095</td><td>0.2604</td></tr><tr><td>2023</td><td>Khan等 人[147]</td><td>DSN</td><td>5分钟</td><td>60.4 KW</td><td>0.0394 (avg)</td><td>0.0051 (avg)</td><td>0.0678 (avg)</td></tr><tr><td>2024</td><td>本章</td><td>MSRWKV-2DTCN</td><td>5分钟</td><td>1164.3 KW</td><td>0.0265 (avg)</td><td>0.0045 (avg)</td><td>0.0671 (avg)</td></tr></table>

# 5.4 本章小结

在本章研究中，提出了MSRWKV-2DTCN以提高短期光伏发电功率预测的准确性。MSRWKV-2DTCN结合多尺度时间混合模块与多尺度2D TCN模块，有效地捕捉历史数据中的周期性和特征间的相互依赖关系。MSRWKV-2DTCN通过FFT识别关键周期性模式，扩展了时间混合模块的感受野，弥补原始RWKV模型的不足，并利用了TCN在序列建模中的优势，学习历史数据中的时空依赖关系。这种组合增强了模型学习复杂依赖关系的能力，有效提高了光伏发电功率预测的准确性，从而有助于电网的高效规划和调度。实验结果一致表明，MSRWKV-2DTCN在短期光伏功率预测准确性方面优于现有的最先进基线模型（DSN)，使其成为短期光伏发电功率预测的一个有效工具。

# 第六章 基于分解聚合注意力的Transformer预测模型

光伏发电作为可再生能源，在能源系统中扮演着重要角色。准确预测光伏发电功率对于调节供需平衡、确保电网稳定至关重要。尽管现有研究已经取得显著进展，但在长期光伏功率预测中，历史数据中的长期趋势和波动尚未得到充分挖掘，而这些特征对提高长期预测的准确性至关重要。因此，本章提出了一种基于分解聚合注意力的Transformer（Decomposition Aggregation Attention Transformer, DAformer）预测模型，来进一步提升长期光伏功率预测的准确性。DAformer通过分解聚合注意力机制，有效捕捉时间序列中的季节性和趋势特征。此外，我们引入了残差块和残差连接，减少输入数据的规模，降低注意力机制的计算复杂度，同时保留原始特征。本章在来自澳大利亚爱丽丝泉光伏系统的数据集上进行了实验，实验结果表明，DAformer在长期光伏发电功率预测方面优于现有的基于Transformer的多元时间序列预测方法。

# 6.1 引言

尽管第五章中提出的MSRWKV-2DTCN模型在短期光伏功率预测中取得了良好的效果，但长期光伏功率预测对于电力系统的长期规划也具有重要作用。电网调度不仅需要应对短期的供需波动，还必须准确预测长期光伏发电量，以确保可再生能源的稳定供应和电网的持续平衡。因此，长期光伏功率预测在保障电网安全、稳定运行中具有重要作用[172]。并且准确的长期预测有助于电力公司优化能源储备和分配，规划电网基础设施升级，并制定清洁能源政策[173]。同时，长期预测还能够提升能源调度效率，减少电力浪费，尤其在可再生能源比例不断上升的背景下，对减少化石能源依赖至关重要[174]。

由于光伏功率数据是序列数据，具有时间特征，许多基于 RNN 及其变体的模型被用于长期光伏功率预测。Jung 等人[175]提出了基于 RNN 的模型，探讨长期光伏功率输出与气象信息之间的依赖关系。Han 等人[176]针对中长期风电和光伏发电预测问题，提出一种基于 Copula 函数和 LSTM 的中长期风电和光伏发电预测方法，该方法能够有效提取影响发电功率的关键气象特征，并从有限的可用数据样本中深入挖掘其长期依赖关系和趋势。

Sharma 等人[177]提出了一种基于 Nadam 优化器的 LSTM 的长期光伏发电功率预测方法。Ray 等人[178]设计了一种基于混合深度学习的长期光伏功率预测方法，结合 LSTM 和 CNN，达到准确度与效率之间的均衡。Liu 等人[179]开发了 KM-PSO-SVR 发电预测模型，该模型能够有效模拟光伏电站区域关键气象因子的变化，提高长期光伏发电功率预测的准确性。Ofori-Ntow 等人[180]提出了一种新的光伏长期发电预测的叠加泛化方法解决长期光伏功率预测问题，该方法中使用基础学习器包括数据处理分组方法(Group Method of Data Handling, GMDH)、径向基函数神经网络(Radial Basis Function Neural Network, RBFNN)、情感神经网络(Emotional Neural Network, ENN)和最小二乘支持向量机(Least Squares support Vector Machine, LSSVM)。近年来，Transformer 在许多领域（如 NLP）中超越了基于 RNN 和 CNN 的方法。与 RNN 模型相比，基于 Transformer 的模型能够捕捉长期复杂的非线性关系，并且能够避免误差累积。因此，许多工作采用基于 Transformer 的模型来解决光伏发电功率预测任务，并取得巨大成功。Kang 等人[181]提出一种视觉 Transformer 方法来预测光伏功率。Phan 等人[182]和 Tao 等人[183]将标准 Transformer 架构应用于光伏功率预测。Liao 等人[184]和 Kim 等人[185]将 Transform 与 LSTM 相结合，进一步提高了光伏功率预测精度。

然而，光伏发电功率数据通常表现出较强的长期依赖性，具有复杂的季节性和趋势特征，这给长期光伏功率预测任务带来了挑战。季节性特征反映了气候变化对光伏发电的周期性影响，而趋势特征则揭示光伏系统输出的长期变化趋势。通过学习数据中的季节性和趋势特征，模型能够更准确地捕捉光伏发电功率的长期波动模式，从而提升长期预测性能。然而，现有的长期光伏功率预测研究大多忽视对这些特征的深入挖掘。为此，本章基于Transformer架构，结合多层感知机（MLP）和基于Loess的季节-趋势分解（STL），提出了用于长期光伏功率预测的DAformer模型。本章的主要贡献如下：

（1）当前长期光伏发电功率预测方法在有效捕捉历史数据的长期趋势和波动性方面存在不足。因此本章提出了DAformer模型，该模型结合分解聚合注意力机制、残差块和残差连接，能够有效捕捉光伏发电功率数据中的长期趋势和季节性特征，提高长期光伏功率预测的准确性。

(2) 设计了基于 MLP 的残差块和残差连接以优化模型性能。通过引入残差块和残

差连接，DAformer不仅减少了输入数据的维度，提高注意力机制的计算效率，同时保留关键的原始特征。

（3）融合STL方法和注意力机制，设计了分解聚合注意力机制，学习历史数据中的长期趋势和波动。

本章其余部分的结构如下：在6.2节中详细介绍DAformer框架；在6.3节中介绍实验数据集、实验设置，然后分析实验结果；在6.4节中总结本章的主要工作。

# 6.2 基于分解聚合注意力的Transformer模型

在本节中，我们首先给出光伏功率预测问题的定义。随后详细介绍DAformer框架的设计与实现（如图6-1所示），包括分解聚合注意力机制、残差块和残差连接。该模型旨在通过捕捉光伏功率数据中的长期趋势和季节性特征，提升长期光伏功率预测的准确性。

DAformer 编码器的过程示例如图 6-2 所示。该编码器部分由三个多头 ProbSparse 分解聚合注意力层和两个残差块组成。编码器首先通过多头 ProbSparse 分解聚合注意力机制捕捉输入数据中的季节性、趋势以及时空依赖关系，然后通过残差块进行信息蒸馏，减少下一层的输入规模，提高注意力机制的计算效率。残差连接则确保模型在层与层之间传递信息时保留原始数据的特征表达。

![](images/cf57f370ebe3ddef58b11f91b8bfd6ed936b8d3d2c21213976c0e91c0119bb6a.jpg)



图6-1DAformer模型架构


![](images/e5260872e32bece398766a07378a505cc9c93630d1b4bf4baef801abd4a7556a.jpg)



图6-2DAformer模型编码器的过程示例


# 6.2.1 问题定义

光伏功率预测任务是通过利用外部影响因素和光伏发电功率的历史数据来预测未来的光伏发电功率。给定一段历史数据，表示为  $X = \{x_{1}^{t}, x_{2}^{t}, \dots, x_{c}^{t}, x_{ot}^{t}\}_{t=1}^{L}$ ，其中  $c (c \geq 1)$  是外部变量的数量， $x_{i}^{t}$  表示在第  $t$  个时间点第  $i$  个变量的值， $x_{ot}^{t}$  表示在第  $t$  个时间点光伏发电功率， $L$  是固定的回溯窗口。光伏功率预测的任务是预测  $\{\hat{x}_{ot}^{t}\}_{t=L+1}^{L+H}$ ，其中  $\hat{x}_{ot}^{t}$  表示在未来时间步  $t$  的预测光伏发电功率， $H (H \geq 1)$  表示未来时间步的长度，预测区间为  $L+1$  到  $L+H$ ，光伏功率的真实值表示为  $\{x_{ot}^{t}\}_{t=L+1}^{L+H}$ 。

![](images/7ddb0b22a1c0b857949652f758692b77cd8f3ef9beaf3c5ce192ae61349ff161.jpg)



图6-3分解聚合注意力机制


# 6.2.2 分解聚合注意力机制

如图6-3所示，我们提出了一种分解聚合注意力机制，以增强模型捕捉光伏发电功率数据中季节性和趋势的能力。具体细节如下：

为学习光伏发电功率数据中的长期时间模式，例如长期趋势和季节性，我们利用STL分解方法将原始时间序列分解为不同的子序列，包括趋势（Trend）、季节性（Seasonal）和残差（Residual）子序列。这些子序列，特别是季节性和趋势子序列，能够有效地表达原始数据中的潜在季节性和趋势特征。值得注意的是，即使经过线性变换和自注意力机制过程，时间序列数据仍然能够保留其内在的时间模式，包括季节性和趋势特征。对于长度为  $L$  的输入序列，表示为  $X = \{x_{1}^{t}, x_{2}^{t}, \dots, x_{c}^{t}, x_{ot}^{t}\}_{t=1}^{L}$ ，其中  $x_{ot}^{t}$  是预测目标。目标时间序列  $\{x_{ot}^{t}\}_{t=1}^{L}$  通过STL过程分解为三个子序列：趋势（T）、季节性（S）和残差（R）。通过自注意力计算，模型能够捕捉这些子序列内及其他影响因素之间的时空依赖关系。这一分解操作使得模型能够有效学习时间序列数据中的季节性和趋势。STL分解过程和注意力机制计算过程说明如下：

$$
\left(\left\{T _ {o t} ^ {t} \right\} _ {t = 1} ^ {L}, \left\{S _ {o t} ^ {t} \right\} _ {t = 1} ^ {L}, \left\{R _ {o t} ^ {t} \right\} _ {t = 1} ^ {L}\right) = S T L D e c o m p \left(\left\{x _ {o t} ^ {t} \right\} _ {t = 1} ^ {L}\right) \tag {6-1}
$$

$$
\tilde {X} = \left\{x _ {1} ^ {t}, x _ {2} ^ {t}, \dots , x _ {c} ^ {t}, T _ {o t} ^ {t}, S _ {o t} ^ {t}, R _ {o t} ^ {t} \right\} _ {t = 1} ^ {L} \tag {6-2}
$$

$$
Q, K = W _ {Q} \tilde {X}, W _ {K} \tilde {X} \tag {6-3}
$$

$$
V = W _ {V} X \tag {6-4}
$$

$$
\text {A t t e n t i o n} _ {-} \operatorname {s c o r e} (Q, K) = \operatorname {S o f t M a x} \left(\frac {\bar {Q} K ^ {T}}{\sqrt {d _ {k}}}\right) \tag {6-5}
$$

$$
A _ {o t} = \operatorname {S o f t M a x} \left(A _ {T} + A _ {S} + A _ {R}\right) \tag {6-6}
$$

$$
\operatorname {A t t e n t i o n} (Q, K, V) = \operatorname {A t t e n t i o n} _ {-} \operatorname {s c o r e} ^ {\prime} (Q, K) V \tag {6-7}
$$

$T_{ot}^{t}$ ,  $S_{ot}^{t}$  和  $R_{ot}^{t}$  分别表示趋势、季节性和残差子序列。在  $\tilde{X}$  中，目标时间序列  $x_{ot}^{t}$  被分解为三个子序列： $T_{ot}^{t}$ ,  $S_{ot}^{t}$  和  $R_{ot}^{t}$  。 $W_{Q}$ ,  $W_{K}$  和  $W_{V}$  为权重矩阵。需要注意的是， $Q$  和  $K$  通过对  $\tilde{X}$  进行线性变换生成，而  $V$  则由  $X$  进行线性变换生成。 $A_{T}$ ,  $A_{S}$  和  $A_{R}$  分别表示趋势、季节性和残差子序列的注意力权重。目标序列的注意力权重的计算如公式（6-6）所示，

该公式通过将趋势  $\left(T_{ot}^{t}\right)$  、季节性  $\left(S_{ot}^{t}\right)$  和残差  $\left(R_{ot}^{t}\right)$  子序列的注意力权重  $\left(A_{T}, A_{S}\right.$  和  $\left.A_{R}\right)$  相结合来计算目标的注意力权重。然后采用目标列注意力权重  $A_{ot}$  替代Attention_score  $(Q, K)$  中的三个子序列的注意力权重  $\left(A_{T}, A_{S}\right.$  和  $\left.A_{R}\right)$ ，形成新的注意力权重 Attention_score'  $(Q, K)$  并用于后续更新。

另外，原始的自注意力机制[10]具有二次时间复杂度，并且需要  $O(L^2)$  的内存使用量。由于自注意力机制的概率分布存在潜在的稀疏性， $\mathrm{Informer}^{[100]}$  提出一种高效的 ProbSparse 机制来降低复杂度。关于 ProbSparse 机制的更多细节可以参考在 Informer[100]，ProbSparse 机制可以将注意力机制计算的复杂度降低到  $O(L \ln L)$  。因此，我们将分解聚合注意力与 ProbSparse 机制结合，以进一步提高计算效率。

# 6.2.3 残差块和残差连接

![](images/ac6fa8aca086073f2de54c0d1cf1fcf39a4e90e69033b7aa0e648ded44e816d8.jpg)



图6-4 残差块的架构


Zhou 等人[100]的研究表明，Transformer 架构的编码器特征图具有冗余的  $V$ ，为了减少模型中的冗余信息，我们引入了基于多层感知机的残差块来实现蒸馏操作，其在时间序列预测中的有效性已经在 TiDE[62]中得到验证。残差块的架构如图 6-4 所示，这种蒸馏操作不仅可以减少下一层的输入大小，而且在蒸馏过程中，能够保留有价值的信息。此外，我们在每个残差块之前引入残差连接，目的是进一步保留在蒸馏操作中可能被忽略的重要特征。最后一层自注意力层的输出与所有残差连接的输出进行拼接，并通过线性层重新整合。该过程的公式表示如下：

$$
X _ {\text {e n c o d e r}} = F C \left(\operatorname {C o n c a t} \left(X _ {\text {l a s t}}, X _ {R 1}, X _ {R 2}, \dots , X _ {R N}\right)\right) \tag {6-8}
$$

其中， $X_{R1}, X_{R2}, \dots, X_{RN}$  表示残差连接的输出， $X_{encoder}$  表示编码器的输出， $X_{last}$  是最后一个自注意力层的输出。

# 6.3 实验

# 6.3.1 实验数据集

在本章研究中，我们使用来自 DKASC[158]爱丽斯泉（Alice Springs）光伏发电系统的数据。爱丽斯泉的光伏发电系统由 38 个小型光伏系统组成，总容量为 263.0 千瓦。整个发电系统的总有功功率由两个主计量表来计量，分别为 96_DKA_MasterMeter1 和 62_DKA_MasterMeter2。我们将这两个主计量表的数据收集为两个数据集：DKAM1 和 DKAM2。由于某些年份的数据存在较为严重的缺失，会影响模型的训练效果，经过详细的数据筛选和比较后，我们最终选取自 2016 年 1 月 1 日 00:00:00 到 2018 年 12 月 31 日 23:55:00 的数据进行实验。数据的时间间隔为 5 分钟，每天包含 288 个时间点，每个数据集包含 315648 个时间点。

每个数据集由16个时间序列组成，包括时间戳（Time Stamp）、相对地线电压平均值（Average Voltage Line to Neutral, AVLN）、有功功率（Active Power, AP）、水平扩散辐射（Diffuse Horizontal Radiation, DHR）、接收的有效能量（Active Energy Delivered Received, AEDR）、倾斜全球辐射（Radiation Global Tilted, RGT）、功率因数签名（Power Factor Signed, PFS）、倾斜扩散辐射（Radiation Diffuse Tilted, RDT）、电流相位平均值（Current Phase Average, CPA）、频率（Frequency, F）、THD电压平均值（THD Voltage Average, TVA）、天气温度（摄氏度）（Weather Temperature in Celsius, WTC）、日降水量（Daily Rainfall, DR）、全球水平辐射（Global Horizontal Radiation, GHR）、天气相对湿度（Weather Relative Humidity, WRH）和风向（Wind Direction, WD）。由于在2016年10月21日13:10:00之后不再记录风速数据，因此我们将风速排除在两个数据集之外。

两个数据集中15个特征（除时间戳外）的缺失值数量如表6-1所示。由于数据中存在大量连续的缺失值，如果直接采用线性插值法，会导致数据与真实值之间的偏差较大。因此，本章采用5.3.1节中介绍的填充方法进行填充缺失值。


表 6-1 每个特征缺失值的数量


<table><tr><td>特征</td><td>DKAM1</td><td>DKAM2</td></tr><tr><td>相对地线电压平均值(AVLN)</td><td>607</td><td>11325</td></tr><tr><td>有功功率(AP)</td><td>607</td><td>11327</td></tr><tr><td>水平扩散辐射(DHR)</td><td>607</td><td>607</td></tr><tr><td>送出/接收活跃能量(AEDR)</td><td>607</td><td>11327</td></tr><tr><td>倾斜面全球辐射(RGT)</td><td>12704</td><td>12704</td></tr><tr><td>功率因数签名(PFS)</td><td>607</td><td>11327</td></tr><tr><td>倾斜面扩散辐射(RDT)</td><td>12704</td><td>12704</td></tr><tr><td>电流相位平均值(CPA)</td><td>607</td><td>11325</td></tr><tr><td>频率(F)</td><td>607</td><td>11325</td></tr><tr><td>THD电压平均值(TVA)</td><td>607</td><td>11325</td></tr><tr><td>气温（摄氏度）(WTC)</td><td>607</td><td>607</td></tr><tr><td>日降水量(DR)</td><td>607</td><td>607</td></tr><tr><td>全球水平辐射(GHR)</td><td>607</td><td>607</td></tr><tr><td>天气相对湿度(WRH)</td><td>607</td><td>607</td></tr><tr><td>风向(WD)</td><td>607</td><td>607</td></tr></table>

数据集 DKAM1 和 DKAM2 的有功功率波动趋势如图 6-5 所示。其中，x 轴表示每天的序列号，y 轴表示一天中的 288 个时间点，z 轴表示光伏系统的有功功率。从图中可以观察到，有功功率呈现出复杂的季节性波动和趋势变化。因此，深入挖掘有功功率数据中的季节性和趋势特征对于提高长期光伏功率预测准确性至关重要。

![](images/b29af5706262b547490105c98bd374783db12d90f44a64403304bfefa53f943e.jpg)



(a) DKAM1


![](images/a43f709daa9561daa3ae80338949519980ab1e9dbd1a1e494389aca38bae213c.jpg)



(b) DKAM2



图6-5 DKAM1和DKAM2数据集的有功功率波动图


除时间特征外，DKAM1和DKAM2数据集均包含15个特征维度。然而，并非所有特征都对有功功率有较大影响。为分析各特征与有功功率之间的关系，我们采用皮尔逊相关系数方法[161]。两个数据集的相关矩阵热力图如图6-6和图6-7所示。根据5.3.6节的分析，我们将相关系数阈值设定为0.2，以排除对有功功率影响较小的特征。从图中可以

明显看出，接收的有效能量（AEDR）、频率（F）、风向（WD）、日降水量（DR）和THD电压平均值（TVA）与有功功率之间的相关性较低。因此，在本章研究中，我们未将这五个特征用于光伏功率预测模型的训练，以减少冗余特征对模型性能的干扰。

![](images/f8c2770037edd81049d285593fa809f5fe41edb0fa2a9f3e29ceaf7b7f97059d.jpg)


![](images/bf96f9e2460728b33fdb4c6c059191f817df67af78cc3ddea198da55a6a92744.jpg)



图6-6数据集DKAM1的特征相关性分析


![](images/fa73efc2a56385647ff5a96a9313c372d68f4c7f71b649ed23325bd7b53cfad8.jpg)



图6-7数据集DKAM2的特征相关性分析


# 6.3.2 实验设置

为评估所提出的DAformer模型的性能，我们选择四种在多元时间序列预测中具有代表性和有效性的模型进行比较。具体基线模型如下：

(1) Informer[100]: 该模型提出 ProbSparse 注意力机制, 通过稀疏化注意力矩阵来减

少计算复杂度，从而提升了模型的计算效率，尤其适用于长时间序列预测。

（2）Crossformer[107]：该模型引入交叉域注意力机制（Cross-domain Attention），能够在局部和全局两个尺度上捕捉多尺度时序依赖关系，适合处理复杂的多尺度时间序列数据。

（3）Autoformer[101]：该模型提出自回归分解结构（Decomposition Structure）和自动相关机制（Auto-correlation Mechanism），能够自适应地分解时间序列中的趋势和季节性成分，从而提升长期预测性能。

（4）FEDformer[102]：该模型提出了频域增强解码器（Frequency Enhanced Decomposer），通过在频域上提取序列特征，减少冗余信息，提升长期时间序列预测的准确性和效率。

![](images/d3c273c3daddf92228d001ad2b1db7d675cd161bec670ec5f0b1248aab8b9b93.jpg)


![](images/8404ea4aa5f143af80aae125725d54fe5834c6f7ebab30e069473a676fe18b47.jpg)


![](images/e3ec2fbfaa182652605516ab51f7737d79c008f5496dc86862169888ae868f12.jpg)



图6-8DAformer模型的超参数分析


模型的性能通过均方误差（MSE）、平均绝对误差（MAE）、均方根误差（RMSE）进行评估。为评估模型在不同预测长度下的性能，我们设置八种预测长度：6（30分钟）、12（1小时）、72（6小时）、144（12小时）、288（1天）、576（2天）、864（3天）、1152（4天）。训练集、验证集和测试集按照0.7:0.1:0.2的比例划分，并进行零均值归一化。我们在验证集上调整超参数，结果如图6-8所示。根据图6-8的结果，模型的编码器设置为2层，解码器设置为1层，隐藏层的维度设置为512。根据对比方法的建议，回溯窗口、批量大小和注意力头的数量分别设置为96、32和8。学习率初始值设置为  $10^{-4}$ ，优化器为ADAM，损失函数为L2。训练的轮次为10，所有实验使用PyTorch深度学习框架实现，并在配备两块NVIDIA GeForce RTX 2080 Ti GPU工作站上执行五次实验。

# 6.3.3 实验结果分析

DAformer和四种对比方法在两个数据集（DKAM1和DKAM2）上的光伏功率预测结果如表6-2所示。以下是对结果的详细分析：

（1）DAformer在各个预测长度下的表现均优于其他对比模型，无论是在MSE、MAE还是RMSE指标上，均呈现出最小的误差。这表明DAformer在捕捉光伏功率数据的季节性和趋势方面表现突出，具有较强的预测精度和稳健性。（2）Crossformer是四个对比方法中表现最好的，但与DAformer相比，仍有一定差距。特别是在较长的预测长度（如576、864、1152时间步）下，DAformer的MSE和RMSE明显低于Crossformer。例如，在DKAM1上，预测长度为1152时，DAformer的MSE为0.179，而Crossformer为0.207，误差减少了  $13.5\%$  ；在DKAM2上，DAformer的MSE为0.182，而Crossformer为0.205，减少了  $11.2\%$  。（3）Informer和FEDformer的表现次于Crossformer，尤其是在预测长度较长时，误差显著增大。例如，在DKAM1的1152步预测下，Informer的MSE达到0.447，FEDformer的MSE为0.531，均远高于DAformer和Crossformer。此外，FEDformer在一些较长的预测任务中（如864、1152步）表现出误差的不稳定性。（4）Autoformer在光伏功率预测任务中的表现不佳，尤其是在预测长度较长时，表现出明显的误差波动。在DKAM1的864步预测下，Autoformer的MSE达到1.870，为所有模型中最高，且在1152步时虽有所下降，但仍高达1.470。这表明Autoformer在处理光伏功率数据时存在稳健性问题，可能难以有效捕捉数据的复杂特征。

从整体结果来看，DAformer在所有预测长度上均优于其他模型，特别是在长时间序列预测任务中，误差降低更加明显，表明其在捕捉光伏数据长期的季节性和趋势特征方面具备优秀的能力。相比之下，Crossformer在四种对比模型中表现相对较好，但仍落后于DAformer；而Informer、FEDformer和Autoformer在光伏功率预测任务中表现出一定的局限性，尤其是在长时间预测任务中，误差显著增大，预测结果的不稳定性更加突出。


表 6-2 两个数据集的光伏功率预测结果


<table><tr><td>模型</td><td>DAformer</td><td colspan="2">Crossformer</td><td colspan="2">Informer</td><td colspan="2">FEDformer</td><td colspan="2">Autoformer</td></tr><tr><td>评价指标</td><td>MSE MAERMSE</td><td>MSE MAERMSE</td><td>MSE MAERMSE</td><td>MSE MAERMSE</td><td>MSE MAERMSE</td><td>MSE MAERMSE</td><td>MSE MAERMSE</td><td>MSE MAERMSE</td><td>MSE MAERMSE</td></tr><tr><td rowspan="16">DKAM1</td><td>6 0.031 0.082 0.176</td><td>0.030 0.091</td><td>0.172</td><td>0.046 0.116</td><td>0.214</td><td>0.098 0.204</td><td>0.314</td><td>0.487 0.558</td><td>0.698</td></tr><tr><td>12 0.033 0.086 0.182</td><td>0.038 0.097</td><td>0.195</td><td>0.055 0.130</td><td>0.234</td><td>0.162 0.286</td><td>0.402</td><td>0.595 0.580</td><td>0.771</td></tr><tr><td>72 0.089 0.151 0.298</td><td>0.097 0.237</td><td>0.311</td><td>0.093 0.168</td><td>0.305</td><td>0.268 0.357</td><td>0.518</td><td>0.857 0.745</td><td>0.926</td></tr><tr><td>144 0.127 0.203 0.356</td><td>0.133 0.246</td><td>0.365</td><td>0.121 0.191</td><td>0.348</td><td>0.199 0.330</td><td>0.446</td><td>1.254 0.884</td><td>1.120</td></tr><tr><td>288 0.143 0.210 0.378</td><td>0.156 0.239</td><td>0.395</td><td>0.150 0.219</td><td>0.387</td><td>0.223 0.360</td><td>0.472</td><td>1.331 0.900</td><td>1.154</td></tr><tr><td>576 0.139 0.216 0.373</td><td>0.164 0.241</td><td>0.405</td><td>0.227 0.287</td><td>0.476</td><td>0.503 0.559</td><td>0.709</td><td>1.400 0.922</td><td>1.183</td></tr><tr><td>864 0.166 0.247 0.407</td><td>0.196 0.318</td><td>0.443</td><td>0.258 0.313</td><td>0.507</td><td>1.098 0.832</td><td>1.048</td><td>1.870 1.071</td><td>1.367</td></tr><tr><td>1152 0.179 0.263 0.423</td><td>0.207 0.290</td><td>0.455</td><td>0.263 0.317</td><td>0.512</td><td>0.444 0.531</td><td>0.667</td><td>1.470 0.934</td><td>1.121</td></tr><tr><td>6 0.029 0.087 0.170</td><td>0.030 0.096</td><td>0.173</td><td>0.049 0.124</td><td>0.221</td><td>0.101 0.209</td><td>0.317</td><td>0.637 0.625</td><td>0.798</td></tr><tr><td>12 0.035 0.087 0.187</td><td>0.036 0.093</td><td>0.189</td><td>0.057 0.137</td><td>0.238</td><td>0.169 0.278</td><td>0.411</td><td>0.572 0.583</td><td>0.756</td></tr><tr><td>72 0.113 0.156 0.336</td><td>0.140 0.226</td><td>0.374</td><td>0.090 0.164</td><td>0.300</td><td>0.250 0.342</td><td>0.500</td><td>0.898 0.758</td><td>0.948</td></tr><tr><td>144 0.126 0.188 0.355</td><td>0.144 0.226</td><td>0.380</td><td>0.119 0.184</td><td>0.345</td><td>0.180 0.310</td><td>0.424</td><td>1.300 0.904</td><td>1.140</td></tr><tr><td>288 0.138 0.193 0.371</td><td>0.155 0.228</td><td>0.394</td><td>0.150 0.216</td><td>0.387</td><td>0.224 0.364</td><td>0.473</td><td>1.347 0.895</td><td>1.160</td></tr><tr><td>576 0.142 0.221 0.377</td><td>0.149 0.228</td><td>0.386</td><td>0.228 0.287</td><td>0.478</td><td>0.511 0.555</td><td>0.715</td><td>1.656 0.987</td><td>1.287</td></tr><tr><td>864 0.177 0.243 0.421</td><td>0.200 0.293</td><td>0.447</td><td>0.172 0.262</td><td>0.415</td><td>1.200 0.872</td><td>1.095</td><td>1.803 1.051</td><td>1.343</td></tr><tr><td>1152 0.182 0.255 0.427</td><td>0.205 0.277</td><td>0.453</td><td>0.270 0.318</td><td>0.519</td><td>0.447 0.528</td><td>0.669</td><td>1.486 0.933</td><td>1.219</td></tr></table>

DAformer 与其他对比模型在 DKAM1 数据集上的每轮训练时间对比如图 6-9 所示。虽然 DAformer 的训练时间略长于 Informer 和 Autoformer，但其在光伏功率预测中的优异表现使得这一差距可以接受。DAformer 有效地捕捉数据中的复杂特征，特别是在长时间序列预测中展现出明显优势。总体而言，尽管 DAformer 训练时间稍长，但其在预测精度上的显著提升能够弥补这一缺点，验证了它在光伏功率预测中的高效性和实用性。

![](images/a53f03b1f97bdb2cd8f39cefd3720bcf99c98b5d12bcd1a758d8591853176908.jpg)



图6-9在DKAM1数据集训练一轮的时间


# 6.3.4消融实验

在所提出的DAformer模型中，有两个主要组成部分：（1）分解聚合注意力机制；（2）

残差块和残差连接。我们依次移除或替换每个模块，以评估其对预测结果的贡献。

- wo/分解聚合注意力：将分解聚合注意力替换为普通注意力机制。

- wo/残差：移除残差块和残差连接。

消融实验的结果如表6-3所示。可以观察到，分解聚合注意力对两个数据集的预测结果有重要影响。此外，残差块和残差连接对结果也有一定的影响，因为它可以保留在蒸馏操作中可能被忽略的特征。


表 6-3 消融实验结果 (预测步长 1152)


<table><tr><td>数据集</td><td></td><td>DKAM1</td><td>DKAM2</td></tr><tr><td rowspan="2">DAformer</td><td>MSE</td><td>0.179</td><td>0.182</td></tr><tr><td>MAE</td><td>0.263</td><td>0.255</td></tr><tr><td rowspan="2">wo/分解聚合注意力</td><td>MSE</td><td>0.242</td><td>0.236</td></tr><tr><td>MAE</td><td>0.286</td><td>0.294</td></tr><tr><td rowspan="2">wo/残差</td><td>MSE</td><td>0.197</td><td>0.193</td></tr><tr><td>MAE</td><td>0.283</td><td>0.286</td></tr></table>

# 6.4 本章小结

本章提出了一种用于长期光伏发电功率预测的DAformer模型。首先，我们对光伏系统发电功率数据进行深入分析，识别与光伏发电功率密切相关的变量。其次，为更好地捕捉数据中的季节性和趋势特征，本章融合STL方法，设计了一种分解聚合注意力机制。另外，我们引入了残差块作为蒸馏层，降低注意力机制的计算复杂度，并通过残差连接保留在蒸馏过程中可能被忽略的重要特征。实验结果显示，DAformer在长期光伏功率预测任务中表现出显著的优势，预测结果优于几种先进的基于Transformer的模型。

# 第七章 总结与展望

# 7.1总结

本文围绕时间序列预测任务，针对现有预测算法在处理多维度、非线性和复杂依赖关系上的局限性，提出了四种基于深度学习的时间序列预测模型。这些模型结合先进的深度学习理论和时间序列分解、多尺度特征学习、注意力机制等前沿技术，旨在提升时间序列预测的精确度。本文的主要工作和贡献总结如下：

（1）基于季节-趋势分解的二维时间卷积密集网络（STL-2DTCDN）：本文提出了STL-2DTCDN模型，该模型有效提取时间序列中的季节性和趋势性特征，并通过二维时间卷积密集网络实现多元时间序列间复杂依赖关系的建模。实验表明，该模型在多个公开数据集上的长期时间序列预测任务中表现出色。

（2）基于预训练的多头接收加权键值与多尺度图卷积网络（PMRWKV-GCN）模型：本文提出了 PMRWKV-GCN 模型，通过通道独立策略实现各时间序列特征的独立学习，避免来自不相关序列噪声干扰，并通过学习多元时间序列的多尺度时空依赖关系，提升预测结果的精度。

（3）基于多尺度接收加权键值与二维时间卷积网络（MSRWKV-2DTCN）的模型：针对短期光伏发电功率预测中的复杂周期性和非线性依赖问题，本文提出了MSRWKV-2DTCN模型，该模型结合快速傅里叶变换和多尺度二维卷积网络，有效捕捉多尺度特征及时空依赖关系。实验结果显示，该模型在多个光伏发电功率数据集上取得显著的性能提升。

（4）基于分解聚合注意力的Transformer模型（DAformer）：本文针对长期光伏发电功率预测任务，提出了DAformer模型，该模型能够有效捕捉时间序列中的长期的趋势和季节性特征，并降低注意力机制计算复杂度。实验验证了该模型在长期光伏功率预测中的有效性。

通过对这些模型的设计、实现和实验验证，本文展示深度学习技术在时间序列预测中的广泛应用潜力，为解决当前时间序列预测任务中的挑战提供新的思路和方法。

# 7.2 研究展望

虽然本文提出的模型在时间序列预测任务上取得较好的预测效果，但仍然存在许多值得进一步研究的问题。随着时间序列数据形态和应用场景日趋复杂化，未来研究可以从多个维度进行深入挖掘，以优化模型的性能并扩大其应用范围。以下几个方向尤为值得关注：

（1）增强模型的可解释性：由于深度学习模型的“黑箱”特性，其预测过程往往难以被直观理解。在一些高风险或决策敏感的领域，用户不仅要求高精度的预测，还期望能够清晰了解模型的决策逻辑。因此，未来的研究可专注于如何让模型更加透明，可以通过结合传统统计方法（如决策树、回归分析）或者开发新的解释手段（如可视化注意力机制、特征重要性分析）来实现。通过提高模型的可解释性，不仅能帮助用户更好地解读预测结果，还能在实际工业和商业应用中增强用户的信任感，进而提升模型应用价值。

（2）优化计算资源与能耗：深度学习模型的计算成本和能耗问题一直是学术界和工业界关注的焦点，特别是在嵌入式系统或物联网设备等资源受限的环境中，这一问题更为突出。因此，未来研究的一个重要方向是在保证模型性能的同时，设计更加高效、轻量化的网络架构，以平衡计算资源消耗和能耗。一些技术手段，例如模型剪枝、知识蒸馏等模型压缩方法，已经被广泛应用，在维持预测精度的同时，减少计算资源消耗。

（3）在未来的光伏功率预测研究中，可以将光伏发电预测与电力负载预测相结合，构建一个集成式的电网管理系统。这一系统不仅能为电网提供更全面的发电与消耗动态变化，还能提升电力调度的灵活性和效率。通过更加精准的能源分配与储存优化，电网的稳定性和安全性也将得到改善。与此同时，未来的研究还可重点探索多源数据的整合，如气象卫星采集的图像数据，以提升光伏功率预测的准确性。此外，利用先进的深度学习技术，有望进一步增强光伏功率预测模型的鲁棒性和泛化能力，从而在更加复杂场景中取得更优表现。

（4）提高对极端情况的预测能力：在实际应用中，极端事件（如极端天气、金融危机、突发系统故障等）往往会对系统的整体性能产生深远影响。因此，提升模型在极端情况下的预测能力成为未来研究中的关键挑战之一。通过引入异常检测技术，并结合自

适应的模型调整机制，可以增强模型应对极端情况的能力，降低系统的风险暴露。此外，进一步探索对极端值的建模也至关重要。结合极值理论（Extreme Value Theory, EVT）与深度学习模型，有望提高模型对极端事件的敏感度与应对能力，从而显著提升系统的鲁棒性。

# 参考文献



[1] 王燕. 应用时间序列分析. 第四版. 中国人民大学出版社, 2016.





[2] 易丹辉. 时间序列分析方法与应用. 第二版. 中国人民大学出版社, 2018.





[3] 艾琳·尼尔森. 时间序列分析实战：基于机器学习和统计学. 人民邮电出版社, 2023.





[4] Kaur J, Parmar K S, Singh S. Autoregressive models in environmental forecasting time series: A theoretical and application review[J]. Environmental Science and Pollution Research, 2023, 30(8): 19617-19641.





[5] Babu C N, Reddy B E. A moving-average filter based hybrid ARIMA-ANN model for forecasting time series data[J]. Applied Soft Computing, 2014, 23: 27-38.





[6] Khashei M, Bijari M, Hejazi S R. Combining seasonal ARIMA models with computational intelligence techniques for time series forecasting[J]. Soft Computing, 2012, 16: 1091-1105.





[7] Chen Z, Ma M, Li T, et al. Long sequence time-series forecasting with deep learning: A survey[J]. Information Fusion, 2023, 97: 101819.





[8] Shen Z, Zhang Y, Lu J, et al. A novel time series forecasting model with deep learning[J]. Neurocomputing, 2020, 396: 302-313.





[9] Torres J F, Hadjout D, Sebaa A, et al. Deep learning for time series forecasting: A survey[J]. Big Data, 2021, 9(1): 3-21.





[10] Vaswani A. Attention is all you need[J]. Advances in Neural Information Processing Systems, 2017.





[11] Hewamalage H, Bergmeir C, Bandara K. Recurrent neural networks for time series forecasting: Current status and future directions[J]. International Journal of Forecasting, 2021, 37(1): 388-427.





[12] Khaldi R, El Afia A, Chiheb R, et al. What is the best RNN-cell structure to forecast each time series behavior?[J]. Expert Systems with Applications, 2023, 215: 119140.





[13] 万晨,李文中,丁望祥,等.一种基于自演化预训练的多变量时间序列预测算法[J].计算机





学报, 2022, 45(3):513-525.





[14] Li X, Ma X, Xiao F, et al. Time-series production forecasting method based on the integration of bidirectional gated recurrent unit (Bi-GRU) network and sparrow search algorithm (SSA)[J]. Journal of Petroleum Science and Engineering, 2022, 208: 109309.





[15] Wen Q, Zhou T, Zhang C, et al. Transformers in time series: A survey[C]. Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. 2023: 6778-6786.





[16] Ahmed S, Nielsen I E, Tripathi A, et al. Transformers in time-series analysis: A tutorial[J]. Circuits, Systems, and Signal Processing, 2023, 42(12): 7433-7466.





[17] Kristiansen T. Forecasting Nord Pool day-ahead prices with an autoregressive model[J]. Energy Policy, 2012, 49: 328-332.





[18] Ivanovski Z, Milenkovski A, Narasanov Z. Time series forecasting using a moving average model for extrapolation of number of tourist[J]. UTMS Journal of Economics, 2018, 9(2): 121-132.





[19] Rojas I, Valenzuela O, Rojas F, et al. Soft-computing techniques and ARMA model for time series prediction[J]. Neurocomputing, 2008, 71(4-6): 519-537.





[20] Corberán-Vallet A, Bermúdez J D, Vercher E. Forecasting correlated time series with exponential smoothing models[J]. International Journal of Forecasting, 2011, 27(2): 252-265.





[21] Pattanayak R M, Panigrahi S, Behera H S. High-order fuzzy time series forecasting by using membership values along with data and support vector machine[J]. Arabian Journal for Science and Engineering, 2020, 45(12): 10311-10325.





[22] Spiliotis E. Decision trees for time-series forecasting[J]. Foresight, 2022, 1: 30-44.





[23] Qiu X, Zhang L, Suganthan P N, et al. Oblique random forest ensemble via least square estimation for time series forecasting[J]. Information Sciences, 2017, 420: 249-262.





[24] Li L, Dai S, Cao Z, et al. Using improved gradient-boosted decision tree algorithm based on Kalman filter (GBDT-KF) in time series prediction[J]. The Journal of Supercomputing,





2020, 76: 6887-6900.





[25] Zhang N, Lin A, Shang P. Multidimensional k-nearest neighbor model based on EEMD for financial time series forecasting[J]. Physica A: Statistical Mechanics and its Applications, 2017, 477: 161-173.





[26] Tajmouati S, Wahbi B E L, Bedoui A, et al. Applying k-nearest neighbors to time series forecasting: Two new approaches[J]. Journal of Forecasting, 2024, 43(5): 1559-1574.





[27] Fan D, Sun H, Yao J, et al. Well production forecasting based on ARIMA-LSTM model considering manual operations[J]. Energy, 2021, 220: 119708.





[28] Albeladi K, Zafar B, Mueen A. Time series forecasting using LSTM and ARIMA[J]. International Journal of Advanced Computer Science and Applications, 2023, 14(1): 313-320.





[29] Wu J. Introduction to convolutional neural networks[J]. National Key Lab for Novel Software Technology. Nanjing University. China, 2017, 5(23): 495.





[30] Chen G, Tian H, Xiao T, et al. Time series forecasting of oil production in Enhanced Oil Recovery system based on a novel CNN-GRU neural network[J]. Geoenergy Science and Engineering, 2024, 233: 212528.





[31] Bai S, Kolter J Z, Koltun V. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling[C]. International Conference on Learning Representations, 2018.





[32] Wan R, Mei S, Wang J, et al. Multivariate temporal convolutional network: A deep neural networks approach for multivariate time series forecasting[J]. Electronics, 2019, 8(8): 876.





[33] Zhang C X, Li J, Huang X F, et al. Forecasting stock volatility and value-at-risk based on temporal convolutional networks[J]. Expert Systems with Applications, 2022, 207: 117951.





[34] Yin L, Xie J. Multi-temporal-spatial-scale temporal convolution network for short-term load forecasting of power systems[J]. Applied Energy, 2021, 283: 116328.





[35] Zhu R, Liao W, Wang Y. Short-term prediction for wind power based on temporal





convolutional network[J]. Energy Reports, 2020, 6: 424-429.





[36] Hewage P, Behera A, Trovati M, et al. Temporal convolutional neural network for an effective weather forecasting using time-series data from the local weather station[J]. Soft Computing, 2020, 24: 16453-16482.





[37] Yan J, Mu L, Wang L, et al. Temporal convolutional networks for the advance prediction of ENSO[J]. Scientific reports, 2020, 10(1): 8055.





[38] Bi J, Zhang X, Yuan H, et al. A hybrid prediction method for realistic network traffic with temporal convolutional network and LSTM[J]. IEEE Transactions on Automation Science and Engineering, 2021, 19(3): 1869-1879.





[39] Zhao W, Gao Y, Ji T, et al. Deep temporal convolutional networks for short-term traffic flow forecasting[J]. IEEE Access, 2019, 7: 114496-114507.





[40] Guo G, Yuan W. Short-term traffic speed forecasting based on graph attention temporal convolutional networks[J]. Neurocomputing, 2020, 410: 387-393.





[41] Ingolfsson T M, Wang X, Hersche M, et al. ECG-TCN: Wearable cardiac arrhythmia detection with a temporal convolutional network[C]. 2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS). IEEE, 2021: 1-4.





[42] Wu Z, Pan S, Chen F, et al. A comprehensive survey on graph neural networks[J]. IEEE transactions on neural networks and learning systems, 2020, 32(1): 4-24.





[43] Jiang W, Luo J. Graph neural network for traffic forecasting: A survey[J]. Expert systems with applications, 2022, 207: 117921.





[44] Bui K H N, Cho J, Yi H. Spatial-temporal graph neural network for traffic forecasting: An overview and open research issues[J]. Applied Intelligence, 2022, 52(3): 2763-2774.





[45] Jiang W, Luo J, He M, et al. Graph neural network for traffic forecasting: The research progress[J]. ISPRS International Journal of Geo-Information, 2023, 12(3): 100.





[46] Cao J, Li Z, Li J. Financial time series forecasting model based on CEEMDAN and LSTM[J]. Physica A: Statistical mechanics and its applications, 2019, 519: 127-139.





[47] Zhang X, Zeman M, Tsiligkaridis T, et al. Graph-guided network for irregularly sampled





multivariate time series[C]. International Conference on Learning Representations, 2021.





[48] Velicković P, Cucurull G, Casanova A, et al. Graph attention networks[C]. International Conference on Learning Representations, 2018.





[49] Yu X, Shi S, Xu L. A spatial-temporal graph attention network approach for air temperature forecasting[J]. Applied Soft Computing, 2021, 113: 107888.





[50] Liu H, Yang D, Liu X, et al. Todynet: Temporal dynamic graph neural network for multivariate time series classification[J]. Information Sciences, 2024: 120914.





[51] Chen Y, Ding F, Zhai L. Multi-scale temporal features extraction based graph convolutional network with attention for multivariate time series prediction[J]. Expert Systems with Applications, 2022, 200: 117011.





[52] Guo K, Hu Y, Sun Y, et al. Hierarchical graph convolution network for traffic forecasting[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(1): 151-159.





[53] Zhao L, Song Y, Zhang C, et al. T-GCN: A temporal graph convolutional network for traffic prediction[J]. IEEE Transactions on Intelligent Transportation Systems, 2019, 21(9): 3848-3858.





[54] Wang S, Chen A, Wang P, et al. Predicting electric vehicle charging demand using a heterogeneous spatio-temporal graph convolutional network[J]. Transportation Research Part C: Emerging Technologies, 2023, 153: 104205.





[55] Yu X, Tang B, Zhang K. Fault diagnosis of wind turbine gearbox using a novel method of fast deep graph convolutional networks[J]. IEEE Transactions on Instrumentation and Measurement, 2021, 70: 1-14.





[56] Xu Y, Liu A, Hao J, et al. PLUTUS: A well pre-trained large unified Transformer can unveil financial time series regularities. arXiv preprint arXiv:2408.10111, 2024.





[57] Ji J, He J, Lei M, et al. Spatio-temporal Transformer network for weather forecasting[J]. IEEE Transactions on Big Data, 2024.





[58] Chen L, You Z, Zhang N, et al. UTRAD: Anomaly detection and localization with U-Transformer[J]. Neural Networks, 2022, 147: 53-62.





[59] TTS-GAN: A transformer-based time-series generative adversarial network[C]. International conference on artificial intelligence in medicine. Cham: Springer International Publishing, 2022: 133-143.





[60] Ravi V, Pradeepkumar D, Deb K. Financial time series prediction using hybrids of chaos theory, multi-layer perceptron and multi-objective evolutionary algorithms[J]. Swarm and Evolutionary Computation, 2017, 36: 136-149.





[61] Zeng A, Chen M, Zhang L, et al. Are Transformers effective for time series forecasting?[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(9): 11121-11128.





[62] Das A, Kong W, Leach A, et al. Long-term forecasting with TiDE: Time-series dense encoder[C]. Transactions on Machine Learning Research, 2023.





[63] Chen S A, Li C L, Yoder N, et al. TSMixer: An all-MLP architecture for time series forecasting[C]. Transactions on Machine Learning Research, 2023.





[64] Zhang S, Tong H, Xu J, et al. Graph convolutional networks: a comprehensive review[J]. Computational Social Networks, 2019, 6(1): 1-23.





[65] Wang X, Zhu M, Bo D, et al. Am-gen: Adaptive multi-channel graph convolutional networks[C]. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020: 1243-1253.





[66] Peng B, Alcaide E, Anthony Q, et al. RWKV: Reinventing RNNs for the Transformer era[C]. The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.





[67] Zoph B, Ghiasi G, Lin T Y, et al. Rethinking pre-training and self-training[J]. Advances in Neural Information Processing Systems, 2020, 33: 3833-3845.





[68] Erhan D, Courville A, Bengio Y, et al. Why does unsupervised pre-training help deep learning?[C]. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings, 2010: 201-208.





[69] Hendrycks D, Lee K, Mazeika M. Using pre-training can improve model robustness and





uncertainty[C]. International Conference on Machine Learning. PMLR, 2019: 2712-2721.





[70] Nie Y, Nguyen N H, Sinthong P, et al. A time series is worth 64 words: Long-term forecasting with Transformers[C]. International Conference on Learning Representations 2022.





[71] Shao Z, Zhang Z, Wang F, et al. Pre-training enhanced spatial-temporal graph neural network for multivariate time series forecasting[C]. Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022: 1567-1577.





[72] Xia H, Ao H, Li L, et al. CI-STHPAN: Pre-trained attention network for stock selection with channel-independent spatio-temporal hypergraph[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(8): 9187-9195.





[73] Cleveland R B, Cleveland W S, McRae J E, et al. STL: A seasonal-trend decomposition[J]. Journal of Official Statistics, 1990, 6(1): 3-73.





[74] Sun T, Zhang T, Teng Y, et al. Monthly electricity consumption forecasting method based on X12 and STL decomposition model in an integrated energy system[J]. Mathematical Problems in Engineering, 2019, 2019(1): 9012543.





[75] Senoguchi J. Stock price prediction through STL decomposition using multivariate two-way long short-term memory[J]. Journal of Computer Science and Technology Studies, 2022, 4(2): 90-96.





[76] Huo Y, Yan Y, Du D, et al. Long-term span traffic prediction model based on STL decomposition and LSTM[C]. 2019 20th Asia-Pacific Network Operations and Management Symposium. IEEE, 2019: 1-4.





[77] Chen B T, Chen M Y, Fan M H, et al. Forecasting stock price based on fuzzy time-series with equal-frequency partitioning and fast Fourier transform algorithm[C]. 2012 Computing, Communications and Applications Conference. IEEE, 2012: 238-243.





[78] Zouaidia K, Ghanemi S, Rais M S. Hourly wind speed forecasting using FFT-encoder-decoder-LSTM in south west of Algeria (Adrar)[J]. International Journal of Informatics and





Applied Mathematics, 2021, 4(1): 72-83.





[79] Reza S, Ferreira M C, Machado J J M, et al. A multi-head attention-based Transformer model for traffic flow forecasting with a comparative analysis to recurrent neural networks[J]. Expert Systems with Applications, 2022, 202: 117275.





[80] Han Y, Mi L, Shen L, et al. A short-term wind speed prediction method utilizing novel hybrid deep learning algorithms to correct numerical weather forecasting[J]. Applied Energy, 2022, 312: 118777.





[81] Khan Z A, Ullah A, Haq I U, et al. Efficient short-term electricity load forecasting for effective energy management[J]. Sustainable Energy Technologies and Assessments, 2022, 53: 102337.





[82] Liang Y, Lin Y, Lu Q. Forecasting gold price using a novel hybrid model with ICEEMDAN and LSTM-CNN-CBAM[J]. Expert Systems with Applications, 2022, 206: 117847.





[83] Johansson C, Zhang Z, Engardt M, et al. Improving 3-day deterministic air pollution forecasts using machine learning algorithms[J]. Atmospheric Chemistry and Physics Discussions, 2023, 2023: 1-52.





[84] Alizadeh M, Rahimi S, Ma J. A hybrid ARIMA-WNN approach to model vehicle operating behavior and detect unhealthy states[J]. Expert Systems with Applications, 2022, 194: 116515.





[85] Smyl S. A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting[J]. International Journal of Forecasting, 2020, 36(1): 75-85.





[86] Xiao J, Zhou Z. Research progress of RNN language model[C]. 2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA). IEEE, 2020: 1285-1288.





[87] Liu Y, Gong C, Yang L, et al. DSTP-RNN: A dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction[J]. Expert Systems with Applications, 2020, 143: 113082.





[88] Hajiabotorabi Z, Kazemi A, Samavati F F, et al. Improving DWT-RNN model via B-spline





wavelet multiresolution to forecast a high-frequency time series[J]. Expert Systems with Applications, 2019, 138: 112842.





[89] Khan M, Wang H, Riaz A, et al. Bidirectional LSTM-RNN-based hybrid deep learning frameworks for univariate time series classification[J]. The Journal of Supercomputing, 2021, 77: 7021-7045.





[90] Zheng W, Chen G. An accurate GRU-based power time-series prediction approach with selective state updating and stochastic optimization[J]. IEEE Transactions on Cybernetics, 2021, 52(12): 13902-13914.





[91] 谢贵才,段磊,蒋为鹏,等.多尺度时序依赖的校园公共区域人流量预测[J].软件学报,2021,32(03):831-844.





[92] Livieris I E, Pintelas E, Pintelas P. A CNN-LSTM model for gold price time-series forecasting[J]. Neural computing and applications, 2020, 32: 17351-17360.





[93] Du L, Gao R, Suganthan P N, et al. Bayesian optimization based dynamic ensemble for time series forecasting[J]. Information Sciences, 2022, 591: 155-175.





[94] Du L, Gao R, Suganthan P N, et al. Graph ensemble deep random vector functional link network for traffic forecasting[J]. Applied Soft Computing, 2022, 131: 109809.





[95] Albuquerque P H M, Peng Y, Silva J P F. Making the whole greater than the sum of its parts: A literature review of ensemble methods for financial time series forecasting[J]. Journal of Forecasting, 2022, 41(8): 1701-1724.





[96] Khan S, Naseer M, Hayat M, et al. Transformers in vision: A survey[J]. ACM Computing Surveys (CSUR), 2022, 54(10s): 1-41.





[97] Wu Y, Zhao Y, Hu B, et al. An efficient memory-augmented Transformer for knowledge-intensive NLP tasks[C]. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022, 5184-5196.





[98] Li S, Jin X, Xuan Y, et al. Enhancing the locality and breaking the memory bottleneck of Transformer on time series forecasting[J]. Advances in Neural Information Processing





Systems, 2019, 32.





[99] Kitaev N, Kaiser L, Levskaya A. Reformer: The efficient Transformer[C]. International Conference on Learning Representations, 2020.





[100] Zhou H, Zhang S, Peng J, et al. Informer: Beyond efficient Transformer for long sequence time-series forecasting[C]. Proceedings of the AAAI conference on artificial intelligence. 2021, 35(12): 11106-11115.





[101] Wu H, Xu J, Wang J, et al. Autoformer: Decomposition Transformers with autocorrelation for long-term series forecasting[J]. Advances in Neural Information Processing Systems, 2021, 34: 22419-22430.





[102] Zhou T, Ma Z, Wen Q, et al. Fedformer: Frequency enhanced decomposed Transformer for long-term series forecasting[C]. International Conference on Machine Learning. PMLR, 2022: 27268-27286.





[103] Wang X, Liu H, Yang Z, et al. CNformer: A convolutional Transformer with decomposition for long-term multivariate time series forecasting[J]. Applied Intelligence, 2023, 53(17): 20191-20205.





[104] He H, Gao S, Jin T, et al. A seasonal-trend decomposition-based dendritic neuron model for financial time series prediction[J]. Applied Soft Computing, 2021, 108: 107488.





[105] Waibel A, Hanazawa T, Hinton G. Phoneme recognition using time-delay neural networks[J]. IEEE Transactions on Acoustics, Speech, and Signal Processing, 1989, 31(3).





[106] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 770-778.





[107] Zhang Y, Yan J. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting[C]. The Eleventh International Conference on Learning Representations. 2023.





[108] Liu Y, Hu T, Zhang H, et al. iTransformer: Inverted Transformers are effective for time series forecasting[C]. International Conference on Learning Representations, 2024.





[109] Hou H, Yu F R. RWKV-TS: Beyond traditional recurrent neural network for time series





tasks. arXiv preprint arXiv:2401.09093, 2024.





[110] Duan Y, Wang W, Chen Z, et al. Vision-RWKV: Efficient and scalable visual perception with RWKV-like architectures. arXiv preprint arXiv:2403.02308, 2024.





[111] Liu X, Su Y, Nier W, et al. An Approach to Mongolian Neural Machine Translation Based on RWKV Language Model and Contrastive Learning[C]. International Conference on Neural Information Processing. Singapore: Springer Nature Singapore, 2023: 327-340.





[112] Fei Z, Fan M, Yu C, et al. Diffusion-RWKV: Scaling RWKV-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024.





[113] Cheng D, Yang F, Xiang S, et al. Financial time series forecasting with multi-modality graph neural network[J]. Pattern Recognition, 2022, 121: 108218.





[114] Li Z L, Zhang G W, Yu J, et al. Dynamic graph structure learning for multivariate time series forecasting[J]. Pattern Recognition, 2023, 138: 109423.





[115] Bai J, Zhu J, Song Y, et al. A3t-gen: Attention temporal graph convolutional network for traffic forecasting[J]. ISPRS International Journal of Geo-Information, 2021, 10(7): 485.





[116] Li Y, Yu R, Shahabi C, et al. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting[J]. International Conference on Learning Representations, 2018.





[117] Zhao L, Shen Y. Rethinking channel dependence for multivariate time series forecasting: Learning from leading indicators[C]. International Conference on Learning Representations, 2024.





[118] Woo G, Liu C, Sahoo D, et al. ETSformer: Exponential smoothing Transformers for time-series forecasting. arXiv preprint arXiv:2202.01381, 2022.





[119] Cai W, Liang Y, Liu X, et al. MSGNet: Learning multi-scale inter-series correlations for multivariate time series forecasting[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(10): 11141-11149.





[120] Wu H, Hu T, Liu Y, et al. TimesNet: Temporal 2d-variation modeling for general time series analysis[C]. International Conference on Learning Representations, 2022.





[121] Wang X, Ma W. A hybrid deep learning model with an optimal strategy based on improved VMD and Transformer for short-term photovoltaic power forecasting[J]. Energy, 2024, 295: 131071.





[122] Helveston J P, He G, Davidson M R. Quantifying the cost savings of global solar photovoltaic supply chains[J]. Nature, 2022, 612(7938): 83-87.





[123] Jäger-Waldau A. Snapshot of photovoltaics- May 2023[J]. EPJ Photovoltaics, 2023, 14: 23.





[124] Li G, Wei X, Yang H. Decomposition integration and error correction method for photovoltaic power forecasting[J]. Measurement, 2023, 208: 112462.





[125] Nguyen T N, Mûsgens F. What drives the accuracy of PV output forecasts?[J]. Applied Energy, 2022, 323: 119603.





[126] Sobri S, Koohi-Kamali S, Rahim N A. Solar photovoltaic generation forecasting methods: A review[J]. Energy conversion and management, 2018, 156: 459-497.





[127] Rajagukguk R A, Ramadhan R A A, Lee H J. A review on deep learning models for forecasting time series data of solar irradiance and photovoltaic power[J]. Energies, 2020, 13(24): 6623.





[128] Das U K, Tey K S, Seyedmahmoudian M, et al. Forecasting of photovoltaic power generation and model optimization: A review[J]. Renewable and Sustainable Energy Reviews, 2018, 81: 912-928.





[129] Luo X, Zhang D, Zhu X. Deep learning based forecasting of photovoltaic power generation by incorporating domain knowledge[J]. Energy, 2021, 225: 120240.





[130] Massidda L, Marrocu M. Use of multilinear adaptive regression splines and numerical weather prediction to forecast the power output of a PV plant in Borkum, Germany[J]. Solar Energy, 2017, 146: 141-149.





[131] Miao S, Ning G, Gu Y, et al. Markov Chain model for solar farm generation and its application to generation performance evaluation[J]. Journal of Cleaner Production, 2018, 186: 905-917.





[132] Agoua X G, Girard R, Kariniotakis G. Short-term spatio-temporal forecasting of photovoltaic power production[J]. IEEE Transactions on Sustainable Energy, 2017, 9(2): 538-546.





[133] De Falco P, Di Noia L P, Rizzo R. Exponential smoothing model for photovoltaic power forecasting[C]. 2021 9th International Conference on Modern Power Systems (MPS). IEEE, 2021: 1-5.





[134] Jiang Y, Zheng L, Ding X. Ultra-short-term prediction of photovoltaic output based on an LSTM-ARMA combined model driven by EEMD[J]. Journal of Renewable and Sustainable Energy, 2021, 13(4).





[135] Halabi L M, Mekhilef S, Hossain M. Performance evaluation of hybrid adaptive neurofuzzy inference system models for predicting monthly global solar radiation[J]. Applied energy, 2018, 213: 247-261.





[136] Raza M Q, Nadarajah M, Ekanayake C. On recent advances in PV output power forecast[J]. Solar Energy, 2016, 136: 125-144.





[137] Kumar G, Singh U P, Jain S. Hybrid evolutionary intelligent system and hybrid time series econometric model for stock price forecasting[J]. International Journal of Intelligent Systems, 2021, 36(9): 4902-4935.





[138] Cao Y, Liu G, Luo D, et al. Multi-timescale photovoltaic power forecasting using an improved Stacking ensemble algorithm based LSTM-Informer model[J]. Energy, 2023, 283: 128669.





[139] Mellit A, Pavan A M, Lughi V. Deep learning neural networks for short-term photovoltaic power forecasting[J]. Renewable Energy, 2021, 172: 276-288.





[140] Zendehboudi A, Baseer M A, Saidur R. Application of support vector machine models for forecasting solar and wind energy resources: A review[J]. Journal of Cleaner Production, 2018, 199: 272-285.





[141] Li G, Wang H, Zhang S, et al. Recurrent neural networks based photovoltaic power





forecasting approach[J]. Energies, 2019, 12(13): 2538.





[142] Zhou F, Wang Z, Zhong T, et al. HydroFlow: Towards probabilistic electricity demand prediction using variational autoregressive models and normalizing flows[J]. International Journal of Intelligent Systems, 2022, 37(10): 6833-6856.





[143] Chai M, Xia F, Hao S, et al. PV power prediction based on LSTM with adaptive hyperparameter adjustment[J]. IEEE Access, 2019, 7: 115473-115486.





[144] Massaoudi M, Chihi I, Sidhom L, et al. Performance evaluation of deep recurrent neural networks architectures: Application to PV power forecasting[C]. 2019 2nd International Conference on Smart Grid and Renewable Energy (SGRE). IEEE, 2019: 1-6.





[145] Ghimire S, Deo R C, Raj N, et al. Deep solar radiation forecasting with convolutional neural network and long short-term memory network algorithms[J]. Applied Energy, 2019, 253: 113541.





[146] Khan Z A, Hussain T, Haq I U, et al. Towards efficient and effective renewable energy prediction via deep learning[J]. Energy Reports, 2022, 8: 10230-10243.





[147] Khan Z A, Hussain T, Baik S W. Dual stream network with attention mechanism for photovoltaic power forecasting[J]. Applied Energy, 2023, 338: 120916.





[148] Wang H, Lei Z, Zhang X, et al. A review of deep learning for renewable energy forecasting[J]. Energy Conversion and Management, 2019, 198: 111799.





[149] Qu J, Qian Z, Pei Y. Day-ahead hourly photovoltaic power forecasting using attention-based CNN-LSTM neural network embedded with multiple relevant and target variables prediction pattern[J]. Energy, 2021, 232: 120996.





[150] Mishra M, Dash P B, Nayak J, et al. Deep learning and wavelet transform integrated approach for short-term solar PV power prediction[J]. Measurement, 2020, 166: 108250.





[151] Wang K, Qi X, Liu H. Photovoltaic power forecasting based LSTM-convolutional network[J]. Energy, 2019, 189: 116225.





[152] Hussain A, Khan Z A, Hussain T, et al. A hybrid deep learning-based network for photovoltaic power forecasting[J]. Complexity, 2022, 2022(1): 7040601.





[153] Wang K, Qi X, Liu H. A comparison of day-ahead photovoltaic power forecasting models based on deep learning neural network[J]. Applied Energy, 2019, 251: 113315.





[154] Li P, Zhou K, Lu X, et al. A hybrid deep learning model for short-term PV power forecasting[J]. Applied Energy, 2020, 259: 114216.





[155] Khan S U, Khan N, Ullah F U M, et al. Towards intelligent building energy management: AI-based framework for power consumption and generation forecasting[J]. Energy and Buildings, 2023, 279: 112705.





[156] Sherozbek J, Park J, Akhtar M S, et al. Transformers-based encoder model for forecasting hourly power output of transparent photovoltaic module systems[J]. Energies, 2023, 16(3): 1353.





[157] Phan Q T, Wu Y K, Phan Q D. Application of a new Transformer-based model and XGBoost to improve one-day-ahead solar power forecasts[C]. 2023 IEEE/IAS 59th Industrial and Commercial Power Systems Technical Conference (I&CPS). IEEE, 2023: 1-7.





[158] The Desert Knowledge Australia Solar Centre (DKASC), https://dkasolarcentre.com.au/.





[159] Xu H, Li F. A multiscale dilated convolution and mixed-order attention-based deep neural network for monocular depth prediction[J]. SN Applied Sciences, 2023, 5(1): 24.





[160] Lea C, Flynn M D, Vidal R, et al. Temporal convolutional networks for action segmentation and detection[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 156-165.





[161] Liu X, Liu Y, Kong X, et al. Deep neural network for forecasting of photovoltaic power based on wavelet packet decomposition with similar day analysis[J]. Energy, 2023, 271: 126963.





[162] Wang C, Lin H, Hu H, et al. A hybrid model with combined feature selection based on optimized VMD and improved multi-objective coati optimization algorithm for short-term wind power prediction[J]. Energy, 2024, 293: 130684.





[163] Liu R, Wei J, Sun G, et al. A short-term probabilistic photovoltaic power prediction





method based on feature selection and improved LSTM neural network[J]. Electric Power Systems Research, 2022, 210: 108069.





[164] Lin W, Zhang B, Li H, et al. Multi-step prediction of photovoltaic power based on two-stage decomposition and BILSTM[J]. Neurocomputing, 2022, 504: 56-67.





[165] Zhang Z, Wang J, Wei D, et al. An improved temporal convolutional network with attention mechanism for photovoltaic generation forecasting[J]. Engineering Applications of Artificial Intelligence, 2023, 123: 106273.





[166] Korkmaz D. SolarNet: A hybrid reliable model based on convolutional neural network and variational mode decomposition for hourly photovoltaic power forecasting[J]. Applied Energy, 2021, 300: 117410.





[167] Li L L, Wen S Y, Tseng M L, et al. Renewable energy prediction: A novel short-term prediction model of photovoltaic output power[J]. Journal of Cleaner Production, 2019, 228: 359-375.





[168] Zhou Y, Zhou N, Gong L, et al. Prediction of photovoltaic power output based on similar day analysis, genetic algorithm and extreme learning machine[J]. Energy, 2020, 204: 117894.





[169] Zang H, Cheng L, Ding T, et al. Day-ahead photovoltaic power forecasting approach based on deep convolutional neural networks and meta learning[J]. International Journal of Electrical Power & Energy Systems, 2020, 118: 105790.





[170] Cheng L, Zang H, Ding T, et al. Multi-meteorological-factor-based graph modeling for photovoltaic power forecasting[J]. IEEE Transactions on Sustainable Energy, 2021, 12(3): 1593-1603.





[171] Khan Z A, Hussain T, Baik S W. Boosting energy harvesting via deep learning-based renewable power generation prediction[J]. Journal of King Saud University-Science, 2022, 34(3): 101815.





[172] Ding S, Li R, Tao Z. A novel adaptive discrete grey model with time-varying parameters for long-term photovoltaic power generation forecasting[J]. Energy Conversion and





Management, 2021, 227: 113644.





[173] Mohamad Radzi P N L, Akhter M N, Mekhilef S, et al. Review on the application of photovoltaic forecasting using machine learning for very short-to long-term forecasting[J]. Sustainability, 2023, 15(4): 2942.





[174] Bāsaran K, Bozyigit F, Siano P, et al. Systematic literature review of photovoltaic output power forecasting[J]. IET Renewable Power Generation, 2020, 14(19): 3961-3973.





[175] Jung Y, Jung J, Kim B, et al. Long short-term memory recurrent neural network for modeling temporal patterns in long-term power forecasting for solar PV facilities: Case study of South Korea[J]. Journal of Cleaner Production, 2020, 250: 119476.





[176] Han S, Qiao Y, Yan J, et al. Mid-to-long term wind and photovoltaic power generation prediction based on copula function and long short term memory network[J]. Applied Energy, 2019, 239: 181-191.





[177] Sharma J, Soni S, Paliwal P, et al. A novel long term solar photovoltaic power forecasting approach using LSTM with Nadam optimizer: A case study of India[J]. Energy Science & Engineering, 2022, 10(8): 2909-2929.





[178] Ray B, Shah R, Islam M R, et al. A new data driven long-term solar yield analysis model of photovoltaic power plants[J]. IEEE Access, 2020, 8: 136223-136233.





[179] Liu Z, Guo J, Wang X, et al. Prediction of long-term photovoltaic power generation in the context of climate change[J]. Renewable Energy, 2024: 121263.





[180] Ofori-Ntow Jnr E, Ziggah Y Y, Rodrigues M J, et al. A new long-term photovoltaic power forecasting model based on stacking generalization methodology[J]. Natural Resources Research, 2022, 31(3): 1265-1287.





[181] Kang Z, Xue J, Lai C S, et al. Vision Transformer-based photovoltaic prediction model[J]. Energies, 2023, 16(12): 4737.





[182] Phan Q T, Wu Y K, Phan Q D. An approach using Transformer-based model for short-term PV generation forecasting[C]. 2022 8th International Conference on Applied System





Innovation (ICASI). IEEE, 2022: 17-20.





[183] Tao K, Zhao J, Tao Y, et al. Operational day-ahead photovoltaic power forecasting based on Transformer variant[J]. Applied Energy, 2024, 373: 123825.





[184] Liao Z, Min W, Li C, et al. Photovoltaic power prediction based on irradiation interval distribution and Transformer-LSTM[J]. Energies, 2024, 17(12): 2969.





[185] Kim J, Obregon J, Park H, et al. Multi-step photovoltaic power forecasting using Transformer and recurrent neural networks[J]. Renewable and Sustainable Energy Reviews, 2024, 200: 114479.

