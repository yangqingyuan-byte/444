# Gated Attention for Large Language Models: Non-linearity, Sparsity,and Attention-Sink-Free

Zihan ${ \bf Q } { \bf i } { \bf u } ^ { * 1 }$ , Zekun Wang∗1, Bo Zheng∗1, Zeyu Huang∗2,Kaiyue Wen3, Songlin Yang4, Rui Men1, Le ${ \bf { Y } } { \bf { u } } ^ { 1 }$ , Fei Huang1, Suozhi Huang5,Dayiheng LiuB1, Jingren Zhou1, Junyang LinB11Qwen Team, Alibaba Group 2University of Edinburgh 3Stanford University4MIT 5Tsinghua University

# Abstract

Gating mechanisms have been widely utilized, from early models like LSTMs (Hochreiter& Schmidhuber, 1997) and Highway Networks (Srivastava et al., 2015) to recent statespace models (Gu & Dao, 2023), linear attention (Hua et al., 2022), and also softmaxattention (Lin et al., 2025). Yet, existing literature rarely examines the specific effects ofgating. In this work, we conduct comprehensive experiments to systematically investigategating-augmented softmax attention variants. Specifically, we perform a comprehen-sive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7Bdense models trained on a 3.5 trillion token dataset. Our central finding is that a simplemodification—applying a head-specific sigmoid gate after the Scaled Dot-Product Attention(SDPA)—consistently improves performance. This modification also enhances training sta-bility, tolerates larger learning rates, and improves scaling properties. By comparingvarious gating positions and computational variants, we attribute this effectiveness totwo key factors: (1) introducing non-linearity upon the low-rank mapping in the softmaxattention, and (2) applying query-dependent sparse gating scores to modulate the SDPAoutput. Notably, we find this sparse gating mechanism mitigates ‘attention sink’ andenhances long-context extrapolation performance, and we also release related codes andmodels to facilitate future research.

# 1 Introduction

Gating mechanism is well-established in neural networks. Early architectures, such as LSTMs (Hochreiter& Schmidhuber, 1997), Highway Networks (Srivastava et al., 2015) and GRUs (Dey & Salem, 2017),pioneer the use of gating to control information flow across time steps or layers and improve gradientpropagation. This principle persists in modern architectures. Recent sequence modeling works, includingstate-space models (Gu & Dao, 2023; Dao & Gu, 2024) and attention mechanisms (Hua et al., 2022; Sunet al., 2023; Qin et al., $2 0 2 4 \mathsf { a }$ ; Yang et al., 2024b; Lin et al., 2025) commonly apply gating, often to modulatethe outputs of token-mixer components. Despite its widespread adoption and empirical success, thefunction and impact of gating mechanisms remain insufficiently explored beyond their initial intuition.

Insufficient understanding hinders assessing gating’s true contribution, especially when confounded withother architectural factors. For instance, while Switch Heads (Csordas et al., 2024a;b) introduces a sigmoidgating to select top-K attention head experts, our experiments reveal an interesting finding (Appendix A.1):substantial performance gains persist even when reduced to a single expert, where the gate simplymodulates the value output. This strongly suggests the gating itself provides significant intrinsic value,separate from the routing mechanism. Similarly, in Native Sparse Attention (NSA) (Yuan et al., 2025),while overall performance improvements are demonstrated, they do not disentangle the contributions ofits gating mechanism from the effects of the sparse attention design itself. These considerations underscorethe need to rigorously disentangle the effects of gating from other architectural components.

In this work, we investigate gating mechanisms in the standard softmax attention (Vaswani, 2017) (Sec.2.2).Specifically, we introduce gating at distinct positions (Fig. 1): after the query $\left( G _ { 4 } \right)$ , key $\left( G _ { 3 } \right)$ , and valueprojections $\left( G _ { 2 } \right)$ ; following the Scaled Dot Product Attention (SDPA) outputs $\left( G _ { 1 } \right)$ ; and after the finaldense output layer $\left( G _ { 5 } \right)$ . Our exploration covers gating variants including elementwise and headwise,head-specific and head-shared, as well as additive and multiplicative forms. We find that: (i) applyingSDPA output head-specific gating $\left( G _ { 1 } \right)$ yields the most significant performance improvements (e.g., up to0.2 PPL reduction and 2 points on MMLU); (ii) the SDPA output gating also improves training stability,nearly eliminating loss spikes, enabling larger learning rates and enhancing model scalability.

We identify two primary factors contributing to the efficacy of gating: (i) Non-Linearity. The twoconsecutive linear layers - the value $\left( W _ { v } \right)$ and dense $( W _ { O } )$ projections - can be rewritten into one low-rank

![](images/e8c0a246b832fcb1a8cf2ce7a6f0ab5ccb4e107a23e4eefae0c3890215ce74e2.jpg)


![](images/0f94cf942d16dc1fe074d2744fe4fc6f9ce72ad83f5c709952d4386eb5962f2b.jpg)


![](images/6dd576eab695f581f90b61f02f16d73546142839acfdde7f0d341d3b3ebd49ce.jpg)



Figure 1: Left: Investigated positions for applying gating operations.; Middle: Performance comparison (Test PPLand MMLU) of 15B MoE models with gating applied at various positions. Gating after SDPA $( \dot { G _ { 1 } } )$ yields the bestoverall results. Gating after the Value layer $\left( \hat { G } _ { 2 } \right)$ also demonstrates notable improvements, particularly in PPL. Right:Training loss comparison (smoothed, 0.9 coeff.) over 3.5T tokens between baseline and SDPA-gated 1.7B densemodels under identical hyperparameters. Gating results in lower final loss and substantially enhanced trainingstability, mitigating loss spikes. This stability allows for potentially higher learning rates and facilitates better scaling.


linear projection. Therefore, introducing non-linearity through gating at positions $G _ { 1 }$ or $G _ { 2 }$ can increasethe expressiveness of this low-rank linear transformation (Sec. 4.1). (ii) Sparsity. Although non-lineargating variants consistently enhance performance, we observe that their gains vary. Our analysis furtherreveals that the pronounced sparsity of the gating scores is another crucial factor, introducing input-dependent sparsity to SDPA outputs (Sec. 4.2). Moreover, sparse gating eliminates the attention sink (Xiaoet al., 2023): the initial tokens disproportionately dominate attention scores (Fig. 2, Sec. 4.3). Previouswork (Xiao et al., 2023; Sun et al., 2024; Gu et al., 2024) explains attention sinks as an accumulationof redundant attention due to non-negative softmax normalization. Empirically, we verify that whenquery-dependent sparse gating is applied at the SDPA output, both our dense and MoE models (trained on3.5T tokens) exhibit no attention sink. Furthermore, these models demonstrate superior performance inlength generalization, achieving a gain of over 10 points on RULER (Hsieh et al., 2024)(Sec.4.4).

In summary, our work highlights the impact of gating in standard attention layers on the performance andbehaviors of models. By evaluating gating variants, we uncover their ability to introduce non-linearityand sparsity, and eliminate attention sinks. These findings deepen our understanding of the mechanismsof gated attention. We will open-source our attention-sink-free models to advance future research.

# 2 Gated-Attention Layer

# 2.1 Preliminary: Multi-Head Softmax Attention

Given an input $X \in \mathbb { R } ^ { n \times d _ { \mathrm { m o d e l } } }$ , where $n$ is the sequence length and $d _ { \mathrm { m o d e l } }$ is the model dimension, thecomputation of transformer’s attention layer (Vaswani, 2017) could be divided into four stages.

QKV Linear Projections: The input $X$ is linearly transformed into queries $Q$ , keys $K _ { \cdot }$ , and values $V$using learned weight matrices $W _ { Q } , \bar { W } _ { K } , W _ { V } \in \bar { \mathbb { R } ^ { d _ { \mathrm { m o d e l } } \times d _ { k } } }$ and $Q , K , V \in \mathbb { R } ^ { n \times d _ { k } }$ :

$$
Q = X W _ {Q}, \quad K = X W _ {K}, \quad V = X W _ {V}. \tag {1}
$$

Scaled Product Dot-Product Attention (SDPA): computes attention scores between queries and keys,followed by a softmax normalization. The output is a weighted sum of the values:

$$
\operatorname {A t t e n t i o n} (Q, K, V) = \operatorname {s o f t m a x} \left(\frac {Q K ^ {T}}{\sqrt {d _ {k}}}\right) V, \tag {2}
$$

where QKT $\frac { Q K ^ { T } } { \sqrt { d _ { k } } } \in \mathbb { R } ^ { n \times n }$ represents the scaled dot-product similarity matrix, and softmax(·) ensures theattention weights are no-negative and sum to 1 across each row.

Multi-Head Concatenation: In multi-head attention, the above process is repeated in parallel for $h$heads, with each head having its projection matrices $W _ { q } ^ { i } , W _ { k } ^ { i } , W _ { v } ^ { i }$ . All heads’ outputs are concatenated:

$$
\operatorname {M u l t i H e a d} (Q, K, V) = \operatorname {C o n c a t} \left(\operatorname {h e a d} _ {1}, \dots , \operatorname {h e a d} _ {h}\right), \tag {3}
$$

where headi = Attention(QW iQ, KW iK , V W iV ).

![](images/82c277ae9eace077e219c03feb86abb6fc9dc23b39e0ceee40531825cde2a50d.jpg)


![](images/0ebfd8d99f57a22efdb00f5db2b3fed642695b7f7ab44fc7a2164be70637f9b3.jpg)


![](images/c893ea0cab07772ed9e2ad0a690018062c9015c4abf5630fe1a18de90a2751c2.jpg)


![](images/a7bb646f5f01ed5aa3c71ae75c66ca6272eeee75d4c4330caeff02dbc47f5f22.jpg)


![](images/7e8e0f17e3044264f94bc44a312abeb6f79d5b00534accc58efc88f52c06f0f9.jpg)


![](images/242c201ca53d0f1c0512a086d7c5cafddd9439f70f652c16409aa24d4454a845.jpg)



Figure 2: Left: Proportion of attention allocated to the initial token per layer (test perplexity dataset). The baselinemodel suffers from a significant attention sink, with an average of $\bar { 4 } 6 . 7 \%$ of attention scores across layers directedtowards the first token. Introducing a gate effectively alleviates this, reducing the proportion to $4 . 8 \%$ . Right: Averageattention map weights for each head. Layer 21 in the baseline model demonstrates a strong attention sink ( $8 3 \%$ on thefirst token), which is substantially reduced by the gate $( 4 \% )$ . In the final output layer, the gate amplifies the existingtendency for the model to attend to individual tokens within the sequence.


Final Output Layer: The concatenated SDPA output is passed through an output layer $W _ { o } \in \mathbb { R } ^ { h d _ { k } \times d _ { \mathrm { m o d e l } } }$

$$
O = \operatorname {M u l t i H e a d} (Q, K, V) W _ {o}. \tag {4}
$$

# 2.2 Augmenting Attention Layer with Gating Mechanisms

The gating mechanism is formalized as:

$$
Y ^ {\prime} = g (Y, X, W _ {\theta}, \sigma) = Y \odot \sigma \left(X W _ {\theta}\right), \tag {5}
$$

where $Y$ is the input to be modulated, $X$ is another input used to compute the gating scores1, $W _ { \theta }$ refersto the learnable parameters of gate, $\sigma$ is an activation function (e.g., sigmoid), and $Y ^ { \prime }$ is the gated output.The gating score, $\sigma ( X W _ { \theta } ) .$ , effectively acts as a dynamic filter, controlling the information flow from $\bar { Y }$ byselectively preserving or erasing its features.

In this work, we comprehensively investigate several variants of gating mechanisms within the attentionlayers. Our exploration focuses on five key aspects: (1) Positions. We study the effect of applying gatingat different positions, as illustrated in Fig. 1(left): (a) after the $Q , K , V$ projections (Equ. 1), correspondingto positions $G _ { 2 } , G _ { 3 } , G _ { 4 }$ in Fig. 1(left); (b) following the SDPA (Equ. 3) outputs $\left( \hat G _ { 1 } \right)$ . (c) after the finalconcatenated multi-head attention outputs (Equ. 4, $G _ { 5 }$ ). (2) Granularity. We consider two levels ofgranularity for the gating score: (a) Headwise: A single scalar gating score modulates the entire output ofan attention head. (b) Elementwise: Gating scores are vectors with the same dimensionality as $Y$ , enablingfine-grained, per-dimension modulation. (3) Head Specific or Shared. Given the multi-head natureof attention, we further consider: (a) Head-Specific: each attention head has its specific gating scores,enabling independent modulation for each head. (b) Head-Shared: $W _ { \theta }$ and gating scores are shared acrossheads. (4) Multiplicative or additive. For applying gating score to $Y _ { . }$ , we consider (a) MultiplicativeGating: The gated output $Y ^ { \prime }$ is computed as: $\grave { Y } ^ { \prime } \grave { = } Y \cdot \sigma ( X \theta )$ . (b) Additive Gating: $\boldsymbol { Y } ^ { \prime } \dot { = } \dot { \boldsymbol { Y } } + \sigma \dot { ( \boldsymbol { X } \theta ) }$ . (5)Activation Function. We mainly consider two common activation functions: SiLU (Shazeer, 2020) andsigmoid. We only use SiLU for additive gating due to its unbounded output range, and sigmoid onlygives scores in $[ 0 , 1 ]$ . Additionally, to further dissect the mechanisms underlying gating’s effectiveness,we also consider Identity Mapping or RMSNorm (Zhang & Sennrich, 2019) (detailed in Sec 4.1).

Unless otherwise specified, we employ head-specific, multiplicative gating utilizing the sigmoid activationfunction $\begin{array} { r } { ( \sigma ( x ) = \frac { 1 } { 1 + e ^ { - x } } } \end{array}$ ).


Table 1: Gating variant performance and results. We train the 15A2B MoE models on 400B tokens. $d _ { k }$ is the headdim, $d _ { \mathrm { m o d e l } }$ is the model’s hidden dim, and $n$ is the number of tokens. $q$ refers to the number of query heads, $k$ refersto the number of key-value heads. ‘Act Func’ is the activation function in Eq 5. ‘Score Shape’ is the gating score shapefor an input $X \in \mathbb { R } ^ { n , d _ { \mathrm { m o d e l } } }$ . ‘added param’ indicates added parameters (Million).


<table><tr><td>Method</td><td>Act Func</td><td>Score Shape</td><td>Added Param</td><td>Avg PPL</td><td>Hellaswag</td><td>MMLU</td><td>GSM8k</td><td>C-eval</td></tr><tr><td colspan="9">Reference Baselines (Baseline uses q = 32, k = 4. All methods use dk = 128.)</td></tr><tr><td>(1) Baseline</td><td>-</td><td>-</td><td>0</td><td>6.026</td><td>73.07</td><td>58.79</td><td>52.92</td><td>60.26</td></tr><tr><td>(2) k = 8</td><td>-</td><td>-</td><td>50</td><td>5.979</td><td>73.51</td><td>59.78</td><td>52.16</td><td>62.26</td></tr><tr><td>(3) q = 48</td><td>-</td><td>-</td><td>201</td><td>5.953</td><td>73.59</td><td>58.45</td><td>53.30</td><td>59.67</td></tr><tr><td>(4) Add 4 Experts</td><td>-</td><td>-</td><td>400</td><td>5.964</td><td>73.19</td><td>58.84</td><td>52.54</td><td>63.19</td></tr><tr><td colspan="9">Gating Position Variants</td></tr><tr><td>(5) SDPA Elementwise G1</td><td>sigmoid</td><td>n × q × dk</td><td>201</td><td>5.761</td><td>74.64</td><td>60.82</td><td>55.27</td><td>62.20</td></tr><tr><td>(6) v Elementwise G2</td><td>sigmoid</td><td>n × k × dk</td><td>25</td><td>5.820</td><td>74.38</td><td>59.17</td><td>53.97</td><td>61.00</td></tr><tr><td>(7) k Elementwise G3</td><td>sigmoid</td><td>n × k × dk</td><td>25</td><td>6.016</td><td>72.88</td><td>59.18</td><td>50.49</td><td>61.74</td></tr><tr><td>(8) q Elementwise G4</td><td>sigmoid</td><td>n × q × dk</td><td>201</td><td>5.981</td><td>73.01</td><td>58.74</td><td>53.97</td><td>62.14</td></tr><tr><td>(9) Dense Output G5</td><td>sigmoid</td><td>n × dmodel</td><td>100</td><td>6.017</td><td>73.32</td><td>59.41</td><td>50.87</td><td>59.43</td></tr><tr><td colspan="9">Gating Granularity Variants</td></tr><tr><td>(10) SDPA Headwise G1</td><td>sigmoid</td><td>n × q</td><td>1.6</td><td>5.792</td><td>74.50</td><td>60.05</td><td>54.44</td><td>62.61</td></tr><tr><td>(11) v Headwise G2</td><td>sigmoid</td><td>n × q</td><td>0.2</td><td>5.808</td><td>74.38</td><td>59.32</td><td>53.53</td><td>62.61</td></tr><tr><td colspan="9">Head-Specific vs. Head-Shared Gating</td></tr><tr><td>(12) SDPA Head-Shared G1</td><td>sigmoid</td><td>n × dk</td><td>201</td><td>5.801</td><td>74.34</td><td>60.06</td><td>53.15</td><td>61.01</td></tr><tr><td>(13) v Head-Shared G2</td><td>sigmoid</td><td>n × dk</td><td>25</td><td>5.867</td><td>74.10</td><td>59.02</td><td>53.03</td><td>60.61</td></tr><tr><td colspan="9">Multiplicative vs. Additive</td></tr><tr><td>(14) SDPA Additive G1</td><td>SiLU</td><td>n × q × dk</td><td>201</td><td>5.821</td><td>74.81</td><td>60.06</td><td>53.30</td><td>60.98</td></tr><tr><td colspan="9">Activation Variants</td></tr><tr><td>(15) SDPA Elementwise G1</td><td>SiLU</td><td>n × q × dk</td><td>201</td><td>5.822</td><td>74.22</td><td>60.49</td><td>54.59</td><td>62.34</td></tr></table>

# 3 Experiments

# 3.1 Experimental Setups

Model Architecture and Training Settings We conduct experiments on both MoE models (15B total pa-rameters with 2.54B activated, 15A2B) and dense models (1.7B total parameters). The 15A2B MoE modelsutilize 128 total experts with top-8 softmax gating, fine-grained experts (Dai et al., 2024), global-batchLBL (Qiu et al., 2025), and z-loss (Zoph et al., 2022). We adopt group query attention (GQA) (Ainslieet al., 2023) for the attention part. We train the models on subsets of a 3.5T high-quality tokens, encom-passing multilingual, math, and general knowledge content. The context sequence length is set to 4096.More detailed configurations, such as learning rate and batch size (bsz), will be introduced in each part.Other hyperparameters follow the default values of the AdamW optimizer. Since the parameters andflops introduced by the gating are small, the wall-time latency introduced by gating is less than $2 \%$ .

Evaluation We test the few-shots results on popular benchmarks, including Hellaswag (Zellers et al.,2019) for English, MMLU (Hendrycks et al., 2020) for general knowledge, GSM8k (Cobbe et al., 2021) formath reasoning, HumanEval (Chen et al., 2021) for coding, C-eval (Huang et al., 2024) and CMMLU (Liet al., 2023) for Chinese proficiency. We also report the perplexity (PPL) of language modeling on diverseheld-out test sets, including domains like English, Chinese, Code, Math, Law, and Literature.

# 3.2 Main Results

# 3.2.1 Gated Attention for MoE models

We first compare the results of different gated attention layers on the training-efficient MoE-15A2B models.All models use a scheduler that warms up to a maximum LR of 2e-3 in 1k steps and decays using cosineto 3e-5. We use a global bsz of 1024, comprising 100k optimization steps. The results are summarized inTab. 1. To provide a fair comparison, we supplement the vanilla MoE baseline (row 1) with parameterexpansion methods, including increasing the number of key-value heads (row 2), increasing the numberof query heads (row 3), and increasing both the total and activated number of experts (row 4). Thesemethods introduce a comparable or greater number of parameters than the gating mechanisms.

From Tab. 1, we observe: (i) SDPA and value output gating are effective. Inserting gates at the output ofSDPA $\left( G _ { 1 } \right)$ or the value map $\left( G _ { 2 } \right)$ is the most effective, achieving lower PPL and better overall benchmarkperformance than other variants. We will further investigate why gating at these two positions is effectivein Sec 4.2. (ii) Head-Specific Gating Matters. Applying headwise gating at $G _ { 1 }$ and $G _ { 2 }$ introduces very fewadditional parameters (less than 2M for the MoE-15A2B model) but still delivers substantial improvements(rows 10 and 11). When sharing gating scores across different attention heads (we average over the query


Table 2: Performance of different methods with varying learning rates, batch sizes, and model configurations. ‘SDPA’refers to the sigmoid gating after SDPA in Eq 3, and ‘sandwitch norm’ (Ding et al., 2021) indicates normalizingattention/ffn outputs before adding them to the residual. When using gating, we reduce the FFN’s width so that allmethods have the same number of parameters. ‘-’ means the model diverges during training.


<table><tr><td>Method</td><td>Max LR</td><td>Avg PPL</td><td>HumanEval</td><td>MMLU</td><td>GSM8k</td><td>Hellaswag</td><td>C-eval</td><td>CMMLU</td></tr><tr><td colspan="9">28 Layer, 1.7B Parameters, 400B Tokens, Batch Size=1024</td></tr><tr><td>(1) Baseline</td><td>4.0 × 10-3</td><td>7.499</td><td>28.66</td><td>50.21</td><td>27.82</td><td>64.94</td><td>49.15</td><td>49.52</td></tr><tr><td>(2) SDPA Elementwise</td><td>4.0 × 10-3</td><td>7.404</td><td>29.27</td><td>51.15</td><td>28.28</td><td>65.48</td><td>50.72</td><td>50.72</td></tr><tr><td colspan="9">28 Layer, 1.7B Parameters, 3.5T Tokens, Batch Size=2048</td></tr><tr><td>(3) Baseline</td><td>4.5 × 10-3</td><td>6.180</td><td>34.15</td><td>59.10</td><td>69.07</td><td>68.02</td><td>68.19</td><td>64.95</td></tr><tr><td>(4) SDPA Elementwise</td><td>4.5 × 10-3</td><td>6.130</td><td>37.80</td><td>59.61</td><td>70.20</td><td>68.84</td><td>68.52</td><td>65.76</td></tr><tr><td colspan="9">48 Layer, 1.7B Parameters, 400B Tokens, Batch Size=1024</td></tr><tr><td>(5) Baseline</td><td>4.0 × 10-3</td><td>7.421</td><td>28.05</td><td>52.04</td><td>32.98</td><td>65.96</td><td>51.11</td><td>51.86</td></tr><tr><td>(6) Baseline</td><td>8.0 × 10-3</td><td>9.195</td><td>21.34</td><td>44.28</td><td>15.24</td><td>57.00</td><td>43.11</td><td>42.63</td></tr><tr><td>(7) Baseline+Sandwich Norm</td><td>8.0 × 10-3</td><td>7.407</td><td>30.49</td><td>52.07</td><td>32.90</td><td>66.00</td><td>52.04</td><td>51.72</td></tr><tr><td>(8) SDPA Elementwise</td><td>4.0 × 10-3</td><td>7.288</td><td>31.71</td><td>52.44</td><td>32.37</td><td>66.28</td><td>52.06</td><td>52.29</td></tr><tr><td>(9) SDPA Headwise</td><td>4.0 × 10-3</td><td>7.370</td><td>31.10</td><td>53.83</td><td>34.12</td><td>65.59</td><td>55.07</td><td>52.38</td></tr><tr><td>(10) SDPA Elementwise</td><td>8.0 × 10-3</td><td>7.325</td><td>31.10</td><td>54.47</td><td>36.62</td><td>66.40</td><td>53.91</td><td>53.80</td></tr><tr><td colspan="9">48 Layer, 1.7B Parameters, 1T Tokens, Batch Size=4096</td></tr><tr><td>(11) Baseline</td><td>5.3 × 10-3</td><td>7.363</td><td>29.88</td><td>54.44</td><td>32.22</td><td>65.43</td><td>53.72</td><td>53.37</td></tr><tr><td>(12) Baseline</td><td>8.0 × 10-3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>(13) SDPA Elementwise</td><td>5.3 × 10-3</td><td>7.101</td><td>34.15</td><td>55.70</td><td>36.69</td><td>67.17</td><td>54.51</td><td>54.68</td></tr><tr><td>(14) SDPA Elementwise</td><td>8.0 × 10-3</td><td>7.078</td><td>31.71</td><td>56.47</td><td>39.73</td><td>67.38</td><td>55.52</td><td>55.77</td></tr></table>

head dimension $q$ to obtain an $n \times d _ { k }$ score from the original $n \times q \times d _ { k }$ ), the benchmark improvementsare smaller than those achieved by headwise gating (row 12 v.s. 10, 13 v.s. 11). This underscores theimportance of applying distinct gating scores for different attention heads. (iii) Multiplicative Gatingis Preferred. Additive SDPA output gating underperforms the multiplicative one, although it showsimprovements over the baselines. (iv) Sigmoid Activation is Better. Replacing the activation function inthe most effective gating configuration (row 5) with SiLU (row 15) leads to less improvement.

Overall, adding gating at the value layer $\left( G _ { 2 } \right)$ and SDPA output $\left( G _ { 1 } \right)$ reduces PPL by more than 0.2,outperforming various parameter-expanding baselines. However, gating at $G _ { 1 }$ achieves better PPL andbenchmark results. As long as different heads receive distinct gating scores, the granularity of gatingand the choice of activation function have relatively minor impacts. We will further analyze the reasonsbehind these observations in Analysis (Sec 4.2).

# 3.2.2 Gated Attention for Dense Models.

We also conduct experiments on dense models following (Yang et al., 2024a) to validate SDPA outputsigmoid gating. When using gating, we reduce the width of FFN to maintain the parameter size. Mostexperiments use optimized hyperparameters for the baseline. For instance, for the 1.7B model trained on400B tokens, we use a maximum LR of 4e-3 and a bsz of 1024. For training on 3.5T tokens, we increase themaximum LR to 4.5e-3 and the bsz to 2048. Prior work has established that while increased network depth,large learning rates, and large batch sizes can significantly improve model performance (McCandlishet al., 2018; Wang et al., 2022; D’Angelo et al., 2024) and distributed training efficiency, they often introducetraining instabilities (Wang et al., 2022; Zeng et al., 2022; Takase et al., 2023). We observe that applyinggating mechanisms demonstrably reduces the occurrence of loss spikes during training (Chowdhery et al.,2023; Takase et al., 2023), suggesting a promising role for gating in enhancing training stability. Motivatedby this finding, we introduce another experimental setting characterized by an increased number of layers,a higher maximum learning rate, and a larger batch size to further probe gating’s stabilizing effects.

Tab. 2 reveals that: (i) Gating is effective across various settings Across various model configurations(row 1 v.s. 2, 5 v.s. 8), training data (row 3 v.s. 4), and hyperparameters (row 11 v.s. 13), applyingSDPA output gating consistently yields benefits. (ii) Gating improves stability and facilitates scaling.Under the $3 . 5 \mathrm { \check { T } }$ token setting, gating improves training stability, largely reducing the loss spike (Fig. 1,right). When increasing the maximum LR, baselines encounter convergence issues (row 6, 12). Whileadding sandwich norm (Ding et al., 2021) restores convergence, the improvement is negligible. In contrast,increasing the maximum LR in models with gating results in a noticeable improvement.

In summary, we identify SDPA element-wise gating as the most effective method to augment the attentionmechanism. Applying this method to dense transformers further demonstrates that the gate enables stabletraining with larger batch sizes and learning rates, resulting in improved performance.


Table 3: Performance of different (non)-linearity augmentations.


<table><tr><td>Method</td><td>Activation Function</td><td>Avg PPL</td><td>Hellaswag</td><td>MMLU</td><td>GSM8k</td><td>C-eval</td></tr><tr><td>(1) Baseline</td><td>-</td><td>6.026</td><td>73.07</td><td>58.79</td><td>52.92</td><td>60.26</td></tr><tr><td>(2) SDPA Elementwise Gate</td><td>Sigmoid</td><td>5.761</td><td>74.64</td><td>60.82</td><td>55.27</td><td>62.20</td></tr><tr><td>(3) v Elementwise Gate</td><td>Sigmoid</td><td>5.820</td><td>74.38</td><td>59.17</td><td>53.97</td><td>61.00</td></tr><tr><td>(4) SDPA Additive Gate</td><td>SiLU</td><td>5.821</td><td>74.81</td><td>60.06</td><td>53.30</td><td>60.98</td></tr><tr><td>(5) SDPA GroupNorm</td><td>RMSNorm</td><td>5.847</td><td>74.10</td><td>60.15</td><td>53.75</td><td>61.14</td></tr><tr><td>(6) SDPA SiLU</td><td>SiLU</td><td>5.975</td><td>73.34</td><td>59.55</td><td>53.19</td><td>60.90</td></tr><tr><td>(7) SDPA Additive Gate</td><td>Identity</td><td>5.882</td><td>74.17</td><td>59.20</td><td>52.77</td><td>59.86</td></tr></table>

# 4 Analysis: Non-Linearity, Sparsity, and Attention-Sink-Free

In this section, we conduct a series of experiments to explore why such a simple gating mechanism canyield significant improvements in performance and training stability. Here are the takeaways accordingto our analysis: (1) Gating operations enhancing non-linearity consistently lead to performance gains(Sec 4.1); (2) The most effective SDPA elementwise $G _ { 1 }$ gate introduces strong input-dependent sparsityto the SDPA outputs (Sec 4.2), which then helps to eliminate the ‘attention sink’ phenomenon.

# 4.1 Non-linearity Improves the Expressiveness of Low-Rank Mapping in Attention

Inspired by prior works that utilize group norm for the SDPA output (Sun et al., 2023; Ye et al., 2024),with the same setting in Sec. 3.2.1, we apply RMSNorm (Zhang & Sennrich, 2019) independently to theoutput of each attention head before concatenation. As shown in Tab. 3 row 5, applying RMSNorm, whichintroduces almost no additional parameters, also leads to a significant reduction in PPL.

In multi-head attention, the output of the $i$ -th token, corresponding to the $k$ -th head, can be expressed as:

$$
o _ {i} ^ {k} = \left(\sum_ {j = 0} ^ {i} S _ {i j} ^ {k} \cdot X _ {j} W _ {V} ^ {k}\right) W _ {O} ^ {k} = \sum_ {j = 0} ^ {i} S _ {i j} ^ {k} \cdot X _ {j} \left(W _ {V} ^ {k} W _ {O} ^ {k}\right), \tag {6}
$$

where $W _ { O } ^ { k }$ is the parameters of the output layer $W _ { O }$ corresponding to the $k$ -th head2. Here, $S _ { i j } ^ { k }$ denotesthe attention score of the $i$ -th token attending to the $j$ -th token in the $k$ -th head, $X _ { j }$ is the input to theattention for token $j ,$ , and $X _ { j } W _ { V } ^ { k }$ represents the value output of token $j$ in the $k$ -th head. From Equ. 6,we can merge $W _ { \scriptscriptstyle . V } ^ { k } W _ { \cal O } ^ { k }$ into one low-rank linear mapping applied over all $X _ { j }$ as $d _ { k } < d _ { m o d e l }$ . With GQA, $W _ { V }$ isshared among heads within the same group, further diminishing the expressiveness.

Given that adding non-linearity between two linear mappings can improve their expensiveness (Montufaret al., 2014), we have two modifications to mitigate the low-rank problem:

$$
o _ {i} ^ {k} = \left(\sum_ {j = 0} ^ {i} S _ {i j} ^ {k} \cdot \text {N o n - L i n e a r i t y - M a p} \left(X _ {j} W _ {V} ^ {k}\right)\right) W _ {O} ^ {k}, \tag {7}
$$

$$
o _ {i} ^ {k} = \text {N o n - L i n e a r i t y - M a p} \left(\sum_ {j = 0} ^ {i} S _ {i j} ^ {k} \cdot X _ {j} W _ {V} ^ {k}\right) W _ {O} ^ {k}. \tag {8}
$$

Notably, adding gating at the $G _ { 2 }$ (Tab. 3 row 3) position corresponds to the first modification (Equ. 7),while adding gating (row 4) or group normalization (row 5) at the $G _ { 1 }$ position corresponds to the second(Equ. 8). This also explains why adding gating or normalization at the $G _ { 5 }$ position after $W _ { O }$ has no effect(Tab. 1 row 9) — it does not address the lack of non-linearity between $\bar { W _ { V } } ^ { - }$ and $W _ { O }$ .

For additive gating at $G _ { 1 } ,$ the output of gating passes through SiLU (Tab. 3 row 4), also introducingsome non-linearity, which explains the observed performance gains, albeit smaller than those achieved bymultiplicative gating. Based on these insights, we conduct two additional experiments: (i) Adding SiLUonly at the $G _ { 1 }$ position without introducing additional parameters (Tab. 3 row 6). Notice this simplemodification also leads to a modest reduction in PPL, but most benchmark scores remain unchanged. (ii)Removing SiLU from additive gating, such that the output of $X _ { j }$ after gating is directly added at the $G _ { 1 }$position (Tab. 3 row 7). This further diminishes the gains of addictive gating.

In summary, the enhanced performance associated with effective gating variants is likely attributable tothe introduction of non-linearity between $W _ { V }$ and $W _ { O }$ . Although applying gating at positions $G _ { 1 }$ and$G _ { 2 }$ can can both introduce this non-linearity, these applications yield differing performance gains. Thisobserved difference motivates us to further analyze the impacts of gating at these two positions.


Table 4: Performance of different gating methods with varying activation functions and average gate scores. ‘Act-Func’refers to the activation function used for computing the gating scores, while ‘M-Act’ denotes the rounded maximumactivation values of the hidden states output by each layer of the model. Additionally, ‘F-Attn’ represents the attentionscore of the first token, with higher values indicating more pronounced ‘attention sink’.


<table><tr><td>Method</td><td>Act-Func</td><td>Gate Score</td><td>M-Act</td><td>F-Attn</td><td>PPL</td><td>Hellaswag</td><td>MMLU</td><td>GSM8k</td></tr><tr><td>(1) Baseline</td><td>-</td><td>-</td><td>1053</td><td>0.467</td><td>6.026</td><td>73.07</td><td>58.79</td><td>52.92</td></tr><tr><td>(2) SDPA Elementwise Gate</td><td>Sigmoid</td><td>0.116</td><td>94</td><td>0.048</td><td>5.761</td><td>74.64</td><td>60.82</td><td>55.27</td></tr><tr><td>(3) SDPA Headwise Gate</td><td>Sigmoid</td><td>0.172</td><td>98</td><td>0.073</td><td>5.792</td><td>74.50</td><td>60.05</td><td>54.44</td></tr><tr><td>(4) SDPA Elementwise Head-shared Gate</td><td>Sigmoid</td><td>0.271</td><td>286</td><td>0.301</td><td>5.801</td><td>74.34</td><td>60.06</td><td>53.15</td></tr><tr><td>(5) v Elementwise Gate</td><td>Sigmoid</td><td>0.221</td><td>125</td><td>0.297</td><td>5.820</td><td>74.38</td><td>59.17</td><td>51.33</td></tr><tr><td>(6) SDPA Input Independent Gate</td><td>Sigmoid</td><td>0.335</td><td>471</td><td>0.364</td><td>5.917</td><td>73.64</td><td>59.02</td><td>52.40</td></tr><tr><td>(7) SDPA Elementwise Gate</td><td>NS-sigmoid</td><td>0.653</td><td>892</td><td>0.451</td><td>5.900</td><td>74.05</td><td>60.05</td><td>52.75</td></tr></table>

![](images/58d51a93e2e0fe638367ed012e5f5b577c417f5505c78096dee61355fa9cd47d.jpg)


![](images/b135af61d0c82241ba0461aeac04ff4d9c241b57ff559e73481b727233b7e5cd.jpg)


![](images/71a0221ae6dc6823ead766c348b383e54319376c11b035eaac19e8f66c41f44d.jpg)



Figure 3: Gating score means and distributions for SDPA elementwise (Left), value Elementwise (Middle), andSDPA elementwise with head-shared gating (Right). Most gating scores are less than 0.5, indicating that the gatingscores are sparse. Among them, the SDPA output gating score exhibits the strongest sparsity.


# 4.2 Gating Introduces Input-Dependent Sparsity

We analyze the gating scores (Tab. 1, ‘Gate Score’ column) of models with gating applied at the value$\left( G _ { 2 } \right)$ and SDPA output $\left( G _ { 1 } \right)$ positions, evaluated on the test language modeling data. The mean gatingscores for all layers are presented in Table 4, with the score distributions visualized in Fig. 3 (layer-wisescores in Appendix A.2). Key observations include:

(i) Effective Gating Scores are Sparse. SDPA output gatings (Element/head-wise) exhibit the lowestmean gating scores. Furthermore, the SDPA output gating score distribution shows a high concentrationnear 0, indicating substantial sparsity, consistent with its superior performance. (ii) Head-SpecificSparsity Matters. Enforcing shared gating scores across attention heads increases the overall gating scoresand diminishes performance gains. Observations (i) and (ii) underscore the importance of head-specificgating, aligning with previous research demonstrating that individual attention heads capture distinctaspects of the input (Voita et al., 2019; Wang et al., 2021; Olsson et al., 2022; Wang et al., 2023).

(iii) Query-Dependency Matters. The scores for value gating $\left( G _ { 2 } \right)$ are higher than those for SDPA outputgating $\left( G _ { 1 } \right)$ , and the performance is inferior. This suggests that gating score sparsity is more effectivewhen query-dependent rather than determined by the key and value. Specifically, SDPA output gatingscores are derived from the hidden states corresponding to the current query (e.g. the Non-Linearity-Mapin Eq 8 depends on $X _ { i }$ ), whereas value gating scores are derived from hidden states associated withpast keys and values (e.g. the Non-Linearity-Map in Eq 7 depends on each $X _ { j }$ ). This implies that gatingscore sparsity may filter out irrelevant contextual information for the query. To further validate the importanceof query-dependency, we introduce input-independent gating by zero-initializing learnable parameters$( q \dot { \times } d _ { k } \dot { ) }$ , applying a sigmoid function, and multiplying it with the SDPA output. As shown in row (6),input-independent gating improves upon the baseline, likely due to the introduction of non-linearity.Moreover, the high gating scores reinforce that effective sparsity should be input-dependent.

(iv) Less Sparse Gating is Worse. To further validate the importance of gating score sparsity, we reducesparsity from the gating formulation. Specifically, we replace the sigmoid function with a modifiedNon-Sparse (NS) version:

$$
\mathrm {N S - s i g m o i d} (x) = 0. 5 + 0. 5 \cdot \operatorname {s i g m o i d} (x),
$$

which constrains the gating scores between [0.5, 1.0]. This ensures introducing non-linearity whileremoving gating score sparsity. As shown in Tab. 4 row (7), the gains of NS-sigmoid gating are inferior tothose of SDPA output sigmoid gating. In Appendix A.2, we provide a more detailed discussion on howsparse gating scores affect the sparsity (the proportion of values below the threshold) in SDPA hiddenstates. We will discuss the impact of different sparsity levels on model behavior, including reducing the‘attention sink’, in the next section.

# 4.3 SDPA Output Gating Reduces Attention-Sink

Based on the observation that gating introduces sparsity to the SDPA output in an input-dependentmanner, we hypothesized that this mechanism can filter out context irrelevant to the current query token,thereby mitigating the attention sink (Xiao et al., 2023; Sun et al., 2024). To verify this, we analyze thedistribution of attention scores (averaged over all heads) and the proportion of attention scores allocatedto the first token (Fig. 2, Tab. 4, ‘F-Attn’ column). Inspired by the discussion about massive activation inhidden states and attention sinks (Sun et al., 2024), we also compute the mean of the maximum hiddenstate activations across layers, as shown in the ‘M-Act’ column of Tab. 4. More detailed layer-wise resultsare provided in the Appendix A.3.

We can observe: (i) Head-wise and element-wise query-dependent sigmoid gating at the SDPA output$\left( G _ { 1 } \right)$ largely reduces the attention score allocated to the first token and decreases massive activations.(ii) Enforcing shared gating scores across heads or applying gating only after the value projection $\left( G _ { 2 } \right)$decreases massive activations, but does not reduce attention scores to the first token. This reinforces theimportance of head-specific gating and suggests that massive activations are not a prerequisite for attentionsinks. (iii) Reducing the input-dependence of gating (row 6) or using NS-sigmoid to reduce sparsity(row 7) intensifies both massive activations and attention sink.

Collectively, these observations indicate that input-dependent, head-specific gating of the SDPA output intro-duces significant sparsity, thereby mitigating the attention sink. Furthermore, sparsity in the SDPA outputsreduces massive activations within the model, with increased sparsity leading to smaller activations.This may explain the improved training stability with gating: by reducing massive activations, the model is lesssusceptible to numerical errors during BF16 training (Budzinskiy et al., 2025). We also observe that massiveactivations originate primarily from early layers (e.g., layer 5), where the FFN outputs large values,consistent with (Yona et al., 2025). Once added to the residual stream, these activations are propagatedthrough subsequent layers via the pre-norm mechanism. This aligns with the effectiveness of sandwichnormalization (Ding et al., 2021) in enhancing training stability (Table 2, row 7): applying LayerNorm tothe FFN output prevents these large activations from entering the residual stream.

# 4.4 SDPA Output Gating Facilitates Context Length Extension

Based on the attention-sink-free pattern, we evaluate theSDPA gating’s effect in thelong-context setting. Specif-ically, we extend the contextlength for the models trainedon 3.5T tokens. We increasethe RoPE (Su et al., 2024)base from 10k to 1M and con-tinue training on data with a

Table 5: Performance of different methods across varying sequence lengths. ‘YaRNExtended’ indicates the expanded context length variant. ‘(values)’ indicate theperformance declines after extending the context length.

<table><tr><td>Method</td><td>4k</td><td>8k</td><td>16k</td><td>32k</td><td>64k</td><td>128k</td></tr><tr><td>Baseline</td><td>88.89</td><td>85.88</td><td>83.15</td><td>79.50</td><td>-</td><td>-</td></tr><tr><td>SDPA-Gate</td><td>90.56</td><td>87.11</td><td>84.61</td><td>79.77</td><td>-</td><td>-</td></tr><tr><td colspan="7">YaRN Extended</td></tr><tr><td>Baseline</td><td>82.90(-6.0)</td><td>71.52(-14.4)</td><td>61.23(-21.9)</td><td>37.94(-41.56)</td><td>37.51</td><td>31.65</td></tr><tr><td>SDPA-Gate</td><td>88.13(-2.4)</td><td>80.01(-7.1)</td><td>76.74(-7.87)</td><td>72.88(-6.89)</td><td>66.60</td><td>58.82</td></tr></table>

sequence length of 32k for an additional 80B tokens. This gives us models with a context length of $3 2 \mathrm { k }$Subsequently, we use YaRN (Peng et al., 2023) to extend the context length to 128k. We evaluate models onthe RULER benchmark (Hsieh et al., 2024) and summarize results in Tab. 5. We observe the following: (i)Under the 32k setting, models with gating slightly outperform the baseline. This suggests that within thetraining length, the attention sink phenomenon may not hurt the model’s long-context performance. (ii)When the context length is extended to 128k using YaRN, both the baseline and gated models experiencea decline within the original 32k range. This observation is consistent with previous works on extendingcontext length by modifying RoPE (Chen et al., 2023; Peng et al., 2023; Dong et al., 2025). Even thoughthe decline is less pronounced for models with gating. (iii) At context lengths of 64k and $1 2 8 \mathbf { k } ,$ , thegated attention models outperform the baseline signifantly. From these observations, we hypothesizethat adding gating helps the model adapt to the context-length extension. A possible explanation is thatbaseline models rely on attention sinks to adjust the distribution of attention scores. Dong et al. (2025)derives the effects of changing the RoPE based on the attention and hidden state distributions. Whentechniques like YaRN are applied to modify the RoPE base, the attention sink pattern may struggle toadapt in a training-free manner, leading to a noticeable drop in performance. In contrast, models withgating primarily rely on input-dependent gating scores to control information flow, making them morerobust to such changes.

# 5 Related Works

# 5.1 Gating in Neural Networks

Gating mechanisms have been widely adopted in neural networks. Early works such as LSTMs (Hochreiter& Schmidhuber, 1997) and GRUs (Dey & Salem, 2017) introduce gates to regulate information flowacross time steps, addressing gradient vanishing/exploding issues by selectively retaining or discardinginformation. Highway Networks (Srivastava et al., 2015) extend this concept to feedforward networks,enabling the successful training of very deep architectures. SwiGLU (Shazeer, 2020) introduce gatingmechanisms into transformer FFN layers, enhancing their expressive power and becoming a standardcomponent in many open-source LLMs (Grattafiori et al., 2024; Yang et al., 2024a).

Several works on state-space models (Gu & Dao, 2023; Dao & Gu, 2024) and Linear Attention, such asFLASH (Hua et al., 2022), RetNet (Sun et al., 2023), Lightning Attention (Qin et al., 2024a;b; Li et al., 2025),and Gated Delta Networks (Yang et al., 2024b), also incorporate gating modules to controlinformationof token-mixer modules. Forgetting Transformer (Lin et al., 2025) applies gating mechanisms to theoutput of softmax attention, observing significant performance improvements. Although these worksdemonstrate the effectiveness of gating, a comprehensive understanding of its precise mechanisms and thereasons behind its effectiveness still needs exploration. This could contribute to a broader appreciation ofgating’s importance beyond RNNs and facilitate designs that better leverage gating’s unique advantages.For example, while Switch Heads (Csordas et al., 2024b;a), NSA (Yuan et al., 2025), and MoSA (Piękoset al., 2025) employ sigmoid-based gating (Csordas et al., 2023) for selection, further investigationinto isolating gating’s specific contribution could offer valuable insights. Comparisons with baselinesincorporating similar gating mechanisms in standard transformers could offer a more refined perspectiveon the effectiveness of their proposed selection mechanisms. The work most closely related to ours isQuantizable Transformers (Bondarenko et al., 2023), which also finds that applying gating in softmaxattention alleviates extreme attention concentration and outliers in hidden states in encoder models likeBERT and ViT. While this work primarily leverages gating to eliminate outliers for model quantization,we provide a detailed analysis of various gating variants, uncovering their benefits through enhancednon-linearity and sparsity, as well as improved training stability. Building on these insights, we scale upgated attention models, demonstrating gating’s broad applicability and impact.

# 5.2 Attention Sink

Xiao et al. (2023) formally identifies the ‘attention sink’ phenomenon, in which specific tokens receive largeattention scores. Similarly, Darcet et al. (2023) finds in the vision transformer, some redundant tokensact as ‘registers’ to store attention scores. Later, Sun et al. (2024) shows that excessive attention scoresare also assigned to tokens associated with massive activation values. However, our work reveals thatapplying gating at the output of value projection eliminates massive activations, yet attention sinks persist,indicating that massive activations are not a necessary condition for attention sinks. Similarly, Gu et al.(2024) characterizes attention sinks as non-informative ‘key biases’ that store redundant attention scores,arguing that softmax’s inherent normalization dependency drives this behavior. Experimental attempts tomodify softmax attention, such as replacing softmax with unnormalized sigmoid attention (Ramapuramet al., 2024; Gu et al., 2024), adding softmax attention gate or clip (Bondarenko et al., 2023), and modifyingsoftmax computation (Zuhri et al., 2025) and denominator (Miller, 2023), show promise in mitigatingattention sinks. Our work demonstrates that sparse gating after SDPA eliminates attention sinks in bothdense (1B-parameter) and MoE (15B-parameter) models, even when trained on 3.5T tokens. Furthermore,we uncover the potential of eliminating attention sinks to benefit context-length extension.

# 6 Conclusion

This work systematically investigates the role of gating mechanisms in the standard softmax attention,revealing their significant impact on performance, training stability, and attention dynamics. Throughextensive experimental comparisons over 30 variants of 15B MoE and 1.7B dense models trained on up to3.5T tokens, we demonstrate that applying a sigmoid gate after scaled dot-product attention yields the mostsubstantial improvements. This simple mechanism enhances non-linearity, introduces input-dependentsparsity, and eliminates inefficiencies like the ‘attention sink’ phenomenon. Additionally, gating facilitatescontext length extension, allowing models to generalize effectively to longer sequences without retraining.We also release the first attention-sink-free models. We believe these empirical validations will pave theway for engineering the next generation of advanced foundation models.

# Limitations

Our work primarily focuses on analyzing the reasons and impacts of attention gating through a series ofablation studies. However, we acknowledge several limitations. The broader implications of non-linearityon the dynamics of attention and the overall training process remain under-explored. Although weobserve that eliminating attention sinks improves performance in long-context extension scenarios, wedo not provide a rigorous theoretical explanation for how attention sinks influence the model’s ability togeneralize to longer sequences.

# References



Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprintarXiv:2305.13245, 2023.





Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers: Removingoutliers by helping attention heads do nothing. Advances in Neural Information Processing Systems, 36:75067–75096, 2023.





Stanislav Budzinskiy, Wenyi Fang, Longbin Zeng, and Philipp Petersen. Numerical error analysis of largelanguage models. arXiv preprint arXiv:2503.10251, 2025.





Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, ArielHerbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, JoshAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati,Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, andWojciech Zaremba. Evaluating large language models trained on code, 2021.





Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window oflarge language models via positional interpolation, 2023. URL https://arxiv.org/abs/2306.15595.





Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling languagemodeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023.





Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math wordproblems. arXiv preprint arXiv:2110.14168, 2021.





Robert Csordas, Kazuki Irie, and Jurgen Schmidhuber. Approximating two-layer feedforward networksfor efficient transformers. arXiv preprint arXiv:2310.10837, 2023.





Robert Csordas, Kazuki Irie, Jurgen Schmidhuber, Christopher Potts, and Christopher D Manning. Moeut:Mixture-of-experts universal transformers. arXiv preprint arXiv:2405.16039, 2024a.





Robert Csordas, Piotr Piekos, Kazuki Irie, and Jurgen Schmidhuber. Switchhead: Accelerating transformerswith mixture-of-experts attention. Advances in Neural Information Processing Systems, 37:74411–74438,2024b.





Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng,Xingkai Yu, Y Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-expertslanguage models. arXiv preprint arXiv:2401.06066, 2024.





Francesco D’Angelo, Maksym Andriushchenko, Aditya Vardhan Varre, and Nicolas Flammarion. Why dowe need weight decay in modern deep learning? Advances in Neural Information Processing Systems, 37:23191–23223, 2024.





Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms throughstructured state space duality. In Forty-first International Conference on Machine Learning, ICML 2024,Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ztn8FCR1td.





Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers.arXiv preprint arXiv:2309.16588, 2023.





Rahul Dey and Fathi M Salem. Gate-variants of gated recurrent unit (gru) neural networks. In 2017 IEEE60th international midwest symposium on circuits and systems (MWSCAS), pp. 1597–1600. IEEE, 2017.





Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, ZhouShao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers,2021.





Zican Dong, Junyi Li, Jinhao Jiang, Mingyu Xu, Wayne Xin Zhao, Bingning Wang, and Weipeng Chen.Longred: Mitigating short-text degradation of long-context large language models via restorationdistillation. ArXiv, abs/2502.07365, 2025. URL https://arxiv.org/abs/2502.07365.





Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models.arXiv preprint arXiv:2407.21783, 2024.





Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprintarXiv:2312.00752, 2023.





Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin.When attention sink emerges in language models: An empirical view. arXiv preprint arXiv:2410.10781,2024.





Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-hardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.





Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,1997.





Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang,and Boris Ginsburg. Ruler: What’s the real context size of your long-context language models? arXivpreprint arXiv:2404.06654, 2024.





Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. In InternationalConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 ofProceedings of Machine Learning Research, pp. 9099–9117. PMLR, 2022. URL https://proceedings.mlr.press/v162/hua22a.html.





Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline chinese evalu-ation suite for foundation models. Advances in Neural Information Processing Systems, 36, 2024.





Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo,Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprintarXiv:2501.08313, 2025.





Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin.Cmmlu: Measuring massive multitask language understanding in chinese, 2023.





Zhixuan Lin, Evgenii Nikishin, Xu Owen He, and Aaron Courville. Forgetting transformer: Softmaxattention with a forget gate. arXiv preprint arXiv:2503.02130, 2025.





Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batchtraining. arXiv preprint arXiv:1812.06162, 2018.





Evan Miller. Attention is off by one, 2023. URL https://www.evanmiller.org/attention-is-off-by-one.html.





Guido Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regionsof deep neural networks, 2014. URL https://arxiv.org/abs/1402.1869.





Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, BenMann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXivpreprint arXiv:2209.11895, 2022.





Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context windowextension of large language models. arXiv preprint arXiv:2309.00071, 2023.





Piotr Piękos, Róbert Csordás, and Jürgen Schmidhuber. Mixture of sparse attention: Content-basedlearnable sparse attention via expert-choice routing, 2025. URL https://arxiv.org/abs/2505.00315.





Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models. arXiv preprintarXiv:2401.04658, 2024a.





Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Various lengths, constantspeed: Efficient language modeling with lightning attention. arXiv preprint arXiv:2405.17381, 2024b.





Zihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu, JingrenZhou, and Junyang Lin. Demons in the detail: On implementing load balancing loss for trainingspecialized mixture-of-expert models, 2025. URL https://arxiv.org/abs/2501.11873.





Jason Ramapuram, Federico Danieli, Eeshan Dhekane, Floris Weers, Dan Busbridge, Pierre Ablin, TatianaLikhomanenko, Jagrit Digani, Zijin Gu, Amitis Shidani, et al. Theory, analysis, and best practices forsigmoid self-attention. arXiv preprint arXiv:2409.04431, 2024.





Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.





Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprintarXiv:1505.00387, 2015.





Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhancedtransformer with rotary position embedding. Neurocomputing, 568:127063, 2024.





Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations in large language models.arXiv preprint arXiv:2402.17762, 2024.





Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and FuruWei. Retentive network: A successor to transformer for large language models, 2023. URL https://arxiv.org/abs/2307.08621.





Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. Spike no more: Stabilizing the pre-trainingof large language models. arXiv preprint arXiv:2312.16903, 2023.





A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.





Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418,2019.





Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascadetoken and head pruning. In 2021 IEEE International Symposium on High-Performance Computer Architecture(HPCA), pp. 97–110. IEEE, 2021.





Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet:Scaling transformers to 1,000 layers, 2022. URL https://arxiv.org/abs/2203.00555.





Zekun Wang, Jingchang Chen, Wangchunshu Zhou, Haichao Zhu, Jiafeng Liang, Liping Shan, MingLiu, Dongliang Xu, Qing Yang, and Bing Qin. Smarttrim: Adaptive tokens and attention pruning forefficient vision-language models. arXiv preprint arXiv:2305.15033, 2023.





Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming languagemodels with attention sinks. arXiv preprint arXiv:2309.17453, 2023.





An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, DayihengLiu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a.





Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with deltarule. arXiv preprint arXiv:2412.06464, 2024b.





Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei. Differential transformer.arXiv preprint arXiv:2410.05258, 2024.





Itay Yona, Ilia Shumailov, Jamie Hayes, Federico Barbero, and Yossi Gandelsman. Interpreting the repeatedtoken phenomenon in large language models. arXiv preprint arXiv:2503.08908, 2025.





Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei,Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainablesparse attention. arXiv preprint arXiv:2502.11089, 2025.



Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machinereally finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, WendiZheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414,2022.

Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural InformationProcessing Systems, 32, 2019.

Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and WilliamFedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906,2022.

Zayd M. K. Zuhri, Erland Hilman Fuadi, and Alham Fikri Aji. Softpick: No attention sink, no massiveactivations with rectified softmax, 2025. URL https://arxiv.org/abs/2504.20966.

# A Supplement Experiments

# A.1 Switch Head Baselines

In this section, we present detailed experiments related to Switch Heads. The Switch Head paperdemonstrates that introducing sparse activation in attention—where each token selects the top-k expertsfrom a pool of key/value/output experts via learnable sigmoid routing—enables the model to achievecomparable results to the baseline. This suggests that, within the Switch Head framework, both expertparameters and activated parameters are beneficial, with more being better under the same total parameterbudget.


Table 6: Performance of different switch head methods with varying parameter additions and configurations. ‘switchkv’ and ‘switch v’ refer to introducing selective computing in key-value and value components, respectively. ‘Switchkv, 8top8’ means there are 8 key and value map experts, and each token select top8 experts. Notice ‘Switch v, 1top1’is equivalent to v Headwise Gate in Tab. 1 row (11).


<table><tr><td>Method</td><td>Added Param (M)</td><td>PPL</td><td>MMLU</td><td>GSM8k</td><td>Hellaswag</td><td>C-eval</td></tr><tr><td>(1) Baseline (q32, kv4)</td><td>-</td><td>6.026</td><td>58.79</td><td>52.92</td><td>73.07</td><td>60.26</td></tr><tr><td>(2) Switch kv, 8top8</td><td>38</td><td>5.847</td><td>59.17</td><td>52.54</td><td>73.32</td><td>61.01</td></tr><tr><td>(3) Switch kv, 4top4</td><td>13</td><td>5.935</td><td>58.14</td><td>53.27</td><td>73.75</td><td>59.67</td></tr><tr><td>(4) Switch v, 4top4</td><td>13</td><td>5.820</td><td>59.02</td><td>52.77</td><td>73.34</td><td>61.74</td></tr><tr><td>(5) Switch v, 8top2</td><td>25</td><td>5.870</td><td>59.10</td><td>53.53</td><td>74.17</td><td>62.34</td></tr><tr><td>(6) Switch v, 1top1</td><td>3</td><td>5.808</td><td>59.32</td><td>53.53</td><td>74.38</td><td>62.61</td></tr></table>

Looking at the results in Tab. 6, we observe an interesting trend: while increasing the number of activatedkv experts (with the same expert parameter settings) appears to offer some improvement in PPL (row4 vs. 5), the gains in overall benchmark performance are less pronounced. Notably, the best results forboth benchmark scores and PPL were achieved by ‘Switch v 1top1’ (row 6), which, as mentioned earlier,is analogous to applying sigmoid gating directly to the output of the value layer. These findings raisean intriguing question about the primary driver of the performance improvements observed in theseexperiments. It suggests that the introduction of gating itself plays a significant role in the effectiveness ofthis approach.

# A.2 More Discussion on Sparse Gating Score

In this section, we analyze the impact of gating score sparsity on attention output. First, we examinethe mean values of SDPA output before and after applying gating to the hidden states. Specifically, wecalculated the mean absolute values of $Y$ and $Y ^ { \prime }$ before and after $\breve { G } _ { 1 }$ at each layer, as shown in Fig. 4. Wealso included results from a baseline without gating for comparison. The results indicate that: (1) aftergating, the mean value of hidden states decreased from 0.71 to 0.05, corresponding to the generally smallgating scores; (2) the gated hidden states closely resemble the baseline, suggesting that gating mightserve a similar function as attention sink in filtering out irrelevant information.

We further analyze the proportion of hidden states below certain thresholds before and after gating, asshown in Fig 5. The results reveal that: (1) after gating, the sparsity in hidden states significantly increasesacross different thresholds. Since the mean gating scores are already small, multiplying hidden states bya small number naturally pushes some values below the threshold. Therefore, (2) we further multiply thepre-gating hidden states by the average gating score and observed that the increase in sparsity is smallerthan with original gating. This suggests that sparse gating scores enhance sparsity in hidden states.

![](images/fa42b8af0a6afdf6b44859187fddd32da917372e204a0475cc1de6d64ccf8dbf.jpg)



Figure 4: Mean absolute values before and after gating. The baseline and post-gating values are similar.


![](images/82e932fdb68fd3e83649a6cee4b5e4966f9b8f7410747634a4d9ca4830e6bd8e.jpg)


![](images/d59ccafb8adb140f7c0f2098c6d0e7679898b1cda77e30aa07c823aab8c1625c.jpg)



Figure 5: Proportion of SDPA output values below threshold after gating (Left: 1e-2, Right: 1e-3). We also includesparsity measurements obtained by multiplying the average gating score with pre-gating hidden states.


# A.3 Layerwise Massive Activations and Attention Sinks

In this section, we compare and analyze the presence of massive activations and attention sinks (theattention score of the first token) within the model. From the results, we observe the following:

For the baseline (row 1), the output of the 6th layer’s FFN contains massive activations, which are subse-quently added to the residual stream, causing large activations to persist in the residuals of subsequentlayers. Correspondingly, significant attention sink phenomena emerge starting from the 6th layer. Afterapplying gating to the SDPA output (row 2), the outputs of the earlier layers in the network remainrelatively small overall, with massive activations growing gradually as the layer depth increases. Notably,no significant attention sink phenomenon is observed in any layer of the network.

When gating is applied only at the value layer (row 3), the model exhibits massive activations similarto row 2. However, a certain degree of attention sink phenomenon persists. This indicates that massiveactivations are not a necessary condition for the emergence of attention sinks. When enforcing sharedgating scores across different heads (row 4) or modifying the activation function of gating to suppresssparsity (row 5), the sparsity introduced by gating is reduced. In these cases, both massive activationsand attention sinks become comparable to those observed in the baseline.

These observations suggest that introducing sufficient sparsity within the attention mechanism mayhelp mitigate the occurrence of massive activations. However, further investigation is needed to fullyunderstand the interplay between sparsity, massive activations, and attention sinks, particularly in thecontext of scaling to deeper and larger models.

# A.4 More Layerwise Gating Scores

In this section, we analyze the distribution of gating scores under two additional constraints while usingSDPA output gating as the baseline (row 1, elementwise/headwise): (1) enforcing the same gating scoreacross different heads (row 2, left), and (2) restricting the minimum value of the gating scores (row 2,

right). When enforcing shared gating scores across different heads, the gating scores for most layersincrease. This indicates that different heads require different sparsity, highlighting the importance ofhead-specific gating mechanisms.

# A.5 Other Attempt to Stabilize Training

We observe that both the addition of sandwich normalization (Ding et al., 2021) and gating mechanismseliminate massive activations while improving training stability. This prompts us to explore whethersimpler methods could prevent large activations within residuals. Specifically, we introduce a clippingoperation to constrain the outputs of attention and FFN layers before they enter the residual connection,limiting their values to the range (-clip, clip). However, we find that regardless of whether the clip valuewas set to 300 or 100, the model still encounters convergence issues at a learning rate of 8e-3. This suggeststhat the instability in pre-norm model training is not solely due to large activations within residuals. Itis likely that any layer producing large outputs can lead to stability problems, indicating the need forfurther investigation into the root causes of training instability.

![](images/edd9528aaedbe98e77b425bdcbd3f47d63c03a9cba4d0a4e20e2a6b9b37df38f.jpg)


![](images/5a820d96014b178bcdb4f6d30d973754abde424e9c09396eafceab5d66bb019a.jpg)


![](images/92f9c1910aca0d5952856ac2000d1baaead3b3878aef78691ded6dd0703915ef.jpg)


![](images/2229d2249fef704f83b2f9f96db8d886c0f5ccee2f5b98dbad87977e3c8d7466.jpg)


![](images/61e7ff7ca8bd38b06c740aaf961c0c4feeb21cf4cced7b4dc745f31c6f2bb561.jpg)


![](images/ac5cc6cab8e213fb7ed037889adc53ded6bb161afb9ed2151b23f056bd9e5c72.jpg)


![](images/71190310db202d5da04318125781a8ab41c290ebc050f3a6d9fd0fa5c9d79fa6.jpg)


![](images/d0ac5211ed5ae4f44a92bffe40ac85b91a1920303b757fec9b1c1bec36e8c276.jpg)


![](images/9d1a7c5cbdb4874c619d3656d23e10f8c1a7eb8e3598d15cea1308d88f05f112.jpg)


![](images/20963d4f3476c7662519484f7a702b5804886303fd5c0f93bac21d8e1714b73b.jpg)



Figure 6: Comparison of massive activations and attention sink phenomena across different gating configurations.Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating):Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2but with residual attention sinks. Rows 4–5 (Reduced Sparsity via cross-head share and NS-sigmoid): Massiveactivations and attention sinks resemble the baseline.


![](images/caad1a3c9c09f601e1bf9d2bc528eaa72f0557fcfa1e84900cd84536ae636632.jpg)


![](images/5b32b95a0a9756b45593d1a793e185ab8175600ad614d9ea1f31f85787568196.jpg)


![](images/e3f98513df45271b9de3b2d1e76c6f4cb1bb3d150270ec84da8988e6f6747991.jpg)


![](images/8be949943adf221c7f673155f450ac944aa78cc7116bbfbeeb6e47fad401b61a.jpg)



Figure 7: Distribution of gating scores under different constraints for SDPA output gating variants.
